import subprocess
import sys
import shutil
import os
import time
from pathlib import Path
import torch # å¼•å…¥ torch ç”¨äºåŠ è½½æ¨¡å‹
import logging # å¼•å…¥ logging ç”¨äºæ§åˆ¶ Nerfstudio è¾“å‡º
import json # å¼•å…¥ json ç”¨äºè¯»å†™ transforms æ–‡ä»¶
import numpy as np # å¼•å…¥ numpy è¿›è¡ŒçŸ©é˜µè¿ç®—

# è®¾ç½® Nerfstudio å†…éƒ¨æ—¥å¿—çº§åˆ«ï¼Œé¿å…å¤§é‡æ‚é¡¹è¾“å‡ºå¹²æ‰°
logging.getLogger('nerfstudio').setLevel(logging.ERROR) 

# ================= é…ç½®åŒºåŸŸ =================
# Linux ä¸‹çš„ä¸´æ—¶é«˜é€Ÿå·¥ä½œåŒº
LINUX_WORK_ROOT = Path.home() / "braindance_workspace"
# åœºæ™¯åŠå¾„ç³»æ•°ï¼šæ§åˆ¶è£å‰ªçš„å®½æ¾ç¨‹åº¦
# 1.8 æ˜¯ä¸€ä¸ªç»éªŒå€¼ï¼Œèƒ½å¾ˆå¥½åœ°åŒ…å®¹å¤šç‰©ä½“æ¡Œé¢ï¼ŒåŒæ—¶åˆ‡é™¤è¿œå¤„çš„å¢™
SCENE_RADIUS_SCALE = 1.8 

# ================= æ ¸å¿ƒç®—æ³•ï¼šæ™ºèƒ½åœºæ™¯åˆ†æä¸è‡ªé€‚åº”è£å‰ª =================
def analyze_and_calculate_adaptive_collider(json_path):
    """
    1. åˆ¤æ–­åœºæ™¯ç±»å‹ (ç‰©ä½“ vs æˆ¿é—´)ã€‚
    2. å¦‚æœæ˜¯ç‰©ä½“ï¼ŒåŸºäºç›¸æœºè·ç¦»åŠ¨æ€è®¡ç®—è£å‰ªèŒƒå›´ (Adaptive Pruning)ï¼Œä¿æŠ¤å¤šä¸»ä½“ã€‚
    """
    print(f"\nğŸ¤– [AI åˆ†æ] æ­£åœ¨è§£æç©ºé—´ç»“æ„ä¸ç›¸æœºè½¨è¿¹...")
    
    try:
        with open(json_path, 'r') as f:
            data = json.load(f)
        
        frames = data["frames"]
        if not frames: return [], "unknown"

        # --- ç¬¬ä¸€æ­¥ï¼šæå–å‡ ä½•ä¿¡æ¯ ---
        positions = []
        forward_vectors = []
        distances_to_origin = [] # Nerfstudio ä¼šå°†ä¸»ä½“ä¸­å¿ƒåŒ–åˆ°åŸç‚¹
        
        for frame in frames:
            c2w = np.array(frame["transform_matrix"])
            pos = c2w[:3, 3]
            positions.append(pos)
            
            # è®¡ç®—å‰å‘å‘é‡ (-Z)
            rot = c2w[:3, :3]
            forward = rot @ np.array([0, 0, -1]) 
            forward_vectors.append(forward)
            
            # è®¡ç®—åˆ°åŸç‚¹çš„è·ç¦»
            dist = np.linalg.norm(pos)
            distances_to_origin.append(dist)
            
        positions = np.array(positions)
        forward_vectors = np.array(forward_vectors)
        distances_to_origin = np.array(distances_to_origin)
        
        # --- ç¬¬äºŒæ­¥ï¼šåˆ¤æ–­åœºæ™¯ç±»å‹ (Inward vs Outward) ---
        center_of_mass = np.mean(positions, axis=0)
        vec_to_center = center_of_mass - positions
        norms = np.linalg.norm(vec_to_center, axis=1, keepdims=True)
        norms[norms < 1e-6] = 1.0 
        vec_to_center_norm = vec_to_center / norms
        dot_products = np.sum(forward_vectors * vec_to_center_norm, axis=1)
        looking_inward_ratio = np.sum(dot_products > 0) / len(frames)
        
        print(f"    -> ç›¸æœºèšåˆåº¦: {looking_inward_ratio:.2f}")

        # --- ç¬¬ä¸‰æ­¥ï¼šå†³ç­–ä¸è®¡ç®— ---
        if looking_inward_ratio > 0.6:
            print("ğŸ’¡ åˆ¤å®šç»“æœï¼šã€ç‰©ä½“/å¤šä¸»ä½“æ¨¡å¼ (Inward)ã€‘")
            
            # === æ ¸å¿ƒå‡çº§ï¼šè‡ªé€‚åº”è®¡ç®— (Adaptive Calculation) ===
            # ä¸å†ä½¿ç”¨å›ºå®šçš„ 2.0/6.0ï¼Œè€Œæ˜¯æ ¹æ®å®é™…æ‹æ‘„è·ç¦»è®¡ç®—
            
            avg_dist = np.mean(distances_to_origin)
            min_dist = np.min(distances_to_origin)
            
            print(f"    -> ç»Ÿè®¡æ•°æ®: ç›¸æœºå¹³å‡è·ç¦» {avg_dist:.2f}, æœ€è¿‘è·ç¦» {min_dist:.2f}")
            
            # åŠ¨æ€è®¡ç®—è£å‰ªé¢
            # å‡è®¾ä¸»ä½“åœ¨åŸç‚¹ï¼ŒåŠå¾„çº¦ä¸º 1.0 (Nerfstudio å½’ä¸€åŒ–ç‰¹æ€§)
            # å®½æ¾åŠå¾„ = 1.0 * ç³»æ•°
            scene_radius = 1.0 * SCENE_RADIUS_SCALE
            
            # è¿‘å¹³é¢ï¼šæœ€è¿‘ç›¸æœºè·ç¦» - åœºæ™¯åŠå¾„ (é˜²æ­¢åˆ‡æ‰çªå‡ºçš„ç‰©ä½“)
            calc_near = max(0.05, min_dist - scene_radius)
            
            # è¿œå¹³é¢ï¼šå¹³å‡ç›¸æœºè·ç¦» + åœºæ™¯åŠå¾„ (åˆ‡æ‰èƒŒæ™¯å¢™)
            calc_far = avg_dist + scene_radius
            
            print(f"    -> ç­–ç•¥ï¼šè‡ªé€‚åº”è£å‰ªã€‚Near={calc_near:.2f}, Far={calc_far:.2f}")
            print(f"       (è¯¥ç­–ç•¥å¯ä¿æŠ¤ç›¸é‚»çš„å¤šç‰©ä½“ï¼ŒåŒæ—¶å»é™¤èƒŒæ™¯)")
            
            return ["--pipeline.model.enable-collider", "True", 
                    "--pipeline.model.collider-params", "near_plane", str(round(calc_near, 2)), 
                    "far_plane", str(round(calc_far, 2))], "object"
        else:
            print("ğŸ’¡ åˆ¤å®šç»“æœï¼šã€å…¨æ™¯/å®¤å†…æ¨¡å¼ (Outward)ã€‘")
            print("    -> ç­–ç•¥ï¼šæ”¾å®½è£å‰ªï¼Œä¿ç•™å®Œæ•´ç¯å¢ƒã€‚")
            
            return ["--pipeline.model.enable-collider", "True", 
                    "--pipeline.model.collider-params", "near_plane", "0.05", "far_plane", "100.0"], "scene"

    except Exception as e:
        print(f"âš ï¸ åˆ†æå¤±è´¥ ({e})ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°ã€‚")
        return ["--pipeline.model.enable-collider", "True", 
                "--pipeline.model.collider-params", "near_plane", "0.1", "far_plane", "50.0"], "unknown"

# ================= ä¸»æµç¨‹ =================

def run_pipeline(video_path, project_name):
    print(f"\nğŸš€ [BrainDance Engine] å¯åŠ¨ä»»åŠ¡: {project_name}")
    
    # 1. è·¯å¾„è§£æ
    video_src = Path(video_path).resolve()
    
    # å®šä¹‰ä¸´æ—¶å·¥ä½œç›®å½•
    work_dir = LINUX_WORK_ROOT / project_name
    data_dir = work_dir / "data"
    output_dir = work_dir / "outputs"
    
    # ================= [æ™ºèƒ½æ£€æŸ¥] æ–­ç‚¹ç»­ä¼ é€»è¾‘ =================
    transforms_file = data_dir / "transforms.json"
    env = os.environ.copy()
    env["QT_QPA_PLATFORM"] = "offscreen" 

    if transforms_file.exists():
        print(f"\nâ© [æ–­ç‚¹ç»­ä¼ ] æ£€æµ‹åˆ°å·²å­˜åœ¨çš„ COLMAP æ•°æ®: {transforms_file}")
    else:
        print(f"ğŸ†• [æ–°ä»»åŠ¡] æœªæ‰¾åˆ°å†å²æ•°æ®ï¼Œå¼€å§‹åˆå§‹åŒ–å·¥ä½œåŒº...")
        if work_dir.exists(): shutil.rmtree(work_dir)
        work_dir.mkdir(parents=True)
        data_dir.mkdir(parents=True)
        
        print(f"ğŸ“‚ [IO ä¼˜åŒ–] è¿ç§»æ•°æ®...")
        video_dst = work_dir / video_src.name
        shutil.copy(str(video_src), str(video_dst))

        # ================= [Step 1] æ•°æ®é¢„å¤„ç† =================
        print(f"\nğŸ¥ [1/3] è§†é¢‘æŠ½å¸§ä¸ä½å§¿è§£ç®— (COLMAP)")
        
        extracted_images_dir = data_dir / "images"
        extracted_images_dir.mkdir(parents=True, exist_ok=True)
        
        # 1.1 FFmpeg (1920å®½ / 4 FPS)
        ffmpeg_cmd = [
            "ffmpeg", "-y", "-i", str(video_dst), 
            "-vf", "scale=1920:-1,fps=4", 
            "-q:v", "2", 
            str(extracted_images_dir / "frame_%05d.jpg")
        ]
        subprocess.run(ffmpeg_cmd, check=True) 
        
        # 1.2 Nerfstudio (COLMAP)
        # æ·»åŠ  --center_method focus æœ‰åŠ©äºå°†ç‰©ä½“ç½®äºåŸç‚¹ï¼Œé…åˆæˆ‘ä»¬çš„è‡ªé€‚åº”ç®—æ³•
        cmd_colmap = [
            "ns-process-data", "images",
            "--data", str(extracted_images_dir),
            "--output-dir", str(data_dir),
            "--verbose",
        ]
        
        process_result = subprocess.run(
            cmd_colmap, check=True, env=env, capture_output=True, text=True
        )
        print(process_result.stdout)
        
        if "COLMAP only found poses" in process_result.stdout:
            print("\nğŸš¨ğŸš¨ğŸš¨ COLMAP å¤±è´¥ï¼šç‰¹å¾ç‚¹ä¸è¶³ã€‚")
            shutil.rmtree(data_dir)
            raise RuntimeError("COLMAP æ•°æ®è´¨é‡ä¸åˆæ ¼ã€‚")

        if not transforms_file.exists():
            raise FileNotFoundError("æœªæ‰¾åˆ° transforms.json æ–‡ä»¶ã€‚")


    # ================= [Step 2] æ¨¡å‹è®­ç»ƒ =================
    
    search_path = output_dir / project_name / "splatfacto"
    run_dirs = sorted(list(search_path.glob("*"))) if search_path.exists() else []

    if run_dirs:
        print(f"\nâ© [è®­ç»ƒè·³è¿‡] æ£€æµ‹åˆ°å·²å®Œæˆçš„è®­ç»ƒç»“æœï¼š{run_dirs[-1].name}")
    else:
        # === æ ¸å¿ƒä¿®æ”¹ï¼šè°ƒç”¨æœ€æ–°çš„è‡ªé€‚åº”è®¡ç®—ç®—æ³• ===
        collider_args, scene_type = analyze_and_calculate_adaptive_collider(transforms_file)
        
        print(f"\nğŸ§  [2/3] å¼€å§‹è®­ç»ƒ (RTX 5070 åŠ é€Ÿä¸­)")
        
        cmd_train = [
            "ns-train", "splatfacto",
            "--data", str(data_dir),
            "--output-dir", str(output_dir),
            "--experiment-name", project_name,
            
            "--pipeline.model.random-init", "False", 
            "--pipeline.model.cull-alpha-thresh", "0.005",

            # æ’å…¥è‡ªé€‚åº”å‚æ•°
            *collider_args,
            
            "--max-num-iterations", "15000",
            "--vis", "viewer+tensorboard", 
            "--viewer.quit-on-train-completion", "True",
            "colmap",
        ]
        subprocess.run(cmd_train, check=True, env=env)

    # ================= [Step 3] å¯¼å‡ºç»“æœ =================
    print(f"\nğŸ’¾ [3/3] å¯¼å‡ºç»“æœ")
    
    if not run_dirs:
        run_dirs = sorted(list(search_path.glob("*")))

    if not run_dirs:
        print("âŒ é”™è¯¯ï¼šè®­ç»ƒç»“æœç›®å½•ä¸ºç©ºã€‚")
        return None
        
    latest_run = run_dirs[-1]
    config_path = latest_run / "config.yml"
    
    cmd_export = [
        "ns-export", "gaussian-splat",
        "--load-config", str(config_path),
        "--output-dir", str(work_dir)
    ]
    subprocess.run(cmd_export, check=True, env=env)
    
    print("â³ ç­‰å¾…æ–‡ä»¶å†™å…¥ç£ç›˜...")
    time.sleep(5) 

    # ================= [Step 4] ç»“æœå›ä¼  =================
    print(f"\nğŸ“¦ [IO åŒæ­¥] å›ä¼ è‡³ Windows...")
    
    target_dir = Path(__file__).parent / "results"
    target_dir.mkdir(exist_ok=True)

    temp_ply_default = work_dir / "point_cloud.ply"
    temp_ply_alt = work_dir / "splat.ply"
    
    if temp_ply_default.exists(): temp_ply = temp_ply_default
    elif temp_ply_alt.exists(): temp_ply = temp_ply_alt
    else: temp_ply = None
        
    transforms_src = data_dir / "transforms.json"
    final_webgl_poses = target_dir / "webgl_poses.json"
    final_ply = target_dir / f"{project_name}.ply"
    final_transforms = target_dir / "transforms.json"
    
    # WebGL å§¿æ€è½¬æ¢
    if transforms_src.exists():
        print("ğŸ”„ ç”Ÿæˆ WebGL å‹å¥½å§¿æ€æ–‡ä»¶...")
        try:
            with open(transforms_src, 'r') as f:
                data = json.load(f)
            webgl_frames = []
            for frame in data["frames"]:
                c2w_matrix = np.array(frame["transform_matrix"], dtype=np.float32)
                webgl_frames.append({
                    "file_path": frame["file_path"],
                    "pose_matrix_c2w": c2w_matrix.tolist() 
                })
            webgl_data = {
                "camera_model": data["camera_model"],
                "w": data["w"], "h": data["h"],
                "fl_x": data["fl_x"], "fl_y": data["fl_y"],
                "frames": webgl_frames
            }
            with open(final_webgl_poses, 'w') as f:
                json.dump(webgl_data, f, indent=4)
        except Exception as e:
            print(f"âŒ å§¿æ€é¢„å¤„ç†å¤±è´¥: {e}")

    if temp_ply and temp_ply.exists():
        subprocess.run(f"cp {str(temp_ply)} {str(final_ply)}", check=True, shell=True)
        if transforms_src.exists():
            subprocess.run(f"cp {str(transforms_src)} {str(final_transforms)}", check=True, shell=True)
        
        shutil.rmtree(work_dir)
        print(f"âœ… æˆåŠŸï¼æœ€ç»ˆæ¨¡å‹: {final_ply}")
        return str(final_ply)
    else:
        print("âŒ å¯¼å‡ºå¤±è´¥ï¼Œæœªæ‰¾åˆ° PLY æ–‡ä»¶ã€‚")
        return None

if __name__ == "__main__":
    script_dir = Path(__file__).resolve().parent
    video_file = script_dir / "test.mp4" 
    
    if len(sys.argv) > 1:
        video_file = Path(sys.argv[1])

    if video_file.exists():
        run_pipeline(video_file, "scene_auto_sync")
    else:
        print(f"âŒ æ‰¾ä¸åˆ°è§†é¢‘: {video_file}")