import sys
import os

# ğŸ”¥ã€å…³é”®ã€‘å¼ºåˆ¶æ·»åŠ  vggt åº“çš„è·¯å¾„
# ä½ çš„ vggt åº“æ‰€åœ¨çš„çœŸå®è·¯å¾„
vggt_lib_path = "/home/ltx/workspace/ai/vggt"

if vggt_lib_path not in sys.path:
    print(f"âš¡ [ç¯å¢ƒä¿®æ­£] æ·»åŠ  VGGT åº“è·¯å¾„: {vggt_lib_path}")
    sys.path.insert(0, vggt_lib_path)

# ğŸ”¥ã€å…³é”®ã€‘ç¡®ä¿å½“å‰ç›®å½•ä¸åœ¨ sys.path çš„é¦–ä½ï¼Œé˜²æ­¢è¯¯å¼•ç”¨
# (å¯é€‰ï¼Œä½†æ¨è)
try:
    current_dir = os.path.dirname(os.path.abspath(__file__))
    if current_dir in sys.path:
        sys.path.remove(current_dir)
        sys.path.append(current_dir) # æŠŠå®ƒæ”¾åˆ°æœ€å
except:
    pass

import subprocess
import shutil
import time
import datetime # å¼•å…¥æ—¶é—´å¤„ç†åº“
from pathlib import Path
import json
import numpy as np
import logging
import cv2 # å¼•å…¥OpenCVåº“
import re # å¼•å…¥æ­£åˆ™åº“ç”¨äºæ—¥å¿—åˆ†æ

# --- ğŸ”¥ æ–°å¢ VGGT å¯¼å…¥ ---
import torch
import torch.nn.functional as F
# å‡è®¾ä½ å·²ç» pip install -e . å®‰è£…äº† vggtï¼Œæˆ–è€…å°† vggt æ–‡ä»¶å¤¹æ”¾åœ¨äº†åŒä¸€ç›®å½•
from vggt.models.vggt import VGGT
from vggt.utils.load_fn import load_and_preprocess_images_square
from vggt.utils.pose_enc import pose_encoding_to_extri_intri
from vggt.utils.geometry import unproject_depth_map_to_point_map
from vggt.utils.helper import create_pixel_coordinate_grid, randomly_limit_trues
from vggt.dependency.np_to_pycolmap import batch_np_matrix_to_pycolmap_wo_track, batch_np_matrix_to_pycolmap
# å¦‚æœéœ€è¦ BA (Bundle Adjustment)ï¼Œè¿˜éœ€è¦å¼•å…¥ track ç›¸å…³åº“ï¼Œä½†ä¸ºäº†é€Ÿåº¦å»ºè®®å…ˆä»…ä½¿ç”¨å‰é¦ˆ

import os

# ğŸ”¥ã€ç»æ€ã€‘å¼ºåˆ¶å°†ç¼–è¯‘å¥½çš„ç³»ç»Ÿçº§ colmap è·¯å¾„æåˆ°æœ€å‰é¢
# è¿™æ ·ç³»ç»Ÿæ‰¾ colmap æ—¶ï¼Œç¬¬ä¸€ä¸ªçœ‹åˆ°çš„å°±æ˜¯ /usr/local/bin é‡Œçš„é‚£ä¸ªå¥½ç‰ˆæœ¬
sys_path = "/usr/local/bin"
current_path = os.environ.get("PATH", "")

if sys_path not in current_path.split(os.pathsep)[0]: # å¦‚æœä¸åœ¨ç¬¬ä¸€ä½
    print(f"âš¡ [ç¯å¢ƒä¿®æ­£] å¼ºåˆ¶è®¾ç½® PATH ä¼˜å…ˆçº§: {sys_path} -> Priority High")
    os.environ["PATH"] = f"{sys_path}{os.pathsep}{current_path}"

# éªŒè¯ä¸€ä¸‹
import shutil
colmap_loc = shutil.which("colmap")
print(f"ğŸ§ [è‡ªæ£€] å½“å‰è„šæœ¬ä½¿ç”¨çš„ COLMAP è·¯å¾„: {colmap_loc}")

# è®¾ç½®æ—¥å¿—çº§åˆ«
logging.getLogger('nerfstudio').setLevel(logging.ERROR) 

# ================= ğŸ”§ ç”¨æˆ·é…ç½® (æš´åŠ›è£å‰ªç‰ˆ) =================
LINUX_WORK_ROOT = Path.home() / "braindance_workspace"
SCENE_RADIUS_SCALE = 1.8 
MAX_IMAGES =25 # ğŸ”¥ å…¨å±€æœ€å¤§å›¾ç‰‡æ•°é‡é™åˆ¶ (VGGT æ˜¾å­˜ä¼˜åŒ–)

# ================= è¾…åŠ©å·¥å…·ï¼šæ—¶é—´æ ¼å¼åŒ– =================
def format_duration(seconds):
    """å°†ç§’æ•°è½¬æ¢ä¸º HH:MM:SS æ ¼å¼"""
    return str(datetime.timedelta(seconds=int(seconds)))

# ================= è¾…åŠ©å·¥å…·ï¼šæ¨¡ç³Šå›¾ç‰‡è¿‡æ»¤ =================
def smart_filter_blurry_images(image_folder, keep_ratio=0.85, max_images=MAX_IMAGES):
    """
    å‡çº§ç‰ˆæ¸…æ´—è„šæœ¬ï¼šæ··åˆç­–ç•¥ (Hybrid Strategy)
    
    ç›®æ ‡ï¼šæ—¢è¦ç”»è´¨å¥½ï¼Œåˆè¦è§†è§’å…¨ã€‚
    
    æµç¨‹ï¼š
    1. [è´¨é‡æ¸…æ´—]ï¼šå…ˆæ— æ¡ä»¶å‰”é™¤æœ€å·®çš„ 15% (keep_ratio)ï¼Œå¹²æ‰ç»å¯¹çš„åºŸç‰‡ã€‚
    2. [å‡åŒ€é‡‡æ ·]ï¼šå¦‚æœå‰©ä¸‹çš„å¥½å›¾æ•°é‡ä¾ç„¶ > max_imagesï¼Œåˆ™æŒ‰æ—¶é—´è½´å‡åŒ€æŠ½æ ·ï¼Œ
       ç¡®ä¿è§†é¢‘çš„æ¯ä¸€æ®µéƒ½æœ‰å›¾ä¿ç•™ï¼Œé˜²æ­¢æŸä¸ªè§†è§’è¢«â€œå›¢ç­â€ã€‚
    """
    print(f"\nğŸ§  [æ™ºèƒ½æ¸…æ´—] æ­£åœ¨åˆ†æå›¾ç‰‡è´¨é‡ (æ··åˆç­–ç•¥ç‰ˆ)...")
    
    image_dir = Path(image_folder)
    images = sorted([p for p in image_dir.iterdir() if p.suffix.lower() in ['.jpg', '.jpeg', '.png']])
    
    if not images:
        print("âŒ æ²¡æ‰¾åˆ°å›¾ç‰‡")
        return

    trash_dir = image_dir.parent / "trash_smart"
    trash_dir.mkdir(exist_ok=True)

    img_scores = []

    # --- ç¬¬ä¸€æ­¥ï¼šè®¡ç®—åˆ†æ•° ---
    for i, img_path in enumerate(images):
        img = cv2.imread(str(img_path))
        if img is None: continue
        
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        h, w = gray.shape

        # ä¹å®«æ ¼è¯„åˆ†
        grid_h, grid_w = h // 3, w // 3
        max_grid_score = 0
        for r in range(3):
            for c in range(3):
                roi = gray[r*grid_h:(r+1)*grid_h, c*grid_w:(c+1)*grid_w]
                score = cv2.Laplacian(roi, cv2.CV_64F).var()
                if score > max_grid_score:
                    max_grid_score = score
        
        img_scores.append((img_path, max_grid_score))
        if i % 20 == 0:
            print(f"  -> åˆ†æä¸­... {img_path.name}: å±€éƒ¨æœ€é«˜åˆ† {max_grid_score:.1f}")

    # --- ç¬¬äºŒæ­¥ï¼šè´¨é‡æ¸…æ´— (å‰”é™¤åºŸç‰‡) ---
    scores = [s[1] for s in img_scores]
    if not scores: return

    num_total = len(scores)
    # æ— è®ºå¦‚ä½•ï¼Œå…ˆå‰”é™¤æœ€å·®çš„ (1-keep_ratio)
    quality_threshold = np.percentile(scores, (1 - keep_ratio) * 100)
    
    print(f"\nğŸ“Š ç»Ÿè®¡ç»“æœ:")
    print(f"   - å›¾ç‰‡æ€»æ•°: {num_total}")
    print(f"   - è´¨é‡é˜ˆå€¼ (Bottom {(1-keep_ratio)*100:.0f}%): {quality_threshold:.2f}")

    good_images = [] # æš‚å­˜åˆæ ¼çš„å›¾ç‰‡ (è·¯å¾„, åˆ†æ•°)
    removed_count_quality = 0

    for img_path, score in img_scores:
        if score < quality_threshold:
            # è´¨é‡å¤ªå·®ï¼Œç›´æ¥æ‰”åƒåœ¾æ¡¶
            # print(f"  âŒ [åºŸç‰‡å‰”é™¤] {img_path.name} ({score:.1f})")
            shutil.move(str(img_path), str(trash_dir / img_path.name))
            removed_count_quality += 1
        else:
            good_images.append(img_path)

    print(f"   -> ç¬¬ä¸€è½®æ¸…æ´—å®Œæˆ: å‰”é™¤ {removed_count_quality} å¼ åºŸç‰‡ï¼Œå‰©ä½™ {len(good_images)} å¼ åˆæ ¼å›¾ç‰‡ã€‚")

    # --- ç¬¬ä¸‰æ­¥ï¼šæ•°é‡æ§åˆ¶ (å‡åŒ€é‡‡æ ·) ---
    removed_count_quantity = 0
    
    if len(good_images) > max_images:
        print(f"   âš ï¸ åˆæ ¼å›¾ç‰‡ ({len(good_images)}) ä»è¶…è¿‡ä¸Šé™ ({max_images})")
        print(f"   -> æ‰§è¡Œã€å‡åŒ€é‡‡æ ·ã€‘ä»¥ä¿è¯è§†è§’è¦†ç›–...")
        
        # ç”Ÿæˆä¿ç•™ç´¢å¼•ï¼šåœ¨ 0 åˆ° len-1 ä¹‹é—´å‡åŒ€å– max_images ä¸ªç‚¹
        # ä¾‹å¦‚ï¼š[0, 2, 4, 6...]
        indices_to_keep = set(np.linspace(0, len(good_images) - 1, max_images, dtype=int))
        
        for idx, img_path in enumerate(good_images):
            if idx not in indices_to_keep:
                # è™½ç„¶è´¨é‡åˆæ ¼ï¼Œä½†ä¸ºäº†æ•°é‡é™åˆ¶ä¸å¾—ä¸åˆ 
                # print(f"  âœ‚ï¸ [å‡åŒ€é‡‡æ ·] {img_path.name} (ä¿ç•™åé¢ä¸è¶³)")
                shutil.move(str(img_path), str(trash_dir / img_path.name))
                removed_count_quantity += 1
    else:
        print(f"   âœ… åˆæ ¼å›¾ç‰‡æ•°é‡ ({len(good_images)}) æœªè¶…æ ‡ï¼Œå…¨éƒ¨ä¿ç•™ã€‚")

    total_removed = removed_count_quality + removed_count_quantity
    final_count = num_total - total_removed
    print(f"âœ¨ æ¸…æ´—ç»“æŸ: å…±ç§»é™¤ {total_removed} å¼  (åºŸç‰‡ {removed_count_quality} + é‡‡æ · {removed_count_quantity})ï¼Œæœ€ç»ˆä¿ç•™ {final_count} å¼ ã€‚")

# ğŸ”¥ å¼ºåˆ¶å¼€å¯çƒä½“åˆ‡å‰²
FORCE_SPHERICAL_CULLING = True

# ğŸ”¥ æ ¸å¿ƒå‚æ•°ï¼šä¿ç•™ç™¾åˆ†æ¯” (0.0 ~ 1.0)
# 0.5 è¡¨ç¤ºåªä¿ç•™ç¦»ä¸­å¿ƒæœ€è¿‘çš„ 50% çš„ç‚¹ (éå¸¸ç‹ )
# 0.65 è¡¨ç¤ºä¿ç•™ 65% (æ¨èï¼Œæ¯”è¾ƒå¹³è¡¡)
# 0.9 è¡¨ç¤ºä¿ç•™ 90% (åªå»æè¿œå¤„çš„èƒŒæ™¯)
KEEP_PERCENTILE = 0.9

# æ£€æŸ¥ä¾èµ–
try:
    from plyfile import PlyData, PlyElement
    HAS_PLYFILE = True
except ImportError:
    HAS_PLYFILE = False
    print("âŒ ä¸¥é‡è­¦å‘Š: æœªå®‰è£… plyfile åº“ï¼æ— æ³•æ‰§è¡Œåˆ‡å‰²ã€‚è¯·è¿è¡Œ: pip install plyfile")

# ================= æ ¸å¿ƒç®—æ³• 1: è®­ç»ƒå‚æ•°è®¡ç®— (ä¿æŒä¸å˜) =================
def analyze_and_calculate_adaptive_collider(json_path):
    print(f"\nğŸ¤– [AI åˆ†æ] è§£æç›¸æœºè½¨è¿¹...")
    try:
        with open(json_path, 'r') as f: data = json.load(f)
        frames = data["frames"]
        if not frames: return [], "unknown"

        positions = []
        forward_vectors = []
        dists_to_origin = [] 
        
        for frame in frames:
            c2w = np.array(frame["transform_matrix"])
            positions.append(c2w[:3, 3])
            forward_vectors.append(c2w[:3, :3] @ np.array([0, 0, -1]))
            dists_to_origin.append(np.linalg.norm(c2w[:3, 3]))
            
        positions = np.array(positions)
        forward_vectors = np.array(forward_vectors)
        
        center = np.mean(positions, axis=0)
        vec_to_center = center - positions
        vec_to_center /= (np.linalg.norm(vec_to_center, axis=1, keepdims=True) + 1e-6)
        ratio = np.sum(np.sum(forward_vectors * vec_to_center, axis=1) > 0) / len(frames)
        
        print(f"    -> ç›¸æœºèšåˆåº¦: {ratio:.2f}")

        # å¦‚æœå¼ºåˆ¶å¼€å¯åˆ‡å‰²ï¼Œæˆ‘ä»¬åœ¨è®­ç»ƒæ—¶ä¹Ÿå€¾å‘äºä½¿ç”¨ç‰©ä½“å‚æ•°
        is_object_mode = ratio > 0.6 or FORCE_SPHERICAL_CULLING

        if is_object_mode:
            avg_dist = np.mean(dists_to_origin)
            min_dist = np.min(dists_to_origin)
            scene_radius = 1.0 * SCENE_RADIUS_SCALE
            calc_near = max(0.05, min_dist - scene_radius)
            calc_far = avg_dist + scene_radius
            
            return ["--pipeline.model.enable-collider", "True", 
                    "--pipeline.model.collider-params", "near_plane", str(round(calc_near, 2)), 
                    "far_plane", str(round(calc_far, 2))], "object"
        else:
            return ["--pipeline.model.enable-collider", "True", 
                    "--pipeline.model.collider-params", "near_plane", "0.05", "far_plane", "100.0"], "scene"

    except Exception as e:
        return ["--pipeline.model.enable-collider", "True", 
                "--pipeline.model.collider-params", "near_plane", "0.1", "far_plane", "50.0"], "unknown"

# ================= æ ¸å¿ƒç®—æ³• 2: åŸºäºåˆ†ä½æ•°çš„æš´åŠ›åˆ‡å‰² (New!) =================
def perform_percentile_culling(ply_path, json_path, output_path):
    if not HAS_PLYFILE: 
        print("âŒ ç¼ºå°‘ plyfile åº“ï¼Œè·³è¿‡åˆ‡å‰²ã€‚")
        return False
        
    print(f"\nâœ‚ï¸ [åå¤„ç†] æ­£åœ¨æ‰§è¡Œã€åˆ†ä½æ•°æš´åŠ›åˆ‡å‰²ã€‘...")
    print(f"ğŸ”¥ ç›®æ ‡: åªä¿ç•™ç¦»åœ†å¿ƒæœ€è¿‘çš„ {KEEP_PERCENTILE*100:.0f}% ç‚¹äº‘")

    try:
        # 1. è®¡ç®—åˆ‡å‰²ä¸­å¿ƒ (ä¾ç„¶ä½¿ç”¨ç›¸æœºé‡å¿ƒï¼Œå› ä¸ºå®ƒæ˜¯è½¨é“çš„åœ†å¿ƒï¼Œæ¯”ç‚¹äº‘é‡å¿ƒæ›´ç¨³)
        with open(json_path, 'r') as f: frames = json.load(f)["frames"]
        cam_pos = np.array([np.array(f["transform_matrix"])[:3, 3] for f in frames])
        center = np.mean(cam_pos, axis=0)
        
        print(f"    -> åˆ‡å‰²åœ†å¿ƒ (ç›¸æœºé‡å¿ƒ): {center}")

        # 2. è¯»å–ç‚¹äº‘
        plydata = PlyData.read(str(ply_path))
        vertex = plydata['vertex']
        
        x, y, z = vertex['x'], vertex['y'], vertex['z']
        points = np.stack([x, y, z], axis=1)
        original_count = len(points)
        
        # 3. è®¡ç®—æ‰€æœ‰ç‚¹åˆ°ä¸­å¿ƒçš„è·ç¦»
        print("    -> æ­£åœ¨è®¡ç®—æ‰€æœ‰ç‚¹çš„è·ç¦»åˆ†å¸ƒ...")
        dists_pts = np.linalg.norm(points - center, axis=1)
        
        # 4. === æ ¸å¿ƒé€»è¾‘ï¼šè®¡ç®—åˆ†ä½æ•°é˜ˆå€¼ ===
        # æ‰¾åˆ°ä¸€ä¸ªè·ç¦» Dï¼Œä½¿å¾—æœ‰ KEEP_PERCENTILE çš„ç‚¹è·ç¦» < D
        threshold_radius = np.percentile(dists_pts, KEEP_PERCENTILE * 100)
        
        print(f"    -> ç»Ÿè®¡ç»“æœ: {KEEP_PERCENTILE*100:.0f}% çš„ç‚¹é›†ä¸­åœ¨åŠå¾„ {threshold_radius:.4f} ä»¥å†…")
        print(f"    -> æ‰§è¡Œåˆ‡å‰²: æ‰€æœ‰å¤§äº {threshold_radius:.4f} çš„ç‚¹å°†è¢«åˆ é™¤")
        
        # 5. æ‰§è¡Œåˆ‡å‰²
        opacities = 1 / (1 + np.exp(-vertex['opacity']))
        
        # é€»è¾‘ï¼šè·ç¦» < é˜ˆå€¼ AND ç‚¹æ¯”è¾ƒå®
        mask = (dists_pts < threshold_radius) & (opacities > 0.05)
        
        filtered_vertex = vertex[mask]
        new_count = len(filtered_vertex)
        
        print(f"    -> åŸå§‹ç‚¹æ•°: {original_count}")
        print(f"    -> å‰©ä½™ç‚¹æ•°: {new_count} (åˆ é™¤äº† {original_count - new_count} ä¸ªèƒŒæ™¯ç‚¹)")
        
        # 6. ä¿å­˜
        PlyData([PlyElement.describe(filtered_vertex, 'vertex')]).write(str(output_path))
        return True

    except Exception as e:
        print(f"âŒ åˆ‡å‰²å¤±è´¥è¯¦æƒ…: {e}")
        return False

# ================= VGGT æ ¸å¿ƒå¤„ç†å‡½æ•° =================
def run_vggt_pipeline(image_dir, output_sparse_dir, use_ba=False):
    """
    ä½¿ç”¨ VGGT æ›¿ä»£ COLMAP è¿›è¡Œç¨€ç–é‡å»º
    """
    print(f"ğŸš€ [VGGT] æ­£åœ¨å¯åŠ¨ç¥ç»ç½‘ç»œ SfM...")
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    dtype = torch.bfloat16 if torch.cuda.get_device_capability()[0] >= 8 else torch.float16
    
    # 1. åŠ è½½æ¨¡å‹ (ä¼šè‡ªåŠ¨ä¸‹è½½æƒé‡)
    model = VGGT()
    # å¦‚æœæ— æ³•è¿æ¥ HuggingFaceï¼Œè¯·æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹å¹¶ä¿®æ”¹æ­¤å¤„è·¯å¾„
    _URL = "https://huggingface.co/facebook/VGGT-1B/resolve/main/model.pt"
    try:
        state_dict = torch.hub.load_state_dict_from_url(_URL)
        model.load_state_dict(state_dict)
    except Exception as e:
        print(f"    âš ï¸ è‡ªåŠ¨ä¸‹è½½æ¨¡å‹å¤±è´¥: {e}")
        print("    -> è¯·ç¡®ä¿ç½‘ç»œé€šç•…æˆ–æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹ã€‚")
        raise e

    model.eval()
    model = model.to(device)
    print("    -> VGGT æ¨¡å‹åŠ è½½å®Œæˆ")

    # 2. åŠ è½½å›¾ç‰‡
    image_paths = sorted(list(Path(image_dir).glob("*")))
    if not image_paths:
        raise ValueError("VGGT è¾“å…¥ç›®å½•ä¸ºç©º")
        
    # VGGT é»˜è®¤æ¨ç†åˆ†è¾¨ç‡ 518ï¼ŒåŠ è½½åˆ†è¾¨ç‡è®¾ä¸º 1024 (ä¿æŒç»†èŠ‚)
    vggt_res = 336
    load_res = 1024 
    
    print(f"    -> æ­£åœ¨é¢„å¤„ç† {len(image_paths)} å¼ å›¾ç‰‡...")
    # images_tensor: (B, 3, H, W), coords: (B, 6) [x1, y1, x2, y2, w, h]
    images_tensor, original_coords = load_and_preprocess_images_square(image_paths, target_size=load_res)
    images_tensor = images_tensor.to(device)
    original_coords = original_coords.to(device)

    # 3. è¿è¡Œ VGGT æ¨ç†
    print("    -> æ­£åœ¨æ‰§è¡Œå‰å‘æ¨ç† (è¿™å¯èƒ½éœ€è¦å‡ ç§’é’Ÿ)...")
    images_input = F.interpolate(images_tensor, size=(vggt_res, vggt_res), mode="bilinear", align_corners=False)
    
    with torch.no_grad():
        with torch.cuda.amp.autocast(dtype=dtype):
            # æ·»åŠ  batch ç»´åº¦
            aggregated_tokens_list, ps_idx = model.aggregator(images_input[None])
            
            # é¢„æµ‹ç›¸æœº
            pose_enc = model.camera_head(aggregated_tokens_list)[-1]
            extrinsic, intrinsic = pose_encoding_to_extri_intri(pose_enc, images_input.shape[-2:])
            
            # é¢„æµ‹æ·±åº¦
            depth_map, depth_conf = model.depth_head(aggregated_tokens_list, images_input[None], ps_idx)
    
    del aggregated_tokens_list
    torch.cuda.empty_cache()

    # ç§»é™¤ batch ç»´åº¦å¹¶è½¬ä¸º numpy
    extrinsic = extrinsic.squeeze(0).cpu().numpy()
    intrinsic = intrinsic.squeeze(0).cpu().numpy()
    depth_map = depth_map.squeeze(0).cpu().numpy()
    depth_conf = depth_conf.squeeze(0).cpu().numpy()
    
    # 3. åæŠ•å½±ç”Ÿæˆ 3D ç‚¹äº‘
    print("    -> ç”Ÿæˆç¨€ç–ç‚¹äº‘...")
    # ç›´æ¥ä¼ å…¥ numpy æ•°ç»„ï¼Œä¸éœ€è¦è½¬ tensorï¼Œä¹Ÿä¸éœ€è¦å†è°ƒ .numpy()
    points_3d = unproject_depth_map_to_point_map(
        depth_map, 
        extrinsic, 
        intrinsic
    )

    # 4. è½¬æ¢ä¸º COLMAP æ ¼å¼
    print("    -> æ­£åœ¨è½¬æ¢ä¸º COLMAP æ ¼å¼...")
    
    # å‡†å¤‡ç‚¹äº‘é¢œè‰²å’Œåæ ‡ç½‘æ ¼
    points_rgb = F.interpolate(images_tensor, size=(vggt_res, vggt_res), mode="bilinear", align_corners=False)
    points_rgb = (points_rgb.cpu().numpy() * 255).astype(np.uint8).transpose(0, 2, 3, 1)
    
    num_frames, height, width, _ = points_3d.shape
    points_xyf = create_pixel_coordinate_grid(num_frames, height, width)
    
    # ğŸ”¥ [ä¿®æ”¹] åŠ¨æ€ç‚¹äº‘è¿‡æ»¤ç­–ç•¥ (é˜²æ­¢ç‚¹äº‘ä¸ºç©º)
    # ç›®æ ‡ï¼šä¿ç•™ç½®ä¿¡åº¦æœ€é«˜çš„ 10ä¸‡ä¸ªç‚¹ (è‡³å°‘ä¿ç•™ä¸€äº›ç‚¹)
    TARGET_POINTS = 100000
    
    # å°†ç½®ä¿¡åº¦å›¾å±•å¹³
    conf_flat = depth_conf.reshape(-1)
    
    # å¦‚æœæ€»åƒç´ ç‚¹å°‘äºç›®æ ‡æ•°ï¼Œåˆ™åªè¿‡æ»¤æä½ä¿¡å¿ƒçš„ç‚¹
    if conf_flat.shape[0] <= TARGET_POINTS:
        conf_threshold = 0.1
    else:
        # æ‰¾åˆ°ç¬¬ K å¤§çš„ç½®ä¿¡åº¦å€¼ä½œä¸ºé˜ˆå€¼
        # ä½¿ç”¨ np.partition å¿«é€Ÿæ‰¾åˆ° Top-K çš„åˆ†ç•Œçº¿
        # æˆ‘ä»¬å–å€’æ•°ç¬¬ TARGET_POINTS ä¸ªä½ç½®çš„å€¼
        k_idx = conf_flat.shape[0] - TARGET_POINTS
        conf_threshold = np.partition(conf_flat, k_idx)[k_idx]
        
        # ç¡®ä¿é˜ˆå€¼è‡³å°‘æ˜¯ 0.1 (è¿‡æ»¤æ‰çº¯å™ªå£°)
        conf_threshold = max(float(conf_threshold), 0.1)

    print(f"    -> åŠ¨æ€è°ƒæ•´ç½®ä¿¡åº¦é˜ˆå€¼: {conf_threshold:.4f} (ä¿ç•™ Top {TARGET_POINTS} ç‚¹)")
    
    # ç”Ÿæˆæ©ç 
    conf_mask = depth_conf >= conf_threshold
    
    points_3d_filtered = points_3d[conf_mask]
    points_xyf_filtered = points_xyf[conf_mask]
    points_rgb_filtered = points_rgb[conf_mask]
    
    # ç”Ÿæˆ PyCOLMAP é‡å»ºå¯¹è±¡
    reconstruction = batch_np_matrix_to_pycolmap_wo_track(
        points_3d_filtered,
        points_xyf_filtered,
        points_rgb_filtered,
        extrinsic,
        intrinsic,
        image_size=np.array([vggt_res, vggt_res]),
        camera_type="PINHOLE"
    )
    
    # 5. ä¿®æ­£ç›¸æœºå‚æ•° (Rescale back to original resolution)
    # è¿™ä¸€æ­¥éå¸¸é‡è¦ï¼Œå› ä¸º VGGT æ˜¯åœ¨ç¼©æ”¾åçš„æ–¹å½¢å›¾ä¸Šæ¨ç†çš„
    base_image_names = [p.name for p in image_paths]
    
    # å¼•ç”¨ demo_colmap.py ä¸­çš„ rename_colmap_recons_and_rescale_camera é€»è¾‘
    # è¿™é‡Œä¸ºäº†ç®€æ´ç›´æ¥åµŒå…¥é€»è¾‘
    for pyimageid in reconstruction.images:
        pyimage = reconstruction.images[pyimageid]
        pycamera = reconstruction.cameras[pyimage.camera_id]
        pyimage.name = base_image_names[pyimageid - 1]
        
        # è·å–åŸå§‹å°ºå¯¸ä¿¡æ¯
        # original_coords: [x1, y1, x2, y2, width, height]
        real_w = original_coords[pyimageid - 1, 4].item()
        real_h = original_coords[pyimageid - 1, 5].item()
        max_dim = max(real_w, real_h)
        
        # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹: ä» vggt_res (518) è¿˜åŸåˆ° load_res (1024) å†è¿˜åŸåˆ°åŸå§‹å°ºå¯¸
        # æ³¨æ„ï¼šload_and_preprocess_images_square åšäº†ä¸¤ä»¶äº‹ï¼špadding square å’Œ resize
        # VGGT è¾“å‡ºçš„æ˜¯åŸºäº vggt_res çš„å‚æ•°
        
        # ä¿®æ­£é€»è¾‘ï¼š
        # VGGT output (518) -> Load Res (1024) -> Original
        scale_vggt_to_load = load_res / vggt_res
        
        # load_res æ˜¯å¯¹åŸå›¾åš padding å resize å¾—åˆ°çš„
        # scale_original_to_load = load_res / max(original_w, original_h)
        scale_load_to_original = max(real_w, real_h) / load_res
        
        total_scale = scale_vggt_to_load * scale_load_to_original
        
        # ç¼©æ”¾å†…å‚ (focal, cx, cy)
        pycamera.params *= total_scale
        
        # ä¿®æ­£ä¸»ç‚¹ (Principal Point) åç§»
        # åŸå§‹é¢„å¤„ç†ä¸­å¯èƒ½æœ‰ padding (left, top)
        # padding åœ¨ load_res å°ºåº¦ä¸‹æ˜¯ï¼š
        padding_left_load = original_coords[pyimageid - 1, 0].item()
        padding_top_load = original_coords[pyimageid - 1, 1].item()
        
        # æˆ‘ä»¬éœ€è¦åœ¨è¿˜åŸåçš„å°ºåº¦ä¸Šå‡å»è¿™ä¸ª padding å¸¦æ¥çš„åç§»å—ï¼Ÿ
        # demo_colmap.py çš„é€»è¾‘æ˜¯ï¼š
        # pred_params[-2:] = real_image_size / 2 (å¼ºåˆ¶è®¾ä¸ºä¸­å¿ƒ)
        # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–å‡è®¾ï¼Œå‡è®¾ä¸»ç‚¹åœ¨ä¸­å¿ƒã€‚VGGT è®­ç»ƒæ—¶é€šå¸¸ä½¿ç”¨äº†ä¸­å¿ƒè£å‰ªã€‚
        pycamera.params[2] = real_w / 2.0
        pycamera.params[3] = real_h / 2.0
        
        pycamera.width = int(real_w)
        pycamera.height = int(real_h)
        
        # å¦‚æœæœ‰ç‚¹çš„ 2D è§‚æµ‹ï¼Œä¹Ÿéœ€è¦ shiftï¼Œä½† batch_np_matrix_to_pycolmap_wo_track 
        # ç”Ÿæˆçš„ reconstruction ç‚¹çš„ 2D åæ ‡æ˜¯åŸºäº 518 åˆ†è¾¨ç‡çš„
        # Nerfstudio è®­ç»ƒæ—¶ä¼šé‡æ–°æŠ•å½±ï¼Œæˆ–è€…æˆ‘ä»¬å¯ä»¥å¿½ç•¥ sparse model é‡Œçš„ 2D points ä½ç½®ï¼Œåªç”¨ç›¸æœºå§¿æ€
    
    # 6. ä¿å­˜
    output_sparse_dir = Path(output_sparse_dir)
    output_sparse_dir.mkdir(parents=True, exist_ok=True)
    reconstruction.write(str(output_sparse_dir))
    print(f"    -> ç»“æœå·²ä¿å­˜è‡³: {output_sparse_dir}")

# ================= ä¸»æµç¨‹ =================

def run_pipeline(video_path, project_name):
    # --- å…¨å±€è®¡æ—¶å¼€å§‹ ---
    global_start_time = time.time()
    print(f"\nğŸš€ [BrainDance Engine V13] å¯åŠ¨ä»»åŠ¡: {project_name}")
    print(f"ğŸ•’ å¼€å§‹æ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ”ª åˆ‡å‰²ç­–ç•¥: ä¿ç•™ {KEEP_PERCENTILE*100}% æœ€è¿‘ç‚¹äº‘")
    
    video_src = Path(video_path).resolve()
    work_dir = LINUX_WORK_ROOT / project_name
    data_dir = work_dir / "data"
    output_dir = work_dir / "outputs"
    # ...
    transforms_file = data_dir / "transforms.json"
    env = os.environ.copy()
    env["QT_QPA_PLATFORM"] = "offscreen" 
    
    # ğŸ”¥ [æ–°å¢] å¼ºåˆ¶ä¿®å¤ setuptools/distutils å†²çª
    env["SETUPTOOLS_USE_DISTUTILS"] = "stdlib"
    # ...

    # [Step 1] æ•°æ®å¤„ç†
    step1_start = time.time()
    
    print(f"ğŸ†• [å¼ºåˆ¶é‡ç½®] æ­£åœ¨åˆå§‹åŒ–å·¥ä½œç¯å¢ƒ...")
    if work_dir.exists(): 
        try:
            shutil.rmtree(work_dir)
        except Exception as e:
            print(f"âš ï¸ è­¦å‘Š: æ—§ç›®å½•æ¸…ç†å¤±è´¥ (å¯èƒ½è¢«å ç”¨): {e}")
    
    work_dir.mkdir(parents=True, exist_ok=True)
    data_dir.mkdir(parents=True, exist_ok=True)
    shutil.copy(str(video_src), str(work_dir / video_src.name))

    print(f"\nğŸ¥ [1/3] æ•°æ®å‡†å¤‡ (æ²™ç›’éš”ç¦»æ¨¡å¼)")
    
    # 1. å®šä¹‰ä¸¤ä¸ªéš”ç¦»åŒºåŸŸ
    # temp_dir: å­˜æ”¾ ffmpeg åŸå§‹äº§ç‰©ï¼Œå¯èƒ½åŒ…å«å‡ ç™¾å¼ å›¾
    temp_dir = work_dir / "temp_extract"
    temp_dir.mkdir(parents=True, exist_ok=True)
    
    # target_dir: æœ€ç»ˆé€ç»™ COLMAP çš„å¹²å‡€ç›®å½• (åªæ”¾ 200 å¼ )
    extracted_images_dir = work_dir / "raw_images"
    extracted_images_dir.mkdir(parents=True, exist_ok=True)
    
    # 2. FFmpeg æŠ½å¸§ (è¾“å‡ºåˆ°ä¸´æ—¶ç›®å½• temp_dir)
    print(f"    -> æ­£åœ¨æŠ½å¸§åˆ°ä¸´æ—¶ç›®å½•...")
    cap = cv2.VideoCapture(str(work_dir / video_src.name))
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    cap.release()
    
    vf_param = "fps=4"
    if width > 1920:
        vf_param = "scale=1920:-1,fps=4"
        
    try:
        subprocess.run(["ffmpeg", "-y", "-i", str(work_dir / video_src.name), 
                        "-vf", vf_param, "-q:v", "2", 
                        str(temp_dir / "frame_%05d.jpg")], check=False) 
    except Exception as e:
        print(f"    âš ï¸ FFmpeg ç»“æŸ: {e}")
    
    # 3. åœ¨ä¸´æ—¶ç›®å½•è¿›è¡Œæ¸…æ´—
    smart_filter_blurry_images(temp_dir, keep_ratio=0.85)
    
    # 4. ã€å…³é”®æ­¥éª¤ã€‘ç™½åå•å¤åˆ¶ (ä» temp -> raw_images)
    print("    -> æ­£åœ¨æ‰§è¡Œã€æ•°é‡é™åˆ¶ä¸è¿ç§»ã€‘...")
    
    # è¯»å–æ‰€æœ‰åˆæ ¼å›¾ç‰‡
    all_candidates = sorted(list(temp_dir.glob("*.jpg")) + list(temp_dir.glob("*.png")))
    total_candidates = len(all_candidates)
    # MAX_IMAGES = 200 # Already global
    
    final_images_list = []
    
    if total_candidates > MAX_IMAGES:
        print(f"    âš ï¸ å›¾ç‰‡è¿‡å¤š ({total_candidates}), æ­£åœ¨å‡åŒ€é€‰å– {MAX_IMAGES} å¼ ...")
        # å‡åŒ€é‡‡æ ·ç´¢å¼•
        indices = np.linspace(0, total_candidates - 1, MAX_IMAGES, dtype=int)
        # ä½¿ç”¨é›†åˆå»é‡ (é˜²æ­¢æç«¯æƒ…å†µ)
        indices = sorted(list(set(indices)))
        
        for idx in indices:
            final_images_list.append(all_candidates[idx])
    else:
        print(f"    âœ… å›¾ç‰‡æ•°é‡ ({total_candidates}) æœªè¶…æ ‡ï¼Œå…¨éƒ¨ä¿ç•™ã€‚")
        final_images_list = all_candidates

    # æ‰§è¡Œå¤åˆ¶ï¼šåªæŠŠé€‰ä¸­çš„æ”¾å…¥ COLMAP ç›®å½•
    for img_path in final_images_list:
        shutil.copy2(str(img_path), str(extracted_images_dir / img_path.name))
        
    print(f"    âœ… å·²å°† {len(final_images_list)} å¼ å¹²å‡€å›¾ç‰‡ç§»å…¥ COLMAP ä¸“ç”¨ç›®å½•ã€‚")
    print(f"    ğŸ§¹ æ­£åœ¨æ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
    shutil.rmtree(temp_dir) # åˆ æ‰è„åŒºï¼Œé˜²æ­¢æ··æ·†

    # =========================================================
    # ğŸš€ COLMAP å¯åŠ¨
    # =========================================================
    
    print(f"    âœ… å‡†å¤‡å¯åŠ¨ COLMAP (Linux GPU æ¨¡å¼)...")
    
    # æ•°æ®åº“è·¯å¾„
    colmap_output_dir = data_dir / "colmap"
    colmap_output_dir.mkdir(parents=True, exist_ok=True)
    database_path = colmap_output_dir / "database.db"
    
    # ç»å¯¹è·¯å¾„è°ƒç”¨
    system_colmap_exe = "/usr/local/bin/colmap" 
    
    # åŒé‡ä¿é™©ï¼šæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(system_colmap_exe):
        # shutil å·²åœ¨æ–‡ä»¶å¤´éƒ¨å¯¼å…¥ï¼Œç›´æ¥ä½¿ç”¨
        found_path = shutil.which("colmap")
        if found_path and "conda" not in found_path:
            system_colmap_exe = found_path
            print(f"    âš ï¸ è­¦å‘Š: /usr/local/bin/colmap ä¸å­˜åœ¨ï¼Œå°è¯•ä½¿ç”¨: {system_colmap_exe}")
        else:
            pass

    full_log_content = []

    def run_colmap_step(cmd, step_desc):
        print(f"\nâš¡ {step_desc}...")
        try:
            with subprocess.Popen(
                cmd, 
                stdout=subprocess.PIPE, 
                stderr=subprocess.STDOUT,
                text=True, 
                env=env,
                bufsize=1 
            ) as process:
                for line in process.stdout:
                    print(line, end='') 
                    full_log_content.append(line)
                
                process.wait()
                if process.returncode != 0:
                    raise subprocess.CalledProcessError(process.returncode, cmd)
        except Exception as e:
            print(f"\nâŒ {step_desc} æ‰§è¡Œå¼‚å¸¸: {e}")
            raise e

    # 3. æ‰‹åŠ¨è¿è¡Œ Feature Extractor (ç‰¹å¾æå–)
    # [å·²ç§»é™¤] VGGT ä¸éœ€è¦ COLMAP ç‰¹å¾æå–
    # run_colmap_step([ ... ], "[1/4] GPU ç‰¹å¾æå–")

    # 4. æ‰‹åŠ¨è¿è¡Œ Sequential Matcher (é¡ºåºåŒ¹é…)
    # [å·²ç§»é™¤] VGGT ä¸éœ€è¦ COLMAP åŒ¹é…
    # run_colmap_step([ ... ], "[2/4] GPU é¡ºåºåŒ¹é…")

    # 4.5 æ‰‹åŠ¨è¿è¡Œ Mapper (ç¨€ç–é‡å»º)
    # [å·²ç§»é™¤] VGGT ä¸éœ€è¦ COLMAP Mapper
    # run_colmap_step([ ... ], "[3/4] ç¨€ç–é‡å»º (Mapper)")

    # ä¿®æ­£è·¯å¾„å®šä¹‰
    colmap_output_dir = data_dir / "colmap"
    sparse_output_dir = colmap_output_dir / "sparse" / "0"
    
    # ğŸ”¥ğŸ”¥ğŸ”¥ æ›¿æ¢å¼€å§‹: ä½¿ç”¨ VGGT æ›¿ä»£ COLMAP ğŸ”¥ğŸ”¥ğŸ”¥
    print(f"\nâš¡ [1/4] ä½¿ç”¨ VGGT æ›¿ä»£ COLMAP è¿›è¡Œç¨€ç–é‡å»º...")
    try:
        # è¿™é‡Œçš„ extracted_images_dir æ˜¯ä½ ä¹‹å‰æ¸…æ´—å¥½çš„å›¾ç‰‡ç›®å½•
        run_vggt_pipeline(
            image_dir=extracted_images_dir,
            output_sparse_dir=sparse_output_dir,
            use_ba=False # åˆå§‹ç‰ˆæœ¬å»ºè®® Falseï¼Œé€Ÿåº¦æœ€å¿«ã€‚å¦‚æœéœ€è¦æ›´é«˜ç²¾åº¦å¯æ”¹ä¸º True (éœ€é¢å¤–ä»£ç )
        )
    except Exception as e:
        print(f"âŒ VGGT è¿è¡Œå¤±è´¥: {e}")
        return None
    # ğŸ”¥ğŸ”¥ğŸ”¥ æ›¿æ¢ç»“æŸ ğŸ”¥ğŸ”¥ğŸ”¥

    print(f"âœ… VGGT è®¡ç®—å®Œæˆï¼")

    # =========================================================
    # ğŸ”§ [3.5] ç›®å½•ç»“æ„å¼ºåŠ›ä¿®æ­£ (Auto-Fixer)
    # ç”±äº VGGT ç›´æ¥è¾“å‡ºåˆ°äº†æ­£ç¡®ä½ç½®ï¼Œå¤§éƒ¨åˆ†ä¿®æ­£é€»è¾‘å¯ä»¥è·³è¿‡
    # =========================================================
    
    # [3.6] æå‰åŒæ­¥å›¾ç‰‡ (ä¸ºäº†è®© ns-process-data èƒ½æ‰¾åˆ°)
    print(f"    -> æ­£åœ¨åŒæ­¥å›¾ç‰‡: raw_images -> data/images ...")
    dest_images_dir = data_dir / "images"
    dest_images_dir.mkdir(parents=True, exist_ok=True)
    
    valid_images = []
    for ext in ["*.jpg", "*.jpeg", "*.png", "*.JPG", "*.PNG"]:
        valid_images.extend(list(extracted_images_dir.glob(ext)))
        
    for img_path in valid_images:
        shutil.copy2(str(img_path), str(dest_images_dir / img_path.name))
    print(f"    âœ… å·²åŒæ­¥ {len(valid_images)} å¼ å›¾ç‰‡ã€‚")

    print(f"âœ… æ•°æ®å‡†å¤‡å°±ç»ªï¼æ­£åœ¨ç”Ÿæˆ transforms.json (ç”¨äºåç»­åˆ‡å‰²)...")

    # 5. è¿è¡Œ ns-process-data (ç”Ÿæˆ transforms.json)
    # ä¿®æ­£ï¼š--data æŒ‡å‘ data/imagesï¼Œ--output-dir æŒ‡å‘ data
    # è¿™æ ·å®ƒä¼šåœ¨ data/colmap æ‰¾æ¨¡å‹ï¼Œåœ¨ data/images æ‰¾å›¾ç‰‡
    run_colmap_step([
        "ns-process-data", "images", 
            "--data", str(dest_images_dir), 
            "--output-dir", str(data_dir), 
            "--verbose", 
            "--skip-colmap", 
            "--skip-image-processing", 
            "--num-downscales", "0"
    ], "[4/4] ç”Ÿæˆ transforms.json")
    # --- è´¨é‡æ£€æµ‹é€»è¾‘ ---
    full_log = "".join(full_log_content)
    
    # 1. æ£€æµ‹ "No convergence"
    if "Termination : No convergence" in full_log:
        print("\nâŒ [ä¸¥é‡é”™è¯¯] COLMAP æ— æ³•æ”¶æ•› (No convergence)ï¼")
        
        # ç”¨æˆ·è¦æ±‚ï¼šè¾“å‡ºç™¾åˆ†æ¯”è€Œä¸æ˜¯çœ‹ä¸æ‡‚çš„ px è¯¯å·®
        # å°è¯•æå–åŒ¹é…ç‡
        match_pct = re.search(r"COLMAP only found poses for (\d+\.?\d*)% of the images", full_log)
        if match_pct:
            print(f"    -> æˆåŠŸæ³¨å†Œå›¾ç‰‡æ¯”ä¾‹: {match_pct.group(1)}% (è´¨é‡è¿‡ä½)")
        else:
            # å¤‡é€‰æ–¹æ¡ˆï¼šä»æ—¥å¿—ä¸­æŠ“å–æ³¨å†Œæ•°é‡å¹¶æ‰‹åŠ¨è®¡ç®—
            # COLMAP æ—¥å¿—é€šå¸¸åŒ…å« "Registered images ... X"
            reg_match = re.findall(r"Registered images.*?(\d+)", full_log)
            if reg_match:
                # å–æœ€åä¸€ä¸ªåŒ¹é…åˆ°çš„æ•°é‡ï¼ˆå› ä¸ºå¯èƒ½æœ‰å¤šæ¬¡è¿­ä»£ï¼‰
                registered_count = int(reg_match[-1])
                ratio = (registered_count / num_images) * 100 if num_images > 0 else 0
                print(f"    -> æˆåŠŸæ³¨å†Œå›¾ç‰‡: {registered_count}/{num_images} ({ratio:.2f}%)")
            
        print("ğŸ›‘ ä»»åŠ¡å·²ç»ˆæ­¢ï¼Œå› ä¸ºç”Ÿæˆçš„ç¨€ç–ç‚¹äº‘è´¨é‡æ— æ³•æ»¡è¶³è®­ç»ƒè¦æ±‚ã€‚")
        
        # æ¸…ç† Linux ä¸´æ—¶æ–‡ä»¶
        if work_dir.exists():
            shutil.rmtree(work_dir)
            print(f"ğŸ§¹ æ¸…ç†å®Œæˆ: å·²åˆ é™¤å·¥ä½œåŒº {work_dir}")
        return None

    # 2. æ£€æµ‹åŒ¹é…ç‡è¿‡ä½
    # ç¤ºä¾‹æ—¥å¿—: COLMAP only found poses for 10.00% of the images. This is low.
    match = re.search(r"COLMAP only found poses for (\d+\.?\d*)% of the images", full_log)
    if match:
        matched_percentage = float(match.group(1))
        print(f"\nğŸ“Š COLMAP åŒ¹é…ç‡æ£€æµ‹: {matched_percentage:.2f}%")
        
        if matched_percentage < 35.0:
            print(f"âŒ [è´¨é‡è­¦å‘Š] åŒ¹é…ç‡è¿‡ä½ (< 35%)ï¼")
            print("    -> è¿™æ„å‘³ç€å¤§éƒ¨åˆ†å›¾ç‰‡æ— æ³•è¢«å®šä½ï¼Œç”Ÿæˆçš„ 3D åœºæ™¯å°†ä¸¥é‡æ®‹ç¼ºã€‚")
            print("ğŸ›‘ ä»»åŠ¡å·²ç»ˆæ­¢ã€‚å»ºè®®ï¼šå¢åŠ å›¾ç‰‡æ•°é‡ã€ä¿è¯å›¾ç‰‡æ¸…æ™°åº¦æˆ–å¢åŠ é‡å ç‡ã€‚")
            
            # æ¸…ç† Linux ä¸´æ—¶æ–‡ä»¶
            if work_dir.exists():
                shutil.rmtree(work_dir)
                print(f"ğŸ§¹ æ¸…ç†å®Œæˆ: å·²åˆ é™¤å·¥ä½œåŒº {work_dir}")
            return None

    step1_duration = time.time() - step1_start
    print(f"â±ï¸ [Step 1 å®Œæˆ] è€—æ—¶: {format_duration(step1_duration)}")

    # [Step 2] è®­ç»ƒ
    step2_start = time.time()
    search_path = output_dir / project_name / "splatfacto"
    run_dirs = sorted(list(search_path.glob("*"))) if search_path.exists() else []
    
    scene_type_detected = "unknown"

    if run_dirs:
        print(f"\nâ© [è®­ç»ƒè·³è¿‡] å·²å®Œæˆ")
        _, scene_type_detected = analyze_and_calculate_adaptive_collider(transforms_file)
    else:
        collider_args, scene_type_detected = analyze_and_calculate_adaptive_collider(transforms_file)
        print(f"\nğŸ§  [2/3] å¼€å§‹è®­ç»ƒ...")
        
        subprocess.run([
            "ns-train", "splatfacto", 
            "--data", str(data_dir), 
            "--output-dir", str(output_dir), 
            "--experiment-name", project_name, 
            "--pipeline.model.random-init", "False", 
            "--pipeline.model.cull-alpha-thresh", "0.005", 
            *collider_args,
            "--max-num-iterations", "15000", 
            "--vis", "viewer+tensorboard", 
            "--viewer.quit-on-train-completion", "True", 
            
            # ğŸ‘‡ å­å‘½ä»¤ï¼šæŒ‡å®šä½¿ç”¨ colmap æ•°æ®è§£æå™¨
            "colmap", 
            
            # ğŸ‘‡ å‚æ•°ä¿®æ­£ï¼šåªéœ€å†™çŸ­åï¼Œå¹¶ä¸”å¿…é¡»æ”¾åœ¨ "colmap" åé¢
            "--downscale-factor", "1"
        ], check=True, env=env)

    step2_duration = time.time() - step2_start
    print(f"â±ï¸ [Step 2 å®Œæˆ] è€—æ—¶: {format_duration(step2_duration)}")

    # [Step 3] å¯¼å‡º
    step3_start = time.time()
    print(f"\nğŸ’¾ [3/3] å¯¼å‡ºç»“æœ")
    if not run_dirs: run_dirs = sorted(list(search_path.glob("*")))
    if not run_dirs: return None
    latest_run = run_dirs[-1]
    
    subprocess.run([
        "ns-export", "gaussian-splat", "--load-config", str(latest_run/"config.yml"), 
        "--output-dir", str(work_dir)
    ], check=True, env=env)
    time.sleep(5) 

    # [Step 3.5] åˆ†ä½æ•°æš´åŠ›åˆ‡å‰²
    raw_ply = work_dir / "point_cloud.ply"
    if not raw_ply.exists(): raw_ply = work_dir / "splat.ply"

    cleaned_ply = work_dir / "point_cloud_cleaned.ply"
    final_ply_to_use = raw_ply

    should_clean = (scene_type_detected == "object") or FORCE_SPHERICAL_CULLING
    
    if should_clean:
        if raw_ply.exists():
            # ä½¿ç”¨æ–°çš„åˆ†ä½æ•°åˆ‡å‰²å‡½æ•°
            if perform_percentile_culling(raw_ply, transforms_file, cleaned_ply):
                print("âœ¨ æš´åŠ›åˆ‡å‰²æˆåŠŸï¼")
                final_ply_to_use = cleaned_ply
        else:
            print(f"âŒ è­¦å‘Šï¼šæœªæ‰¾åˆ° PLY æ–‡ä»¶")
    else:
        print(f"â„¹ï¸ è·³è¿‡åˆ‡å‰²")

    step3_duration = time.time() - step3_start
    print(f"â±ï¸ [Step 3 å®Œæˆ] è€—æ—¶: {format_duration(step3_duration)}")

    # [Step 4] å›ä¼ 
    print(f"\nğŸ“¦ [IO åŒæ­¥] å›ä¼ è‡³ Windows...")
    target_dir = Path(__file__).parent / "results"
    target_dir.mkdir(exist_ok=True, parents=True) 
    
    transforms_src = data_dir / "transforms.json"
    final_webgl_poses = target_dir / "webgl_poses.json"
    final_ply_dst = target_dir / f"{project_name}.ply"
    final_transforms = target_dir / "transforms.json"
    
    # --- å§¿æ€é¢„å¤„ç†é€»è¾‘ (æ¥è‡ª process_3dgs.py) ---
    if transforms_src.exists():
        print("ğŸ”„ æ­£åœ¨ç”Ÿæˆ WebGL å‹å¥½å§¿æ€æ–‡ä»¶ (webgl_poses.json)...")
        try:
            with open(transforms_src, 'r') as f:
                data = json.load(f)
            
            webgl_frames = []
            for frame in data["frames"]:
                c2w_matrix = np.array(frame["transform_matrix"], dtype=np.float32)
                # è®¡ç®— W2C (è™½ç„¶è¿™é‡Œåªå­˜äº† C2Wï¼Œä½†å¯ä»¥é¢„ç•™é€»è¾‘)
                webgl_frames.append({
                    "file_path": frame["file_path"],
                    "pose_matrix_c2w": c2w_matrix.tolist() 
                })
                
            webgl_data = {
                "camera_model": data.get("camera_model", "OPENCV"),
                "w": data.get("w", 0),
                "h": data.get("h", 0),
                "fl_x": data.get("fl_x", 0),
                "fl_y": data.get("fl_y", 0),
                "frames": webgl_frames
            }
            
            with open(final_webgl_poses, 'w') as f:
                json.dump(webgl_data, f, indent=4)
            print(f"âœ… WebGL å§¿æ€æ–‡ä»¶å·²ä¿å­˜è‡³: {final_webgl_poses.resolve()}")
        except Exception as e:
            print(f"âŒ å§¿æ€é¢„å¤„ç†å¤±è´¥: {e}")

    if final_ply_to_use and final_ply_to_use.exists():
        try:
            # 1. å¤åˆ¶æœ€ç»ˆ PLY (å¯èƒ½æ˜¯è£å‰ªè¿‡çš„)
            shutil.copy2(str(final_ply_to_use), str(final_ply_dst))
            
            # 2. é¢å¤–å›ä¼ åŸå§‹æœªè£å‰ªæ¨¡å‹ (ç”¨äºå¯¹æ¯”æˆ–å¤‡ä»½)
            final_raw_ply_dst = target_dir / f"{project_name}_raw.ply"
            if raw_ply.exists():
                shutil.copy2(str(raw_ply), str(final_raw_ply_dst))
                print(f"    -> åŸå§‹æ¨¡å‹å·²å¤‡ä»½: {final_raw_ply_dst.name}")
            
            # å¤åˆ¶ transforms.json æ–‡ä»¶
            if transforms_src.exists():
                shutil.copy2(str(transforms_src), str(final_transforms))
            
            # æ¸…ç† Linux ä¸´æ—¶æ–‡ä»¶
            shutil.rmtree(work_dir)
            print(f"ğŸ§¹ æ¸…ç†å®Œæˆ: å·²åˆ é™¤å·¥ä½œåŒº {work_dir}")
            
            # --- æœ€ç»ˆæ—¶é—´æ±‡æ€» ---
            total_time = time.time() - global_start_time
            print(f"\nâœ… =============================================")
            print(f"ğŸ‰ ä»»åŠ¡å…¨éƒ¨å®Œæˆï¼å®‰å¿ƒç¡è§‰å§ã€‚")
            print(f"ğŸ“‚ æœ€ç»ˆæ¨¡å‹: {final_ply_dst}")
            print(f"â±ï¸ æ€»å…±è€—æ—¶: {format_duration(total_time)}")
            print(f"âœ… =============================================")
            
            return str(final_ply_dst)
        except Exception as e:
            print(f"âŒ å›ä¼ å¤±è´¥: {e}")
            return None
    else:
        print("âŒ å¯¼å‡ºå¤±è´¥ï¼Œæœªæ‰¾åˆ° PLY æ–‡ä»¶ (point_cloud.ply æˆ– splat.ply)ã€‚")
        return None

if __name__ == "__main__":
    script_dir = Path(__file__).resolve().parent
    video_file = script_dir / "test.mp4" 
    if len(sys.argv) > 1: video_file = Path(sys.argv[1])

    if video_file.exists():
        run_pipeline(video_file, "scene_auto_sync")
    else:
        print(f"âŒ æ‰¾ä¸åˆ°è§†é¢‘: {video_file}")