# ==============================================================================
# å¯¼å…¥æ ‡å‡†åº“å’Œç¬¬ä¸‰æ–¹åº“
# ==============================================================================
import subprocess  # [æ ‡å‡†åº“] ç”¨äºæ‰§è¡Œå¤–éƒ¨ç³»ç»Ÿå‘½ä»¤ï¼ˆå¦‚ ffmpeg, colmapï¼‰ï¼Œå®ç° Python ä¸æ“ä½œç³»ç»Ÿçš„äº¤äº’
import sys         # [æ ‡å‡†åº“] ç”¨äºè®¿é—®ä¸ Python è§£é‡Šå™¨ç´§å¯†ç›¸å…³çš„å˜é‡å’Œå‡½æ•°ï¼Œå¦‚ sys.argv, sys.path
import shutil      # [æ ‡å‡†åº“] é«˜çº§æ–‡ä»¶æ“ä½œåº“ï¼Œç”¨äºå¤åˆ¶ã€ç§»åŠ¨ã€åˆ é™¤æ–‡ä»¶å’Œç›®å½•
import os          # [æ ‡å‡†åº“] æä¾›æ“ä½œç³»ç»Ÿæ¥å£ï¼Œç”¨äºè·¯å¾„æ“ä½œã€ç¯å¢ƒå˜é‡è·å–ç­‰
import time        # [æ ‡å‡†åº“] ç”¨äºæ—¶é—´å¤„ç†ï¼Œè®¡ç®—ä»£ç è¿è¡Œè€—æ—¶
import datetime    # [æ ‡å‡†åº“] ç”¨äºå¤„ç†æ—¥æœŸå’Œæ—¶é—´æ ¼å¼
from pathlib import Path  # [Python è¿›é˜¶] é¢å‘å¯¹è±¡çš„æ–‡ä»¶ç³»ç»Ÿè·¯å¾„åº“ï¼Œæ¯” os.path æ›´ä¼˜é›…ã€æ˜“ç”¨
import json        # [æ ‡å‡†åº“] ç”¨äºè¯»å†™ JSON æ ¼å¼æ–‡ä»¶ï¼ˆå¦‚ transforms.json ç›¸æœºå‚æ•°æ–‡ä»¶ï¼‰
import numpy as np # [ç¬¬ä¸‰æ–¹åº“] ç§‘å­¦è®¡ç®—åº“ï¼Œç”¨äºå¤„ç†çŸ©é˜µã€å›¾åƒæ•°ç»„ï¼ˆHWCæ ¼å¼ï¼‰
import logging     # [æ ‡å‡†åº“] æ—¥å¿—ç³»ç»Ÿï¼Œç”¨äºæ§åˆ¶æ§åˆ¶å°è¾“å‡ºçº§åˆ«
import cv2         # [ç¬¬ä¸‰æ–¹åº“] OpenCVï¼Œç”¨äºå›¾åƒå¤„ç†ï¼ˆè¯»å–ã€å†™å…¥ã€å½¢æ€å­¦æ“ä½œã€è½®å»“æŸ¥æ‰¾ç­‰ï¼‰
import re          # [æ ‡å‡†åº“] æ­£åˆ™è¡¨è¾¾å¼ï¼Œè™½åœ¨å¼€å¤´å¼•å…¥ä½†å‰350è¡Œæš‚æœªæ˜¾å¼ç”¨åˆ°å¤æ‚çš„æ­£åˆ™

# ================= ğŸ§  AI ä¾èµ–å¼•å…¥ =================
# [å·¥ç¨‹åŒ–æ€è·¯] è½¯ä¾èµ–å¯¼å…¥ï¼šä¸è¦å› ä¸ºç¼ºå¤±éæ ¸å¿ƒåŠŸèƒ½çš„åº“è€Œå¯¼è‡´æ•´ä¸ªç¨‹åºå´©æºƒã€‚
# è¿™é‡Œä½¿ç”¨äº† try-except å—æ¥æ£€æµ‹æ˜¯å¦å®‰è£…äº† AI ç›¸å…³çš„åº“ã€‚
try:
    import dashscope  # [ç¬¬ä¸‰æ–¹åº“] é˜¿é‡Œäº‘ç™¾ç‚¼ SDKï¼Œç”¨äºè°ƒç”¨ Qwen-VL å¤šæ¨¡æ€å¤§æ¨¡å‹
    from dashscope import MultiModalConversation # å…·ä½“å¯¼å…¥å¤šæ¨¡æ€å¯¹è¯ç±»
    from ultralytics import SAM, YOLOWorld # [ç¬¬ä¸‰æ–¹åº“] Ultralytics åº“ï¼Œå°è£…äº† YOLOï¼ˆç›®æ ‡æ£€æµ‹ï¼‰å’Œ SAMï¼ˆåˆ†å‰²ä¸‡ç‰©ï¼‰
    HAS_AI = True     # [å˜é‡] æ ‡è®°ä½ï¼Œç”¨äºåç»­é€»è¾‘åˆ¤æ–­æ˜¯å¦å¯ç”¨ AI åŠŸèƒ½
except ImportError:
    HAS_AI = False
    # [ç”¨æˆ·äº¤äº’] å‹å¥½çš„é”™è¯¯æç¤ºï¼Œå‘ŠçŸ¥ç”¨æˆ·ç¼ºå¤±äº†ä»€ä¹ˆä»¥åŠå¦‚ä½•ä¿®å¤
    print("âš ï¸ [ç¯å¢ƒè­¦å‘Š] æœªæ£€æµ‹åˆ° dashscope æˆ– ultralytics åº“ã€‚")
    print("    -> æ™ºèƒ½åˆ†å‰²åŠŸèƒ½å°†è¢«ç¦ç”¨ã€‚è¯·è¿è¡Œ: pip install dashscope ultralytics")

# ğŸ”¥ è¯·åœ¨æ­¤å¤„å¡«å…¥ä½ çš„ API KEY (æˆ–è€…ç¡®ä¿ç¯å¢ƒå˜é‡ DASHSCOPE_API_KEY å·²å­˜åœ¨)
# os.environ["DASHSCOPE_API_KEY"] = "sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"

# ================= ğŸ”§ åŸºç¡€é…ç½® =================
# [å·¥ç¨‹åŒ–æ€è·¯] ç¯å¢ƒå˜é‡ä¼˜å…ˆçº§ç®¡ç†
# å¾ˆå¤šæœåŠ¡å™¨ä¸Šç³»ç»Ÿè‡ªå¸¦çš„ colmap ç‰ˆæœ¬è¾ƒè€ï¼Œè¿™é‡Œå¼ºåˆ¶å°†ç”¨æˆ·ç¼–è¯‘çš„é«˜ç‰ˆæœ¬è·¯å¾„æåˆ° PATH ç¯å¢ƒå˜é‡çš„æœ€å‰é¢
sys_path = "/usr/local/bin" # [å˜é‡] æŒ‡å®šé«˜ä¼˜å…ˆçº§äºŒè¿›åˆ¶æ–‡ä»¶è·¯å¾„
current_path = os.environ.get("PATH", "") # è·å–å½“å‰ PATH
# æ£€æŸ¥ sys_path æ˜¯å¦å·²ç»åœ¨ PATH çš„é¦–ä½
if sys_path not in current_path.split(os.pathsep)[0]:
    print(f"âš¡ [ç¯å¢ƒä¿®æ­£] å¼ºåˆ¶è®¾ç½® PATH ä¼˜å…ˆçº§: {sys_path} -> Priority High")
    # æ‹¼æ¥æ–°çš„ PATHï¼Œå°† sys_path æ”¾åœ¨æœ€å‰é¢
    os.environ["PATH"] = f"{sys_path}{os.pathsep}{current_path}"

# è®¾ç½®æ—¥å¿—çº§åˆ«
# å±è”½ nerfstudio åº“ä¸­é Error çº§åˆ«çš„æ—¥å¿—ï¼Œé˜²æ­¢æ§åˆ¶å°è¢«åˆ·å±
logging.getLogger('nerfstudio').setLevel(logging.ERROR) 






# # å·¥ä½œåŒºé…ç½®
# # [Python è¿›é˜¶] ä½¿ç”¨ Path.home() è·å–ç”¨æˆ·ä¸»ç›®å½•ï¼Œå®ç°è·¨å¹³å°ï¼ˆWindows/Linuxï¼‰å…¼å®¹
# LINUX_WORK_ROOT = Path.home() / "braindance_workspace"
# # ğŸ”¥ æ–°å¢ï¼šè¯æ±‡æ ‘æ–‡ä»¶è·¯å¾„ï¼ŒCOLMAP è¿›è¡Œç‰¹å¾åŒ¹é…æ—¶éœ€è¦çš„é¢„è®­ç»ƒæ•°æ®
# VOCAB_TREE_PATH = LINUX_WORK_ROOT / "vocab_tree_flickr100k_words.bin" 
# SCENE_RADIUS_SCALE = 1.8  # [å˜é‡] åœºæ™¯åŠå¾„ç¼©æ”¾å› å­ï¼Œç”¨äºè®¡ç®—ç›¸æœºè¿‘å¹³é¢/è¿œå¹³é¢
# MAX_IMAGES = 180          # [å˜é‡] é™åˆ¶æœ€å¤§å¤„ç†å›¾ç‰‡æ•°é‡ï¼Œé˜²æ­¢æ˜¾å­˜çˆ†ç‚¸æˆ–è®­ç»ƒæ—¶é—´è¿‡é•¿

# # åˆ‡å‰²é…ç½®
# FORCE_SPHERICAL_CULLING = True # [å˜é‡] æ˜¯å¦å¼ºåˆ¶ä½¿ç”¨çƒå½¢è£å‰ªï¼ˆä¿ç•™ä¸­å¿ƒç‰©ä½“ï¼Œåˆ‡é™¤å‘¨å›´æ‚æ™¯ï¼‰
# KEEP_PERCENTILE = 0.9          # [å˜é‡] ä¿ç•™ 90% çš„ç‚¹äº‘å¯†åº¦ï¼Œå»é™¤ç¦»ç¾¤ç‚¹
from dataclasses import dataclass, field
from pathlib import Path
import os
import sys

@dataclass
class PipelineConfig:
    # 1. ã€å¿…å¡«é¡¹ã€‘ç”¨æˆ·åˆå§‹åŒ–æ—¶å¿…é¡»ç»™æˆ‘çš„
    project_name: str
    video_path: Path
    
    # 2. ã€é€‰å¡«é¡¹ã€‘æœ‰é»˜è®¤å€¼çš„é…ç½® (å¯¹åº”ä½ åŸä»£ç çš„å…¨å±€å˜é‡)
    work_root: Path = Path.home() / "braindance_workspace"
    max_images: int = 180
    force_spherical_culling: bool = True 
    scene_radius_scale: float = 1.8
    keep_percentile: float = 0.9
    enable_ai: bool = True  # æ–°å¢æ§åˆ¶å¼€å…³
    
    # 3. ã€è‡ªåŠ¨è®¡ç®—é¡¹ã€‘ç”¨æˆ·ä¸ç”¨ä¼ ï¼Œæˆ‘è‡ªå·±ç®—å‡ºæ¥çš„è·¯å¾„
    # field(init=False) çš„æ„æ€æ˜¯ï¼šè¿™ä¸ªå˜é‡å­˜åœ¨ï¼Œä½†åœ¨åˆå§‹åŒ–(__init__)æ—¶ä¸éœ€è¦ä½œä¸ºå‚æ•°ä¼ å…¥
    project_dir: Path = field(init=False)
    data_dir: Path = field(init=False)
    images_dir: Path = field(init=False)
    masks_dir: Path = field(init=False)
    transforms_file: Path = field(init=False)
    vocab_tree_path: Path = field(init=False)

    def __post_init__(self):
        """
        è¿™ä¸ªå‡½æ•°ä¼šåœ¨ç±»åˆå§‹åŒ–å®Œæˆä¹‹åï¼Œè‡ªåŠ¨æ‰§è¡Œï¼
        æˆ‘ä»¬åœ¨è¿™é‡Œé›†ä¸­å¤„ç†æ‰€æœ‰çš„è·¯å¾„æ‹¼æ¥å’Œç¯å¢ƒè®¾ç½®ã€‚
        """
        # --- A. è‡ªåŠ¨è®¡ç®—è·¯å¾„ (å†ä¹Ÿä¸ç”¨åœ¨ä¸»å‡½æ•°é‡Œå†™ä¸€éäº†) ---
        self.project_dir = self.work_root / self.project_name
        self.data_dir = self.project_dir / "data"
        self.images_dir = self.data_dir / "images"
        self.masks_dir = self.data_dir / "masks"
        self.transforms_file = self.data_dir / "transforms.json"
        
        # è¯æ±‡æ ‘è·¯å¾„ (å¯¹åº”åŸä»£ç  VOCAB_TREE_PATH)
        self.vocab_tree_path = self.work_root / "vocab_tree_flickr100k_words.bin"

        # --- B. ç¯å¢ƒä¿®æ­£ (å¯¹åº”åŸä»£ç çš„ PATH è®¾ç½®é€»è¾‘) ---
        # æŠŠè®¾ç½®ç¯å¢ƒå˜é‡çš„é€»è¾‘æ¬åˆ°è¿™é‡Œï¼Œä¿è¯ config ä¸€åŠ è½½ï¼Œç¯å¢ƒå°±æ˜¯å¯¹çš„
        sys_path = "/usr/local/bin"
        current_path = os.environ.get("PATH", "")
        if sys_path not in current_path.split(os.pathsep)[0]:
            print(f"âš¡ [Config] è‡ªåŠ¨ä¼˜åŒ– PATH ä¼˜å…ˆçº§: {sys_path}")
            os.environ["PATH"] = f"{sys_path}{os.pathsep}{current_path}"
            
        # è®¾ç½® Setuptools ä¿®å¤ (å¯¹åº”åŸä»£ç  env["SETUPTOOLS_USE_DISTUTILS"])
        os.environ["SETUPTOOLS_USE_DISTUTILS"] = "stdlib"






# æ£€æŸ¥ plyfile åº“
# plyfile ç”¨äºè¯»å†™ .ply ç‚¹äº‘æ–‡ä»¶
try:
    from plyfile import PlyData, PlyElement
    HAS_PLYFILE = True
except ImportError:
    HAS_PLYFILE = False

# ================= ğŸ§  AI æ ¸å¿ƒé€»è¾‘å‡½æ•° =================

def get_central_object_prompt(images_dir: Path, sample_count=3):
    """
    [Step 1.1] ä½¿ç”¨ Qwen-VL-Plus å¤šå›¾åˆ†æï¼Œæå–ä¸­å¿ƒç‰©ä½“çš„æ–‡æœ¬æè¿°
    
    å‚æ•°:
        images_dir (Path): å›¾ç‰‡æ–‡ä»¶å¤¹è·¯å¾„
        sample_count (int): é‡‡æ ·å›¾ç‰‡æ•°é‡ï¼Œé»˜è®¤3å¼ ï¼ŒèŠ‚çœ Token å¹¶åŠ å¿«é€Ÿåº¦
    
    è¿”å›:
        prompt_text (str): å¤§æ¨¡å‹ç”Ÿæˆçš„ç‰©ä½“æè¿°æç¤ºè¯
    """
    # è·å– API Key
    api_key = os.environ.get("DASHSCOPE_API_KEY")
    if not api_key:
        print("âŒ æœªè®¾ç½® DASHSCOPE_API_KEYï¼Œæ— æ³•è°ƒç”¨å¤§æ¨¡å‹ã€‚")
        return None

    print(f"\nğŸ§  [AI åˆ†æ] æ­£åœ¨è°ƒç”¨ Qwen-VL-Plus åˆ†æåœºæ™¯...")
    
    # [Python è¿›é˜¶] ä½¿ç”¨ glob è·å–æ‰€æœ‰ jpg/png å›¾ç‰‡ï¼Œå¹¶æ’åºç¡®ä¿é¡ºåºä¸€è‡´
    image_files = sorted(list(images_dir.glob("*.jpg")) + list(images_dir.glob("*.png")))
    if not image_files: return None
    
    # [ç®—æ³•é€»è¾‘] å‡åŒ€é‡‡æ ·ï¼šä½¿ç”¨ numpy çš„ linspace åœ¨å›¾ç‰‡åºåˆ—ä¸­å‡åŒ€æŠ½å– sample_count å¼ å›¾
    # è¿™æ ·èƒ½è¦†ç›–ç‰©ä½“çš„ä¸åŒè§’åº¦ï¼Œæ¯”åªå–å‰ä¸‰å¼ æ›´ç¨³å¥
    indices = np.linspace(0, len(image_files) - 1, sample_count, dtype=int)
    sampled_imgs = [image_files[i] for i in indices]
    
    # æ„å»ºå¤šæ¨¡æ€æ¶ˆæ¯ä½“ (Dashscope SDK è¦æ±‚çš„æ ¼å¼)
    content = [{"image": str(img_path)} for img_path in sampled_imgs]
    content.append({
        "text": (
            "è¿™äº›æ˜¯ä¸€ä¸ªè§†é¢‘çš„æŠ½å¸§å›¾ç‰‡ã€‚è¯·åˆ†æç”»é¢ä¸­å¿ƒå§‹ç»ˆå­˜åœ¨çš„ã€æœ€ä¸»è¦çš„ä¸€ä¸ªç‰©ä½“æ˜¯ä»€ä¹ˆã€‚"
            "è¯·è¾“å‡ºä¸€ä¸ªé€‚åˆç”¨äºç‰©ä½“æ£€æµ‹æ¨¡å‹çš„è‹±æ–‡åè¯çŸ­è¯­ï¼ˆPromptï¼‰ã€‚"
            "âš ï¸ å…³é”®ç­–ç•¥ï¼šè¯·ä¼˜å…ˆæè¿°ã€è§†è§‰ç‰¹å¾ã€‘ï¼ˆé¢œè‰²ã€æè´¨ã€å½¢çŠ¶ï¼‰ï¼Œè€Œä¸æ˜¯ã€åŠŸèƒ½åç§°ã€‘ã€‚"
            "è¶Šç®€å•ã€è¶Š'åœŸ'çš„è¯ï¼Œæ£€æµ‹æ¨¡å‹è¶Šå®¹æ˜“è¯†åˆ«ã€‚"
            "ä¾‹å¦‚ï¼š"
            " - ä¸è¦è¯´ 'electric shaver' (ç”µåŠ¨å‰ƒé¡»åˆ€)ï¼Œè¯·è¯´ 'gray metal object' æˆ– 'device'ã€‚"
            " - ä¸è¦è¯´ 'portable charger' (å……ç”µå®)ï¼Œè¯·è¯´ 'white rectangular box'ã€‚"
            "è¦æ±‚ï¼šä¸¥æ ¼åªè¾“å‡ºè¿™ä¸ªè‹±æ–‡çŸ­è¯­ï¼Œä¸è¦åŒ…å«ä»»ä½•æ ‡ç‚¹ç¬¦å·ã€è§£é‡Šã€‚"
        )
    })
    
    # å°è£…ç”¨æˆ·æ¶ˆæ¯
    messages = [{"role": "user", "content": content}]

    try:
        # è°ƒç”¨é˜¿é‡Œäº‘ Qwen-VL-Plus æ¨¡å‹
        response = dashscope.MultiModalConversation.call(
            model='qwen-vl-plus', 
            messages=messages
        )
        
        # è§£æè¿”å›ç»“æœ
        if response.status_code == 200:
            # æå–æ–‡æœ¬å†…å®¹
            prompt_text = response.output.choices[0].message.content[0]["text"].strip()
            # [æ•°æ®æ¸…æ´—] å»æ‰å¯èƒ½å­˜åœ¨çš„æ ‡ç‚¹ç¬¦å·ï¼Œé˜²æ­¢å¹²æ‰° YOLO
            prompt_text = prompt_text.replace(".", "").replace('"', "").replace("'", "")
            # \033[92m æ˜¯ ANSI è½¬ä¹‰ç ï¼Œç”¨äºåœ¨æ§åˆ¶å°è¾“å‡ºç»¿è‰²æ–‡å­—
            print(f"    ğŸ¤– Qwen è®¤ä¸ºä¸­å¿ƒç‰©ä½“æ˜¯: [ \033[92m{prompt_text}\033[0m ]")
            return prompt_text
        else:
            print(f"âŒ Qwen è°ƒç”¨å¤±è´¥: {response.code} - {response.message}")
            return None
    except Exception as e:
        print(f"âŒ API è¿æ¥å¼‚å¸¸: {e}")
        return None

def clean_and_verify_mask(mask, img_name=""):
    """
    [å‡€åŒ–ç‰ˆ] Mask åå¤„ç†æ ¸å¿ƒç®—æ³•
    åŠŸèƒ½ï¼š
    1. å¼ºåˆ¶æ¸…æ´—ï¼šåªä¿ç•™ç”»é¢ä¸­æœ€å¤§çš„è¿é€šå— (å»é™¤å­¤ç«‹å™ªç‚¹)ã€‚
    2. ä¸¥æ ¼è´¨æ£€ï¼šæ¸…æ´—åå¦‚æœå½¢çŠ¶ä¾ç„¶æ¯›ç³™(ç²˜è¿é˜´å½±)ï¼Œåˆ™å‰”é™¤ã€‚
    3. è¾¹ç¼˜è…èš€ï¼šå‘å†…æ”¶ç¼© Maskï¼Œå»é™¤è¾¹ç¼˜æ‚è‰²ã€‚
    
    å‚æ•°:
        mask (numpy array): å•é€šé“äºŒå€¼å›¾åƒ (0æˆ–255)
        img_name (str): ç”¨äºæ—¥å¿—è¾“å‡ºçš„æ–‡ä»¶å
        
    è¿”å›:
        tuple: (æ˜¯å¦åˆæ ¼ bool, æ¸…æ´—åçš„å¹²å‡€Mask, åŸå›  str)
    """
    h, w = mask.shape # è·å–å›¾åƒé«˜å®½
    
    # --- 1. è¿é€šåŸŸåˆ†æ & å¼ºåˆ¶æ¸…æ´— (Cleaning) ---
    # [ç®—æ³•é€»è¾‘] è¿é€šç»„ä»¶åˆ†æ (Connected Components)
    # è¿™é‡Œçš„ connectivity=8 è¡¨ç¤ºåˆ¤æ–­åƒç´ ç›¸è¿æ—¶è€ƒè™‘å‘¨å›´8ä¸ªæ–¹å‘
    # stats åŒ…å«æ¯ä¸ªè¿é€šå—çš„ [å·¦ä¸Šè§’x, å·¦ä¸Šè§’y, å®½, é«˜, é¢ç§¯]
    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(mask, connectivity=8)
    
    # num_labels è‡³å°‘ä¸º 2 (èƒŒæ™¯0 + è‡³å°‘ä¸€ä¸ªå‰æ™¯å—)ï¼Œå¦‚æœå°äº2è¯´æ˜å…¨æ˜¯é»‘çš„
    if num_labels < 2: 
        return False, None, "ç©ºè’™ç‰ˆ"

    # [ç®—æ³•é€»è¾‘] å¯»æ‰¾æœ€å¤§å‰æ™¯å—
    # éå†æ‰€æœ‰æ ‡ç­¾ï¼ˆä»1å¼€å§‹ï¼Œè·³è¿‡0èƒŒæ™¯ï¼‰ï¼Œæ‰¾åˆ°é¢ç§¯æœ€å¤§çš„é‚£ä¸ª
    max_area = 0
    max_label = -1
    for i in range(1, num_labels):
        if stats[i, cv2.CC_STAT_AREA] > max_area:
            max_area = stats[i, cv2.CC_STAT_AREA]
            max_label = i
            
    # [å·¥ç¨‹åŒ–æ€è·¯] é˜ˆå€¼è¿‡æ»¤ï¼šå¦‚æœæœ€å¤§çš„å—å æ¯”ä¸åˆ°å…¨å›¾çš„ 0.5%ï¼Œé€šå¸¸æ˜¯å™ªç‚¹
    if max_area < (h * w * 0.005):
        return False, None, "ä¸»ä½“è¿‡å°ï¼Œç–‘ä¼¼å™ªç‚¹"

    # ğŸ”¥ æ ¸å¿ƒæ“ä½œï¼šé‡æ„ Mask
    # åªä¿ç•™ label ç­‰äº max_label çš„åƒç´ ï¼Œå…¶ä½™ç½®ä¸º 0ã€‚
    # è¿™æ­¥æ“ä½œèƒ½å®Œç¾å»é™¤å‘¨å›´çš„é£æº…å™ªç‚¹ã€‚
    cleaned_mask = (labels == max_label).astype(np.uint8) * 255

    # --- 2. å¯¹æ¸…æ´—åçš„ Mask è¿›è¡Œâ€œä½“æ£€â€ (Verification) ---
    
    # [ç®—æ³•é€»è¾‘] è½®å»“æå–
    # RETR_EXTERNAL åªå–æœ€å¤–å±‚è½®å»“
    contours, _ = cv2.findContours(cleaned_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if not contours: return False, None, "æ¸…æ´—åæ— è½®å»“"
    
    # å–æœ€å¤§è½®å»“
    main_cnt = max(contours, key=cv2.contourArea)
    
    # [ç®—æ³•é€»è¾‘] å®å¿ƒåº¦ (Solidity) è®¡ç®—
    # å‡¸åŒ… (Convex Hull) åƒæ˜¯ç”¨æ©¡çš®ç­‹åŒ…ä½ç‰©ä½“çš„å½¢çŠ¶ã€‚
    # å®å¿ƒåº¦ = è½®å»“é¢ç§¯ / å‡¸åŒ…é¢ç§¯ã€‚
    # æ­£å¸¸ç‰©ä½“å®å¿ƒåº¦é«˜ (~0.95)ï¼Œå¦‚æœæœ‰ç²˜è¿é˜´å½±ï¼Œè½®å»“ä¼šå¾ˆä¸è§„åˆ™ï¼Œå®å¿ƒåº¦ä¼šé™ä½ã€‚
    hull = cv2.convexHull(main_cnt)
    hull_area = cv2.contourArea(hull)
    if hull_area == 0: return False, None, "å‡¸åŒ…é¢ç§¯ä¸º0"
    
    solidity = max_area / hull_area
    
    # é˜ˆå€¼è®¾å®šï¼š0.88 (ç»éªŒå€¼ï¼Œä½äºæ­¤å€¼é€šå¸¸æ„å‘³ç€è¾¹ç¼˜éå¸¸æ¯›ç³™æˆ–æœ‰ç²˜è¿)
    if solidity < 0.88:
        return False, None, f"è¾¹ç¼˜ä¸¥é‡æ¯›ç³™/ç²˜è¿é˜´å½± (å®å¿ƒåº¦ {solidity:.2f})"

    # [ç®—æ³•é€»è¾‘] é•¿å®½æ¯”æ£€æŸ¥ (Aspect Ratio)
    # é˜²æ­¢æŠŠé•¿æ¡å½¢çš„æ¡Œå­ç¼éš™ã€å¢™è§’çº¿å½“æˆç‰©ä½“
    x, y, w_rect, h_rect = cv2.boundingRect(main_cnt)
    aspect_ratio = w_rect / h_rect
    if aspect_ratio > 4.5: # å…è®¸ä¸€å®šç¨‹åº¦çš„é•¿æ¡ï¼Œä½†è¶…è¿‡ 4.5 å€å°±å¤ªå¤¸å¼ äº†
        return False, None, f"å½¢çŠ¶å¼‚å¸¸ (é•¿å®½æ¯” {aspect_ratio:.1f})"

    # ğŸ”¥ æ–°å¢ï¼šè¾¹ç¼˜è…èš€ (Erosion)
    # [ç®—æ³•é€»è¾‘] è…èš€æ“ä½œ
    # å·ç§¯æ ¸ kernel åœ¨å›¾åƒä¸Šæ»‘åŠ¨ï¼Œåªæœ‰æ ¸å†…å…¨ä¸º 255 æ—¶æ‰ä¿ç•™ä¸­å¿ƒç‚¹ã€‚
    # æ•ˆæœæ˜¯è®©ç™½è‰²åŒºåŸŸå‘å†…æ”¶ç¼©ï¼Œåˆ‡æ‰ç‰©ä½“è¾¹ç¼˜å¯èƒ½å­˜åœ¨çš„â€œå…‰æ™•â€æˆ–èƒŒæ™¯æ‚è‰²ã€‚
    kernel_size = 3  # 3x3 çš„æ ¸ï¼Œå¤§çº¦æ”¶ç¼© 1 åƒç´ 
    kernel = np.ones((kernel_size, kernel_size), np.uint8)
    cleaned_mask = cv2.erode(cleaned_mask, kernel, iterations=1)
    
    return True, cleaned_mask, "åˆæ ¼"

def get_salient_box(img_path, margin_ratio=0.1):
    """
    [çº¯æœ¬åœ° CV ç®—æ³•] å½“ AI å¤±è´¥æ—¶ï¼Œä½¿ç”¨ä¼ ç»Ÿè§†è§‰ç®—æ³•è®¡ç®—'è§†è§‰æ˜¾è‘—åŒºåŸŸ'ã€‚
    åŸç†ï¼šåˆ©ç”¨æ‹‰æ™®æ‹‰æ–¯ç®—å­æ‰¾è¾¹ç¼˜ -> è†¨èƒ€è¿æ¥ -> æ‰¾æœ€å¤§å¤–æ¥çŸ©å½¢
    
    å‚æ•°:
        img_path: å›¾ç‰‡è·¯å¾„
        margin_ratio: ç»“æœæ¡†çš„æ‰©è¾¹æ¯”ä¾‹ (padding)
    """
    try:
        img = cv2.imread(str(img_path))
        if img is None: return None
        
        # 1. è½¬ç°åº¦å¹¶è®¡ç®—è¾¹ç¼˜ (Laplacian)
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        # [ç®—æ³•é€»è¾‘] æ‹‰æ™®æ‹‰æ–¯ç®—å­è®¡ç®—äºŒé˜¶å¯¼æ•°ï¼Œå¯¹è¾¹ç¼˜æå…¶æ•æ„Ÿ
        # CV_64F å…è®¸è´Ÿå€¼ï¼Œé˜²æ­¢æˆªæ–­
        laplacian = cv2.Laplacian(gray, cv2.CV_64F)
        laplacian = np.uint8(np.absolute(laplacian)) # å–ç»å¯¹å€¼è½¬å› uint8
        
        # 2. æ¨¡ç³Šä¸äºŒå€¼åŒ–
        # [ç®—æ³•é€»è¾‘] é«˜æ–¯æ¨¡ç³Šç”¨äºå¹³æ»‘çº¹ç†ï¼Œè®©é›¶æ•£çš„è¾¹ç¼˜èšé›†
        blurred = cv2.GaussianBlur(laplacian, (25, 25), 0)
        # [ç®—æ³•é€»è¾‘] åŠ¨æ€é˜ˆå€¼ï¼šåªä¿ç•™äº®åº¦å‰ 20% çš„åŒºåŸŸï¼ˆå³çº¹ç†æœ€ä¸°å¯Œçš„åœ°æ–¹ï¼‰
        threshold_val = np.percentile(blurred, 80) 
        _, binary = cv2.threshold(blurred, threshold_val, 255, cv2.THRESH_BINARY)
        
        # 3. æ‰¾æœ€å¤§è½®å»“
        contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        if not contours: return None
        
        # å‡è®¾çº¹ç†æœ€å¤æ‚çš„åŒºåŸŸå°±æ˜¯ä¸»ä½“
        max_cnt = max(contours, key=cv2.contourArea)
        x, y, w, h = cv2.boundingRect(max_cnt)
        
        # 4. åŠ ä¸Šå®‰å…¨è¾¹è· (Padding)
        H, W = img.shape[:2]
        pad_x = int(w * margin_ratio)
        pad_y = int(h * margin_ratio)
        
        # é™åˆ¶åæ ‡ä¸è¶…å‡ºå›¾åƒè¾¹ç•Œ
        x1 = max(0, x - pad_x)
        y1 = max(0, y - pad_y)
        x2 = min(W, x + w + pad_x)
        y2 = min(H, y + h + pad_y)
        
        # è¿”å› torch tensor æ ¼å¼ï¼Œé€‚é… YOLO/SAM çš„è¾“å…¥è¦æ±‚
        import torch
        return torch.tensor([[x1, y1, x2, y2]], dtype=torch.float32)
        
    except Exception as e:
        print(f"       âš ï¸ è§†è§‰é‡å¿ƒè®¡ç®—å¤±è´¥: {e}")
        return None

def run_ai_segmentation_pipeline(data_dir: Path):
    """
    [Step 1.2] æ‰§è¡Œ AI åˆ†å‰²æ€»æµæ°´çº¿
    é€»è¾‘æµç¨‹ï¼š
    1. å°è¯•ç”¨ Qwen åˆ†æç‰©ä½“ -> å¾—åˆ° Prompt
    2. åŠ è½½ YOLO å’Œ SAM æ¨¡å‹
    3. éå†æ¯ä¸€å¼ å›¾ï¼š
        a. YOLO æ ¹æ® Prompt æ‰¾æ¡†
        b. å¦‚æœæœ‰å¤šä¸ªæ¡†ï¼Œé€‰æœ€ä¸­å¿ƒçš„
        c. å¦‚æœæ²¡æ¡†ï¼Œç”¨ SAM ä¸­å¿ƒç‚¹æ¨¡å¼
        d. SAM ç”Ÿæˆ Mask
        e. clean_and_verify_mask æ¸…æ´— Mask
        f. å¦‚æœåˆæ ¼ -> ç”Ÿæˆé€æ˜ PNGï¼Œæ›´æ–° transforms.json
        g. å¦‚æœä¸åˆæ ¼ -> åˆ é™¤å›¾ç‰‡
    """
    if not HAS_AI: return False # å¦‚æœç¼ºå°‘åº“ï¼Œç›´æ¥è·³è¿‡
    
    # è·¯å¾„å®šä¹‰
    images_dir = data_dir / "images"
    masks_dir = data_dir / "masks"
    cfg.transforms_file = data_dir / "transforms.json" # COLMAP ç”Ÿæˆçš„ç›¸æœºä½å§¿æ–‡ä»¶

    if not cfg.transforms_file.exists():
        print("âš ï¸ æœªæ‰¾åˆ° transforms.jsonï¼Œæ— æ³•è¿›è¡Œ Mask å¤„ç†ã€‚")
        return False

    # ================= æ ¸å¿ƒä¿®æ”¹é€»è¾‘å¼€å§‹ =================
    print(f"\nâœ‚ï¸ [AI åˆ†å‰²] æ­£åœ¨åˆå§‹åŒ–...")

    # --- ç¬¬ä¸€å±‚ï¼šå°è¯•è°ƒç”¨å¤§æ¨¡å‹è·å–ç²¾å‡† Prompt ---
    text_prompt = None
    try:
        # è°ƒç”¨ä¹‹å‰å®šä¹‰çš„å‡½æ•°
        text_prompt = get_central_object_prompt(images_dir)
    except Exception as e:
        print(f"    âš ï¸ å¤§æ¨¡å‹è°ƒç”¨å‡ºé”™: {e}")

    # --- ç¬¬äºŒå±‚ï¼šå¦‚æœå¤§æ¨¡å‹å¤±è´¥ï¼Œä½¿ç”¨é€šç”¨ Prompt ---
    if not text_prompt:
        # [å·¥ç¨‹åŒ–æ€è·¯] é™çº§ç­–ç•¥ (Fallback)
        # å¦‚æœå¤§æ¨¡å‹æŒ‚äº†ï¼Œæˆ–è€… API æ²¡é’±äº†ï¼Œä½¿ç”¨é€šç”¨è¯ "central object"
        text_prompt = "central object; single object"
        print(f"    âš ï¸ æœªèƒ½è·å–ç²¾å‡†æè¿°ï¼Œé™çº§ä½¿ç”¨é€šç”¨ Prompt: '{text_prompt}'")
    else:
        print(f"    ğŸ¯ è·å–åˆ°ç²¾å‡† Prompt: '\033[92m{text_prompt}\033[0m'")

    masks_dir.mkdir(parents=True, exist_ok=True) # åˆ›å»º Mask ç›®å½•
    # ================= æ ¸å¿ƒä¿®æ”¹é€»è¾‘ç»“æŸ =================

    # 2. åŠ è½½æ¨¡å‹ (æ¨èç”¨ Large)
    print("    -> æ­£åœ¨åŠ è½½ SAM 2 Large æ¨¡å‹...")
    
    # ğŸ”¥ è‡ªåŠ¨è¿ç§» AI æ¨¡å‹æ–‡ä»¶
    # [å·¥ç¨‹åŒ–æ€è·¯] è‡ªåŠ¨éƒ¨ç½²ï¼šè„šæœ¬è¿è¡Œæ—¶æ£€æŸ¥å½“å‰ç›®å½•æ˜¯å¦æœ‰æ¨¡å‹æ–‡ä»¶ï¼Œå¦‚æœæœ‰åˆ™å¤åˆ¶åˆ°å·¥ä½œåŒº
    model_files = ["yolov8s-worldv2.pt", "sam2.1_l.pt"]
    for model_name in model_files:
        target_model_path = cfg.work_root / model_name
        local_model_path = Path(__file__).parent / model_name # è„šæœ¬æ‰€åœ¨ç›®å½•
        
        if not target_model_path.exists():
            if local_model_path.exists():
                print(f"    ğŸ“¦ æ£€æµ‹åˆ°æœ¬åœ°æ¨¡å‹ {model_name}ï¼Œæ­£åœ¨è¿ç§»è‡³å·¥ä½œåŒº...")
                shutil.copy2(str(local_model_path), str(target_model_path))
            else:
                # å¦‚æœéƒ½æ²¡æœ‰ï¼ŒUltralytics åº“ä¼šåœ¨è°ƒç”¨æ—¶è‡ªåŠ¨ä¸‹è½½
                print(f"    âš ï¸ æœªåœ¨è„šæœ¬ç›®å½•æ‰¾åˆ° {model_name}ï¼Œå°†å°è¯•è‡ªåŠ¨ä¸‹è½½...")

    try:
        # å®šä¹‰æ¨¡å‹è·¯å¾„
        yolo_path = cfg.work_root / "yolov8s-worldv2.pt"
        sam_path = cfg.work_root / "sam2.1_l.pt"
        
        # YOLO-World: å¼€æ”¾è¯æ±‡æ£€æµ‹æ¨¡å‹ï¼Œèƒ½â€œå¬æ‡‚â€æ–‡å­—å¹¶æ‰¾åˆ°æ¡†
        det_model = YOLOWorld(str(yolo_path) if yolo_path.exists() else "yolov8s-worldv2.pt") 
        # è®¾ç½® YOLO éœ€è¦å¯»æ‰¾çš„ç±»åˆ«
        det_model.set_classes([text_prompt])
        
        # SAM 2: åˆ†å‰²æ¨¡å‹ï¼Œæ ¹æ®æ¡†ï¼ˆBox Promptï¼‰æˆ–ç‚¹ï¼ˆPoint Promptï¼‰æŠ å›¾
        sam_model = SAM(str(sam_path) if sam_path.exists() else "sam2.1_l.pt") 
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return False

    # 3. è¯»å– transforms.json
    # [Python è¿›é˜¶] è¯»å– JSON åˆ°å­—å…¸
    with open(cfg.transforms_file, 'r') as f:
        meta = json.load(f)
    
    # [Python è¿›é˜¶] å»ºç«‹å“ˆå¸Œæ˜ å°„ (HashMap / Dict)
    # å°†æ–‡ä»¶åæ˜ å°„åˆ°å¸§æ•°æ®å¯¹è±¡ï¼Œåç»­æŸ¥æ‰¾å¤æ‚åº¦ä¸º O(1)ï¼Œé¿å…éå†åˆ—è¡¨
    # Path(f["file_path"]).name æå–å¦‚ "frame_0001.jpg"
    frames_map = {Path(f["file_path"]).name: f for f in meta["frames"]}
    
    image_files = sorted(list(images_dir.glob("*.jpg")) + list(images_dir.glob("*.png")))
    total_imgs = len(image_files)
    
    valid_frames_list = [] # å­˜æ”¾æ¸…æ´—åˆæ ¼åçš„å¸§æ•°æ®
    deleted_count = 0
    
    print(f"    -> å¼€å§‹å¤„ç† {total_imgs} å¼ å›¾ç‰‡...")

    # [Python è¿›é˜¶] enumerate ç”¨äºåŒæ—¶è·å–ç´¢å¼• i å’Œå…ƒç´  img_path
    for i, img_path in enumerate(image_files):
        # --- A. æ£€æµ‹ä¸åˆ†å‰² ---
        try:
            # 1. YOLO æ£€æµ‹
            # conf=0.05: åªè¦ç½®ä¿¡åº¦å¤§äº 5% å°±è®¤ä¸ºå¯èƒ½æœ‰ä¸œè¥¿
            det_results = det_model.predict(img_path, conf=0.05, verbose=False)
            
            # ============================================================
            # ğŸ•µï¸â€â™‚ï¸ [DEBUG æ¨¡å¼] è°ƒè¯•å¯è§†åŒ–ä»£ç å—
            # ============================================================
            debug_dir = data_dir / "debug_yolo_visuals"
            debug_dir.mkdir(parents=True, exist_ok=True)
            
            num_boxes = len(det_results[0].boxes)
            
            if num_boxes > 0:
                plotted_img = det_results[0].plot() # YOLO è‡ªå¸¦ç”»å›¾åŠŸèƒ½
                debug_path = debug_dir / f"debug_{img_path.name}"
                cv2.imwrite(str(debug_path), plotted_img)
                
                if i < 3: # åªæ‰“å°å‰3å¼ ï¼Œé˜²æ­¢åˆ·å±
                    print(f"\n    ğŸ‘€ [DEBUG] {img_path.name}: æ‰¾åˆ°äº† {num_boxes} ä¸ªç›®æ ‡")
            # ============================================================

            # è·å–æ£€æµ‹æ¡†åæ ‡ (xyxyæ ¼å¼: xmin, ymin, xmax, ymax)
            bboxes = det_results[0].boxes.xyxy.cpu() 

            # ============================================================
            # ğŸ”¥ æ ¸å¿ƒä¿®æ”¹ï¼šå¤šç›®æ ‡ç­›é€‰ (åªå–æœ€ä¸­é—´çš„ä¸€ä¸ª)
            # ============================================================
            if len(bboxes) > 1:
                import torch
                # è·å–åŸå›¾å°ºå¯¸
                img_h, img_w = det_results[0].orig_shape[:2]
                # è®¡ç®—å±å¹•ä¸­å¿ƒåæ ‡
                screen_center = torch.tensor([img_w / 2.0, img_h / 2.0])
                
                min_dist = float('inf') # åˆå§‹åŒ–æœ€å°è·ç¦»ä¸ºæ— ç©·å¤§
                best_idx = 0
                
                # éå†æ¯ä¸ªæ¡†ï¼Œè®¡ç®—å…¶ä¸­å¿ƒç‚¹åˆ°å±å¹•ä¸­å¿ƒçš„æ¬§æ°è·ç¦»
                for idx, box in enumerate(bboxes):
                    box_center_x = (box[0] + box[2]) / 2.0
                    box_center_y = (box[1] + box[3]) / 2.0
                    
                    dist = torch.sqrt((box_center_x - screen_center[0])**2 + (box_center_y - screen_center[1])**2)
                    
                    if dist < min_dist:
                        min_dist = dist
                        best_idx = idx
                
                # æ›´æ–° bboxesï¼Œåªä¿ç•™æœ€ä¸­å¿ƒçš„ä¸€ä¸ª
                # unsqueeze(0) ç”¨äºä¿æŒç»´åº¦ä¸º [1, 4]ï¼Œè€Œä¸æ˜¯å˜æˆ [4]
                bboxes = bboxes[best_idx].unsqueeze(0) 

            # ============================================================
            # ğŸ”¥ æ ¸å¿ƒä¿®æ”¹ï¼šä»â€œæ­»æ¡†â€æ”¹ä¸ºâ€œæ™ºèƒ½ä¸­å¿ƒç‚¹æ‰©æ•£â€
            # ============================================================
            
            use_point_prompt = False
            
            # å¦‚æœ YOLO æ²¡æ‰¾åˆ°ä»»ä½•ä¸œè¥¿
            if len(bboxes) == 0:
                print(f"       âš ï¸ YOLO æœªè¯†åˆ«åˆ°ç‰©ä½“ï¼Œåˆ‡æ¢ä¸º [SAM ä¸­å¿ƒç‚¹æ¨¡å¼]")
                h, w = det_results[0].orig_shape[:2]
                
                # [ç®—æ³•é€»è¾‘] ç›²çŒœä¸­å¿ƒï¼šå‡è®¾ç‰©ä½“åœ¨ç”»é¢æ­£ä¸­å¤®
                # ç»™ SAM ä¸€ä¸ªä¸­å¿ƒç‚¹æç¤ºï¼Œè®©å®ƒå°è¯•å‘å¤–æ‰©æ•£åˆ†å‰²
                use_point_prompt = True
            
            # 3. æ‰§è¡Œ SAM åˆ†å‰²
            if use_point_prompt:
                # æ„é€ ä¸€ä¸ªä½äºä¸­å¿ƒçš„æå°æ¡†ï¼Œæ¨¡æ‹Ÿç‚¹å‡»æ•ˆæœ
                cx, cy = w / 2, h / 2
                margin = 5 
                bboxes = torch.tensor([[cx-margin, cy-margin, cx+margin, cy+margin]], device=det_model.device)
                
                # è°ƒç”¨ SAM
                sam_results = sam_model(img_path, bboxes=bboxes, verbose=False)
            else:
                # ä½¿ç”¨ YOLO ç¡®å®šçš„æ¡†è°ƒç”¨ SAM
                sam_results = sam_model(img_path, bboxes=bboxes, verbose=False)
            
            # è·å– SAM ç»“æœ
            if sam_results[0].masks is not None:
                # masks.data æ˜¯ [N, H, W] çš„ tensor
                all_masks = sam_results[0].masks.data.cpu().numpy()
                # [Python è¿›é˜¶] np.any(axis=0) å°†æ‰€æœ‰æ£€æµ‹åˆ°çš„ mask åˆå¹¶ï¼ˆé€»è¾‘æˆ–ï¼‰
                final_mask = np.any(all_masks, axis=0).astype(np.uint8) * 255
            else:
                final_mask = np.zeros(det_results[0].orig_shape[:2], dtype=np.uint8)

            # -------------------------------------------------
            # ğŸ”¥ æ ¸å¿ƒä¿®æ”¹ï¼šè°ƒç”¨æ¸…æ´—å‡½æ•°è¿›è¡Œè´¨æ£€ ğŸ”¥
            # -------------------------------------------------
            # è¿™é‡Œè°ƒç”¨äº†å‰é¢å®šä¹‰çš„ clean_and_verify_mask
            is_good, cleaned_mask, reason = clean_and_verify_mask(final_mask, img_path.name)

            if is_good:
                # âœ… åˆæ ¼é€»è¾‘
                original_img = cv2.imread(str(img_path))
                if original_img is not None:
                    # [ç®—æ³•é€»è¾‘] ç¾½åŒ– (Feathering)
                    # å¯¹ Mask è¿›è¡Œé«˜æ–¯æ¨¡ç³Šï¼Œä½¿è¾¹ç¼˜åŠé€æ˜ï¼Œé¿å…åˆæˆæ—¶å‡ºç°é”¯é½¿
                    mask_blurred = cv2.GaussianBlur(cleaned_mask, (5, 5), 0)
                    
                    # å½’ä¸€åŒ– Alpha é€šé“ (0.0 - 1.0)
                    alpha_channel = mask_blurred.astype(np.float32) / 255.0
                    img_float = original_img.astype(np.float32)
                    
                    # [ç®—æ³•é€»è¾‘] é¢„ä¹˜ Alpha (Premultiplied Alpha)
                    # æ ‡å‡†çš„å›¾å½¢å­¦æ“ä½œï¼šRGB = RGB * Alpha
                    # è¿™æ ·èƒŒæ™¯åŒºåŸŸå°±ä¼šå˜æˆçº¯é»‘ (0,0,0)
                    b, g, r = cv2.split(img_float)
                    b = b * alpha_channel
                    g = g * alpha_channel
                    r = r * alpha_channel
                    
                    # åˆå¹¶é€šé“ç”Ÿæˆ BGRA å›¾ç‰‡
                    img_bgra = cv2.merge([
                        b.astype(np.uint8), 
                        g.astype(np.uint8), 
                        r.astype(np.uint8), 
                        mask_blurred # Alpha é€šé“
                    ])
                    
                    # ä¿å­˜ä¸º PNG (JPG ä¸æ”¯æŒé€æ˜é€šé“)
                    new_img_path = img_path.with_suffix('.png')
                    cv2.imwrite(str(new_img_path), img_bgra)
                    
                    # åˆ é™¤æ—§çš„ JPG æ–‡ä»¶ä»¥èŠ‚çœç©ºé—´å’Œé¿å…æ··æ·†
                    if img_path.suffix.lower() == '.jpg':
                        try: img_path.unlink()
                        except: pass
                        
                    final_img_path_name = new_img_path.name
                else:
                    final_img_path_name = img_path.name

                # ä¿å­˜ Mask ä¾›è°ƒè¯•æˆ–è®­ç»ƒä½¿ç”¨
                cv2.imwrite(str(masks_dir / f"{img_path.stem}.png"), cleaned_mask)

                # æ›´æ–° JSON å…ƒæ•°æ®
                if img_path.name in frames_map:
                    frame_data = frames_map[img_path.name]
                    # ä¿®æ”¹æ–‡ä»¶è·¯å¾„æŒ‡å‘æ–°çš„ PNG
                    frame_data["file_path"] = f"images/{final_img_path_name}" 
                    valid_frames_list.append(frame_data)

            else:
                # âŒ ä¸åˆæ ¼é€»è¾‘
                print(f"       ğŸ—‘ï¸ [å‰”é™¤] {img_path.name}: {reason}")
                # [å·¥ç¨‹åŒ–æ€è·¯] ç‰©ç†åˆ é™¤è´¨é‡å·®çš„æ•°æ®ï¼Œé˜²æ­¢è¿›å…¥è®­ç»ƒæµç¨‹æ±¡æŸ“æ¨¡å‹
                img_path.unlink() 
                deleted_count += 1
                # valid_frames_list ä¸­ä¸æ·»åŠ è¯¥å¸§ï¼Œç›¸å½“äºåœ¨ transforms.json ä¸­ä¹Ÿåˆ é™¤äº†

        except Exception as e:
            # å¼‚å¸¸å¤„ç†ï¼šå•ä¸ªå›¾ç‰‡å¤„ç†å¤±è´¥ä¸å½±å“æ•´ä½“æµç¨‹
            print(f"       âŒ å¤„ç†å‡ºé”™ {img_path.name}: {e}")
            continue

        # è¿›åº¦æ¡æ‰“å°
        if i % 10 == 0:
            # end="\r" å®ç°å•è¡Œåˆ·æ–°
            print(f"       è¿›åº¦: {i}/{total_imgs} (å·²å‰”é™¤ {deleted_count} å¼ )...", end="\r")

    # 4. ç»“ç®—ä¸æ›´æ–°
    print(f"\n\nğŸ“Š ç­›é€‰æŠ¥å‘Š:")
    print(f"   - åŸå§‹æ€»æ•°: {total_imgs}")
    print(f"   - å‰”é™¤æ•°é‡: {deleted_count} ({deleted_count/total_imgs:.1%})")
    print(f"   - å‰©ä½™å¯ç”¨: {len(valid_frames_list)}")

    if len(valid_frames_list) == 0:
        print("âŒ é”™è¯¯ï¼šæ‰€æœ‰å›¾ç‰‡éƒ½è¢«å‰”é™¤äº†ï¼è¯·æ£€æŸ¥æç¤ºè¯æˆ–æ‹æ‘„è´¨é‡ã€‚")
        return False

    # 5. é‡å†™ transforms.json
    # [å…³é”®æ­¥éª¤] ç”¨æ¸…æ´—åçš„ valid_frames_list è¦†ç›–åŸå§‹æ•°æ®
    # è¿™æ · Nerfstudio è®­ç»ƒæ—¶å°±åªä¼šè¯»å–åˆ°å¹²å‡€ã€å¸¦æœ‰é€æ˜é€šé“çš„å›¾ç‰‡
    meta["frames"] = valid_frames_list
    with open(cfg.transforms_file, 'w') as f:
        json.dump(meta, f, indent=4)
        
    print(f"    âœ… transforms.json å·²æ›´æ–°ï¼Œæ•°æ®é›†å·²æ¸…æ´—å®Œæ¯•ã€‚")
    return True

# ================= è¾…åŠ©å·¥å…· =================

def format_duration(seconds):

    """
    [è¾…åŠ©å‡½æ•°] å°†ç§’æ•°è½¬æ¢ä¸ºæ˜“è¯»çš„ HH:MM:SS æ ¼å¼
    """
    # [æ ‡å‡†åº“] datetime.timedelta è‡ªåŠ¨å¤„ç†æ—¶é—´æ¢ç®—ï¼ˆå¦‚ 3661ç§’ -> 1:01:01ï¼‰
    return str(datetime.timedelta(seconds=int(seconds)))

class ImageProcessor:
    def __init__(self, config: PipelineConfig):
        self.cfg = config

    def smart_filter_blurry_images(self, image_folder, keep_ratio=0.85):
        """
        [å›¾åƒæ¸…æ´—ç®—æ³•] æ··åˆç­–ç•¥æ¨¡ç³Šæ£€æµ‹
        """
        print(f"\nğŸ§  [æ™ºèƒ½æ¸…æ´—] æ­£åœ¨åˆ†æå›¾ç‰‡è´¨é‡ (æ··åˆç­–ç•¥ç‰ˆ)...")
        image_dir = Path(image_folder)
        images = sorted([p for p in image_dir.iterdir() if p.suffix.lower() in ['.jpg', '.jpeg', '.png']])
        if not images: return
        
        trash_dir = image_dir.parent / "trash_smart"
        trash_dir.mkdir(exist_ok=True)
        
        # --- è¿™é‡Œçš„ä»£ç ä¿æŒä¸å˜ï¼Œç›´åˆ° good_images è®¡ç®—å®Œæ¯• ---
        img_scores = []
        for i, img_path in enumerate(images):
            img = cv2.imread(str(img_path))
            if img is None: continue
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            h, w = gray.shape
            grid_h, grid_w = h // 3, w // 3
            max_grid_score = 0
            for r in range(3):
                for c in range(3):
                    roi = gray[r*grid_h:(r+1)*grid_h, c*grid_w:(c+1)*grid_w]
                    score = cv2.Laplacian(roi, cv2.CV_64F).var()
                    if score > max_grid_score: max_grid_score = score
            img_scores.append((img_path, max_grid_score))
            if i % 50 == 0: print(f"  -> åˆ†æä¸­... {i}/{len(images)}", end="\r")
        
        scores = [s[1] for s in img_scores]
        if not scores: return
        quality_threshold = np.percentile(scores, (1 - keep_ratio) * 100)
        
        good_images = []
        for img_path, score in img_scores:
            if score < quality_threshold:
                shutil.move(str(img_path), str(trash_dir / img_path.name))
            else:
                good_images.append(img_path)
        
        # ======================================================
        # ğŸ”¥ æ ¸å¿ƒä¿®æ”¹åœ¨è¿™é‡Œ ğŸ”¥
        # ======================================================
        # ä»é…ç½®å¯¹è±¡ä¸­è¯»å–æœ€å¤§å›¾ç‰‡æ•°é‡
        max_imgs = self.cfg.max_images  
        
        # ä½¿ç”¨ max_imgs æ›¿ä»£åŸæ¥çš„ max_images
        if len(good_images) > max_imgs:
            print(f"    âš ï¸ å›¾ç‰‡è¿‡å¤š ({len(good_images)} å¼ ), æ­£åœ¨é™é‡‡æ ·è‡³ {max_imgs} å¼ ...")
            # np.linspace ç”Ÿæˆå‡åŒ€åˆ†å¸ƒçš„ç´¢å¼•
            indices_to_keep = set(np.linspace(0, len(good_images) - 1, max_imgs, dtype=int))
            for idx, img_path in enumerate(good_images):
                if idx not in indices_to_keep:
                    shutil.move(str(img_path), str(trash_dir / img_path.name))
                    
        print(f"âœ¨ æ¸…æ´—ç»“æŸï¼Œå‰©ä½™ {len(list(image_dir.glob('*')))} å¼ ã€‚")

def analyze_and_calculate_adaptive_collider(json_path, force_cull=False, radius_scale=1.8):
    """
    [3D åœºæ™¯ç†è§£ç®—æ³•] è§£æç›¸æœºè½¨è¿¹ï¼Œè‡ªåŠ¨åˆ¤æ–­åœºæ™¯ç±»å‹å¹¶è®¡ç®—åŒ…å›´ç›’ (Collider)
    é€»è¾‘ï¼š
    1. è¯»å– transforms.json è·å–æ‰€æœ‰ç›¸æœºä½å§¿ã€‚
    2. è®¡ç®—æ‰€æœ‰ç›¸æœºçš„è§†çº¿å‘é‡ä¸â€œç›¸æœºä¸­å¿ƒ-åœºæ™¯ä¸­å¿ƒâ€å‘é‡çš„ç‚¹ç§¯ã€‚
    3. å¦‚æœå¤§éƒ¨åˆ†ç›¸æœºéƒ½çœ‹å‘ä¸­å¿ƒ -> Object Mode (ç‰©ä½“æ¨¡å¼)ã€‚
    4. å¦‚æœç›¸æœºå‘å››é¢å…«æ–¹çœ‹ -> Scene Mode (åœºæ™¯æ¨¡å¼)ã€‚
    """
    print(f"\nğŸ¤– [AI åˆ†æ] è§£æç›¸æœºè½¨è¿¹...")
    try:
        with open(json_path, 'r') as f: data = json.load(f)
        frames = data["frames"]
        if not frames: return [], "unknown"
        
        has_mask = "mask_path" in frames[0]
        if has_mask:
            print("    -> æ£€æµ‹åˆ° Mask æ•°æ®ï¼å°†å¯ç”¨ç‰©ä½“èšç„¦æ¨¡å¼ã€‚")
        
        # [çº¿æ€§ä»£æ•°] æå–æ‰€æœ‰ç›¸æœºçš„ä½ç§» (Translation)
        # transform_matrix æ˜¯ 4x4 çŸ©é˜µï¼Œ[:3, 3] æ˜¯ XYZ åæ ‡
        positions = [np.array(f["transform_matrix"])[:3, 3] for f in frames]
        
        # æå–ç›¸æœºçš„å‰å‘å‘é‡ (Forward Vector)
        # åœ¨ OpenCV/Colmap å®šä¹‰ä¸­ï¼Œ+Z è½´é€šå¸¸æ˜¯ç›¸æœºçœ‹å‘çš„æ–¹å‘ï¼Œæˆ–è€… -Zï¼Œéœ€æ ¹æ®å…·ä½“åæ ‡ç³»åˆ¤å®š
        # è¿™é‡Œå‡è®¾ -Z æ˜¯å‰æ–¹ (NeRF å¸¸ç”¨çº¦å®š)
        forward_vectors = [np.array(f["transform_matrix"])[:3, :3] @ np.array([0, 0, -1]) for f in frames]
        
        # è®¡ç®—æ‰€æœ‰ç›¸æœºä½ç½®çš„å‡ ä½•ä¸­å¿ƒ
        center = np.mean(positions, axis=0)
        
        # è®¡ç®—æ¯ä¸ªç›¸æœºä½ç½®æŒ‡å‘åœºæ™¯ä¸­å¿ƒçš„å‘é‡
        vec_to_center = center - positions
        # å½’ä¸€åŒ–å‘é‡ (é™¤ä»¥æ¨¡é•¿)
        vec_to_center /= (np.linalg.norm(vec_to_center, axis=1, keepdims=True) + 1e-6)
        
        # [æ ¸å¿ƒç®—æ³•] è®¡ç®—â€œè§†çº¿â€ä¸â€œæŒ‡å‘ä¸­å¿ƒå‘é‡â€çš„å¯¹é½ç¨‹åº¦
        # ç‚¹ç§¯ > 0 è¡¨ç¤ºæ–¹å‘åŸºæœ¬ä¸€è‡´ï¼ˆå¤¹è§’å°äº90åº¦ï¼‰
        # å¦‚æœ ratio > 0.6ï¼Œè¯´æ˜è¶…è¿‡ 60% çš„ç›¸æœºéƒ½çœ‹å‘ä¸­å¿ƒåŒºåŸŸ
        ratio = np.sum(np.sum(forward_vectors * vec_to_center, axis=1) > 0) / len(frames)
        
        # ç»¼åˆåˆ¤å®šï¼šå‘å¿ƒç‡é«˜ OR å¼ºåˆ¶å¼€å¯çƒå½¢è£å‰ª OR æœ‰ Mask
        is_object_mode = ratio > 0.6 or force_cull or has_mask

        if is_object_mode:
            # ç‰©ä½“æ¨¡å¼ï¼šè®¾ç½®ç´§å‡‘çš„ Near/Far Plane
            dists = [np.linalg.norm(p) for p in positions] # ç›¸æœºåˆ°åŸç‚¹çš„è·ç¦»
            avg_dist = np.mean(dists)
            
            scene_radius = 1.0 * radius_scale  # åœºæ™¯åŠå¾„
            
            # è®¡ç®— Near Plane (è¿‘å¹³é¢)ï¼šä¸èƒ½å¤ªè¿‘ï¼Œå¦åˆ™ä¼šåˆ‡æ‰ç›¸æœºå‰çš„ç‰©ä½“
            calc_near = max(0.05, min(dists) - scene_radius)
            # è®¡ç®— Far Plane (è¿œå¹³é¢)ï¼šåªè¦åŒ…ä½ç‰©ä½“å³å¯
            calc_far = avg_dist + scene_radius
            
            # è¿”å› nerfstudio éœ€è¦çš„è®­ç»ƒå‚æ•°
            return ["--pipeline.model.enable-collider", "True", 
                    "--pipeline.model.collider-params", "near_plane", str(round(calc_near, 2)), 
                    "far_plane", str(round(calc_far, 2))], "object"
        else:
            # åœºæ™¯æ¨¡å¼ï¼šç©ºé—´å¾ˆå¤§ï¼ŒFar Plane è®¾è¿œä¸€ç‚¹
            return ["--pipeline.model.enable-collider", "True", 
                    "--pipeline.model.collider-params", "near_plane", "0.05", "far_plane", "100.0"], "scene"
    except:
        return [], "unknown"

def perform_percentile_culling(ply_path, json_path, output_path, keep_percentile=0.9):
    """
    [ç‚¹äº‘åå¤„ç†] åŸºäºç»Ÿè®¡åˆ†ä½æ•°çš„æš´åŠ›åˆ‡å‰²
    åŠŸèƒ½ï¼šå»é™¤ Gaussian Splatting è®­ç»ƒåäº§ç”Ÿåœ¨è¿œå¤„çš„èƒŒæ™¯ä¼ªå½±ã€‚
    ä¾èµ–ï¼šplyfile åº“
    """
    # æ£€æŸ¥ä¾èµ–
    if not HAS_PLYFILE: return False
    print(f"\nâœ‚ï¸ [åå¤„ç†] æ­£åœ¨æ‰§è¡Œã€åˆ†ä½æ•°æš´åŠ›åˆ‡å‰²ã€‘...")
    try:
        # 1. è®¡ç®—åœºæ™¯ä¸­å¿ƒ
        with open(json_path, 'r') as f: frames = json.load(f)["frames"]
        cam_pos = np.array([np.array(f["transform_matrix"])[:3, 3] for f in frames])
        center = np.mean(cam_pos, axis=0)
        
        # 2. è¯»å– PLY ç‚¹äº‘æ•°æ®
        plydata = PlyData.read(str(ply_path))
        vertex = plydata['vertex']
        # å †å  x,y,z åæ ‡
        points = np.stack([vertex['x'], vertex['y'], vertex['z']], axis=1)
        
        # 3. è®¡ç®—æ‰€æœ‰ç‚¹åˆ°ä¸­å¿ƒçš„è·ç¦»
        dists_pts = np.linalg.norm(points - center, axis=1)

        # [ç®—æ³•é€»è¾‘] ç¡®å®šé˜ˆå€¼åŠå¾„
        # 2. âœ… è¿™é‡Œä¿®æ”¹ï¼šä½¿ç”¨ä¼ å…¥çš„å‚æ•° keep_percentile
        threshold_radius = np.percentile(dists_pts, keep_percentile * 100)
        
        # 4. è¯»å–ä¸é€æ˜åº¦ (Opacity) å¹¶è¿‡æ»¤
        # Gaussian Splatting å­˜å‚¨çš„ opacity é€šå¸¸ç»è¿‡ sigmoid æ¿€æ´»ï¼Œéœ€è¦è¿˜åŸ
        # è¿™é‡Œ simplified: å‡è®¾ vertex['opacity'] æ˜¯ logit
        opacities = 1 / (1 + np.exp(-vertex['opacity']))
        
        # è”åˆæ©ç ï¼š(åœ¨åŠå¾„å†…) AND (ä¸é€æ˜åº¦ > 0.05)
        mask = (dists_pts < threshold_radius) & (opacities > 0.05)
        filtered_vertex = vertex[mask]
        
        # 5. å†™å…¥æ–°æ–‡ä»¶
        PlyData([PlyElement.describe(filtered_vertex, 'vertex')]).write(str(output_path))
        return True
    except Exception as e:
        print(f"âŒ åˆ‡å‰²å¤±è´¥: {e}")
        return False


# ==============================================================================
# æ¨¡å—åŒ–ï¼šCOLMAP ä½å§¿è§£ç®—ç±»
# ==============================================================================
class ColmapRunner:
    def __init__(self, cfg: PipelineConfig):
        self.cfg = cfg
        # æŸ¥æ‰¾ colmap å¯æ‰§è¡Œæ–‡ä»¶ (å¯¹åº”åŸæ¥çš„ shutil.which é€»è¾‘)
        self.colmap_exe = shutil.which("colmap") or "/usr/local/bin/colmap"
        # è¿™é‡Œçš„ vocab_tree_path å·²ç»åœ¨ cfg é‡Œå®šä¹‰å¥½äº†
        
    def run(self):
        """æ‰§è¡Œ COLMAP å®Œæ•´æµç¨‹ (å«é‡è¯•æœºåˆ¶)"""
        print(f"\nğŸ“ [2/4] COLMAP ä½å§¿è§£ç®— (å¢å¼ºç‰ˆ)")
        
        # 0. å‡†å¤‡å·¥ä½œï¼šæŠŠæ¸…æ´—å¥½çš„å›¾ç‰‡å¤åˆ¶åˆ° colmap éœ€è¦çš„ç›®å½•
        # COLMAP è¿™ä¸€æ­¥é€šå¸¸éœ€è¦æŠŠ raw_images å¤åˆ¶åˆ° data/images
        # ä½ çš„ extracted_images_dir åº”è¯¥æ˜¯ cfg.project_dir / "raw_images"
        raw_images_dir = self.cfg.project_dir / "raw_images"
        dest_images_dir = self.cfg.images_dir
        
        dest_images_dir.mkdir(parents=True, exist_ok=True)
        # å¢é‡å¤åˆ¶ï¼Œé¿å…é‡å¤
        for img in raw_images_dir.glob("*"):
            if not (dest_images_dir / img.name).exists():
                shutil.copy2(str(img), str(dest_images_dir / img.name))

        # 1. æ•°æ®åº“è·¯å¾„
        colmap_output_dir = self.cfg.data_dir / "colmap"
        colmap_output_dir.mkdir(parents=True, exist_ok=True)
        database_path = colmap_output_dir / "database.db"
        
        # 2. é‡è¯•å¾ªç¯
        max_retries = 3
        for attempt in range(1, max_retries + 1):
            print(f"\nğŸ”„ [COLMAP] æ­£åœ¨æ‰§è¡Œç¬¬ {attempt} / {max_retries} æ¬¡å°è¯•...")
            
            # æ¸…ç†æ—§æ•°æ® (å¦‚æœä¸æ˜¯ç¬¬ä¸€æ¬¡)
            if attempt > 1:
                print("    ğŸ§¹ [é‡è¯•å‡†å¤‡] æ­£åœ¨æ¸…ç†æ—§æ•°æ®...")
                if database_path.exists(): database_path.unlink()
                sparse_dir = colmap_output_dir / "sparse"
                if sparse_dir.exists(): shutil.rmtree(sparse_dir)
                if self.cfg.transforms_file.exists(): self.cfg.transforms_file.unlink()

            try:
                # --- Step 1: ç‰¹å¾æå– ---
                self._run_cmd([
                    self.colmap_exe, "feature_extractor",
                    "--database_path", str(database_path),
                    "--image_path", str(raw_images_dir),
                    "--ImageReader.camera_model", "OPENCV",
                    "--ImageReader.single_camera", "1"
                ], "Step 1: ç‰¹å¾æå–")
                
                # --- Step 2: è¯æ±‡æ ‘åŒ¹é… ---
                # æ£€æŸ¥è¯æ±‡æ ‘æ˜¯å¦å­˜åœ¨ (è¿™é‡Œç”¨ self.cfg)
                local_vocab = Path(__file__).parent / "vocab_tree_flickr100k_words.bin"
                if not self.cfg.vocab_tree_path.exists():
                    if local_vocab.exists():
                        shutil.copy2(str(local_vocab), str(self.cfg.vocab_tree_path))
                    else:
                        raise FileNotFoundError(f"Missing vocab tree: {self.cfg.vocab_tree_path}")

                self._run_cmd([
                    self.colmap_exe, "vocab_tree_matcher",
                    "--database_path", str(database_path),
                    "--VocabTreeMatching.vocab_tree_path", str(self.cfg.vocab_tree_path),
                    "--VocabTreeMatching.match_list_path", ""
                ], "Step 2: è¯æ±‡æ ‘åŒ¹é…")
                
                # --- Step 3: ç¨€ç–é‡å»º ---
                sparse_dir = colmap_output_dir / "sparse"
                sparse_dir.mkdir(parents=True, exist_ok=True)
                self._run_cmd([
                    self.colmap_exe, "mapper",
                    "--database_path", str(database_path),
                    "--image_path", str(raw_images_dir),
                    "--output_path", str(sparse_dir)
                ], "Step 3: ç¨€ç–é‡å»º")
                
                # --- Step 4: ç›®å½•ä¿®æ­£ (Auto-Fix) ---
                self._fix_sparse_folder(sparse_dir)
                
                # --- Step 5: ç”Ÿæˆ transforms.json ---
                # æ³¨æ„ï¼šns-process-data éœ€è¦ images ç›®å½•
                self._run_cmd([
                    "ns-process-data", "images",
                    "--data", str(dest_images_dir),
                    "--output-dir", str(self.cfg.data_dir),
                    "--skip-colmap",
                    "--skip-image-processing",
                    "--num-downscales", "0"
                ], "ç”Ÿæˆ transforms.json")
                
                # --- Step 6: è´¨é‡æ£€æŸ¥ ---
                if self._check_quality(raw_images_dir):
                    print(f"    âœ¨ COLMAP æˆåŠŸï¼")
                    return True # æˆåŠŸè¿”å›
                    
            except Exception as e:
                print(f"    âŒ ç¬¬ {attempt} æ¬¡å°è¯•å¤±è´¥: {e}")
                if attempt < max_retries: time.sleep(3)
        
        print("âŒ COLMAP æœ€ç»ˆå¤±è´¥ã€‚")
        return False

    def _run_cmd(self, cmd, desc):
        """å†…éƒ¨å·¥å…·ï¼šæ‰§è¡Œå‘½ä»¤"""
        print(f"ğŸš€ {desc}...")
        # ä¸ºäº†ç®€åŒ–ï¼Œè¿™é‡Œç”¨äº† check=Trueï¼Œä½ ä¹Ÿå¯ä»¥ç”¨åŸæ¥çš„ Popen å†™æ³•
        subprocess.run(cmd, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.PIPE)

    def _fix_sparse_folder(self, sparse_dir):
        """å†…éƒ¨å·¥å…·ï¼šä¿®æ­£ sparse/0 æ–‡ä»¶å¤¹ç»“æ„"""
        target_dir_0 = sparse_dir / "0"
        target_dir_0.mkdir(parents=True, exist_ok=True)
        required = ["cameras.bin", "images.bin", "points3D.bin"]
        
        # ç®€å•æ£€æŸ¥é€»è¾‘ï¼šå¦‚æœæ–‡ä»¶ä¸åœ¨ 0 é‡Œé¢ï¼Œå°±å»å­ç›®å½•æ‰¾å¹¶ç§»åŠ¨ä¸Šæ¥
        if all((target_dir_0 / f).exists() for f in required): return

        for root, dirs, files in os.walk(sparse_dir):
            if all(f in files for f in required):
                src = Path(root)
                if src != target_dir_0:
                    for f in required:
                        if (target_dir_0/f).exists(): (target_dir_0/f).unlink()
                        shutil.move(str(src/f), str(target_dir_0/f))
                break

    def _check_quality(self, raw_images_dir):
        """å†…éƒ¨å·¥å…·ï¼šæ£€æŸ¥åŒ¹é…ç‡"""
        if not self.cfg.transforms_file.exists(): return False
        with open(self.cfg.transforms_file, 'r') as f: meta = json.load(f)
        
        reg_count = len(meta["frames"])
        total_count = len(list(raw_images_dir.glob("*.jpg")) + list(raw_images_dir.glob("*.png")))
        ratio = reg_count / total_count if total_count > 0 else 0
        print(f"    ğŸ“Š åŒ¹é…ç‡: {ratio:.2%}")
        
        if ratio < 0.35: raise RuntimeError(f"Low match ratio: {ratio:.2%}")
        return True

        
# ================= ä¸»æµç¨‹ =================

def run_pipeline(cfg: PipelineConfig):
    """
    [ä¸»å‡½æ•°] å®Œæ•´çš„é¡¹ç›®æ‰§è¡Œæµæ°´çº¿
    Step 1: è§†é¢‘æŠ½å¸§ä¸æ¸…æ´—
    Step 2: COLMAP å§¿æ€è§£ç®— (å«é‡è¯•ä¸ä¿®æ­£)
    Step 3: AI è¯­ä¹‰åˆ†å‰²
    Step 4: Nerfstudio è®­ç»ƒ
    Step 5: å¯¼å‡ºä¸åå¤„ç†
    """
    global_start_time = time.time()
    print(f"\nğŸš€ [BrainDance Engine AI-Enhanced] å¯åŠ¨ä»»åŠ¡: {cfg.project_name}")
    print(f"ğŸ•’ {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    

    # 1. å®ä¾‹åŒ–å„ä¸ªæ¨¡å— (åœ¨è¿™é‡Œåˆ›å»º ImageProcessor çš„å®ä¾‹)
    # è¿™ç›¸å½“äºæ‹›è˜äº†ä¸€ä¸ªâ€œå›¾ç‰‡å¤„ç†ä¸“å‘˜â€ï¼Œå¹¶æŠŠé…ç½®å•(cfg)ç»™ä»–
    img_processor = ImageProcessor(cfg)
    colmap_runner = ColmapRunner(cfg) # å®ä¾‹åŒ–ï¼Œä½†å…ˆä¸è·‘
    

    # è·¯å¾„è§£æä¸å·¥ä½œåŒºå‡†å¤‡
    video_src = Path(cfg.video_path).resolve()

    # è·¯å¾„è§£æ (ç›´æ¥ä» cfg è·å–)
    work_dir = cfg.project_dir
    data_dir = cfg.data_dir

    output_dir = work_dir / "outputs"
    # cfg.transforms_file = data_dir / "transforms.json"
    
    # [ç¯å¢ƒé…ç½®] å¤åˆ¶å½“å‰ç¯å¢ƒå˜é‡
    env = os.environ.copy()
    # é’ˆå¯¹ Linux æœåŠ¡å™¨æ— å¤´æ¨¡å¼ (Headless)ï¼Œé˜²æ­¢ Qt æŠ¥é”™
    env["QT_QPA_PLATFORM"] = "offscreen" 
    # ã€æ–°å¢ã€‘ä¿®å¤ Python distutils æŠ¥é”™çš„å…³é”®ç¯å¢ƒå˜é‡ (é’ˆå¯¹ Setuptools 60+ ç‰ˆæœ¬)
    env["SETUPTOOLS_USE_DISTUTILS"] = "stdlib" 

    # [Step 1] æ•°æ®å¤„ç†
    step1_start = time.time()
    
    # ç›®å½•åˆå§‹åŒ–ï¼šæ¯æ¬¡è¿è¡Œå‰æ¸…ç†æ—§æ•°æ®ï¼Œä¿è¯ç¯å¢ƒçº¯å‡€
    if work_dir.exists(): shutil.rmtree(work_dir, ignore_errors=True)
    work_dir.mkdir(parents=True, exist_ok=True)
    data_dir.mkdir(parents=True, exist_ok=True)
    shutil.copy(str(video_src), str(work_dir / video_src.name))

    print(f"\nğŸ¥ [1/4] æ•°æ®å‡†å¤‡ä¸æ¸…æ´—")
    temp_dir = work_dir / "temp_extract"
    temp_dir.mkdir(parents=True, exist_ok=True)
    extracted_images_dir = work_dir / "raw_images"
    extracted_images_dir.mkdir(parents=True, exist_ok=True)
    
    # [å¤–éƒ¨è°ƒç”¨] FFmpeg æŠ½å¸§
    # -vf fps=10: æ¯ç§’æŠ½ 10 å¸§
    # -q:v 2: è®¾ç½®é«˜è´¨é‡ JPEG
    try:
        subprocess.run(["ffmpeg", "-y", "-i", str(work_dir / video_src.name), 
                        "-vf", "fps=10", "-q:v", "2", 
                        str(temp_dir / "frame_%05d.jpg")], check=False, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL) 
    except: pass
    
    # è°ƒç”¨ä¹‹å‰çš„æ¸…æ´—å‡½æ•°
    img_processor.smart_filter_blurry_images(temp_dir, keep_ratio=0.85)
    
    # è¿ç§»åˆæ ¼å›¾ç‰‡
    all_candidates = sorted(list(temp_dir.glob("*.jpg")) + list(temp_dir.glob("*.png")))
    final_images_list = []
    # å¦‚æœå›¾ç‰‡è¿˜æ˜¯å¤ªå¤šï¼Œè¿›è¡Œå‡åŒ€é‡‡æ ·
    if len(all_candidates) > cfg.max_images:
        indices = np.linspace(0, len(all_candidates) - 1, cfg.max_images, dtype=int)
        indices = sorted(list(set(indices)))
        for idx in indices: final_images_list.append(all_candidates[idx])
    else:
        final_images_list = all_candidates

    for img_path in final_images_list:
        shutil.copy2(str(img_path), str(extracted_images_dir / img_path.name))
    shutil.rmtree(temp_dir) # åˆ é™¤ä¸´æ—¶ç›®å½•

    # COLMAP æµç¨‹ (å¢å¼ºç‰ˆ - åŒ…å«è‡ªåŠ¨ä¿®æ­£)
    # =================================================================
    # Step 2: COLMAP å§¿æ€è§£ç®— (è¿™é‡Œå¼€å§‹ä½¿ç”¨æ–°æ¨¡å—ï¼)
    # =================================================================
    success = colmap_runner.run()
    
    if not success:
        print("âŒ COLMAP æµç¨‹å¤±è´¥ï¼Œç¨‹åºç»ˆæ­¢")
        return # ç†”æ–­é€€å‡º

    # ================= ğŸ”¥ AI ä»‹å…¥ç‚¹ (æ–°å¢) =================
    if HAS_AI:
        print(f"\nğŸ§  [3/4] AI æ™ºèƒ½åˆ†å‰²ä»‹å…¥ (Qwen + YOLO + SAM)")
        # è°ƒç”¨ä¹‹å‰å®šä¹‰çš„ AI æµç¨‹
        ai_success = run_ai_segmentation_pipeline(data_dir)
        if ai_success:
            print("âœ¨ AI åˆ†å‰²æµç¨‹å®Œæˆï¼ŒMask å·²æ³¨å…¥ï¼")
        else:
            print("âš ï¸ AI åˆ†å‰²æµç¨‹é‡åˆ°é—®é¢˜ï¼Œå°†ä½¿ç”¨åŸå§‹å›¾åƒè®­ç»ƒã€‚")
    else:
        print("\nâ© è·³è¿‡ AI åˆ†å‰² (æœªæ»¡è¶³ä¾èµ–)")
    # ======================================================

    step1_duration = time.time() - step1_start
    print(f"â±ï¸ [é¢„å¤„ç†å®Œæˆ] è€—æ—¶: {format_duration(step1_duration)}")

    # [Step 2] è®­ç»ƒ
    step2_start = time.time()
    print(f"\nğŸ”¥ [4/4] å¼€å§‹è®­ç»ƒ (Splatfacto)")
    
    # åŠ¨æ€è®¡ç®—è®­ç»ƒå‚æ•°
    collider_args, scene_type = analyze_and_calculate_adaptive_collider(
        cfg.transforms_file, 
        force_cull=cfg.force_spherical_culling,
        radius_scale=cfg.scene_radius_scale
    )
    
    # æ„å»ºè®­ç»ƒå‘½ä»¤ (ns-train splatfacto)
    train_cmd = [
        "ns-train", "splatfacto", 
        "--data", str(data_dir), 
        "--output-dir", str(output_dir), 
        "--experiment-name", cfg.project_name, 
        "--pipeline.model.random-init", "False", # ç¦ç”¨éšæœºåˆå§‹åŒ–ï¼Œä½¿ç”¨ COLMAP ç‚¹äº‘åˆå§‹åŒ–
        
        # ğŸ”¥ æ–°å¢å‚æ•° 1: å‘Šè¯‰ Nerfstudio èƒŒæ™¯æ˜¯éšæœºé¢œè‰²æˆ–é€æ˜çš„
        # è¿™æ ·æ¨¡å‹ä¸ä¼šæŠŠé»‘è‰²èƒŒæ™¯å½“æˆçœŸå®çš„é»‘è‰²ç‰©ä½“å»å­¦ä¹ ï¼Œè€Œæ˜¯å€¾å‘äºå°†èƒŒæ™¯é€æ˜åŒ–
        "--pipeline.model.background-color", "random", 
        
        # ğŸ”¥ æ–°å¢å‚æ•° 2: æé«˜ Alpha å‰”é™¤é˜ˆå€¼
        # é»˜è®¤ 0.005ï¼Œæé«˜åˆ° 0.05 å¯ä»¥è®©é‚£ç§æ·¡æ·¡çš„çƒŸé›¾çŠ¶å™ªç‚¹ç›´æ¥ä¸æ¸²æŸ“
        "--pipeline.model.cull-alpha-thresh", "0.05", 

        # [é«˜é˜¶è°ƒä¼˜å‚æ•°]
        # æé«˜åˆ†è£‚é˜ˆå€¼ï¼šè®©é«˜æ–¯çƒæ›´éš¾åˆ†è£‚ï¼Œé˜²æ­¢äº§ç”Ÿè¿‡å¤šç»†ç¢çš„æµ®ç‚¹
        "--pipeline.model.densify-grad-thresh", "0.0008",
        # æå‰åœæ­¢åˆ†è£‚ï¼šæœ€åé˜¶æ®µåªä¼˜åŒ–ä½ç½®å’Œé¢œè‰²ï¼Œä¸å¢åŠ æ–°ç‚¹ï¼Œæœ‰åŠ©äºç¨³å®šç”»é¢
        "--pipeline.model.stop-split-at", "10000",
        # ç¼©çŸ­çƒ­èº«æœŸ
        "--pipeline.model.warmup-length", "500",
        
        # æ³¨å…¥ä¹‹å‰è®¡ç®—çš„ collider å‚æ•° (Near/Far plane)
        *collider_args,
        
        "--max-num-iterations", "15000", # è¿­ä»£æ¬¡æ•°
        "--vis", "viewer+tensorboard",   # å¼€å¯å¯è§†åŒ–æ”¯æŒ
        "--viewer.quit-on-train-completion", "True", # è®­ç»ƒå®Œè‡ªåŠ¨å…³é—­ Viewer
        
        "nerfstudio-data", # æ•°æ®è§£æå™¨ç±»å‹
        "--downscale-factor", "1", # å›¾ç‰‡ç¼©æ”¾æ¯”ä¾‹ï¼Œ1 è¡¨ç¤ºåŸå›¾
        "--orientation-method", "none", # ä¸è‡ªåŠ¨è°ƒæ•´æ–¹å‘ (ä¿¡ä»» COLMAP)
        "--center-method", "none",      # ä¸è‡ªåŠ¨å±…ä¸­ (ä¿¡ä»» COLMAP)
        "--auto-scale-poses", "False"   # ä¸è‡ªåŠ¨ç¼©æ”¾ä½å§¿
    ]
    
    # æ‰§è¡Œè®­ç»ƒ
    subprocess.run(train_cmd, check=True, env=env)
    step2_duration = time.time() - step2_start

    # [Step 3] å¯¼å‡º
    step3_start = time.time()
    print(f"\nğŸ’¾ æ­£åœ¨å¯¼å‡º...")
    # å¯»æ‰¾è®­ç»ƒç”Ÿæˆçš„ config.yml æ–‡ä»¶
    search_path = output_dir / cfg.project_name / "splatfacto"
    run_dirs = sorted(list(search_path.glob("*"))) # é€šå¸¸æ˜¯ä¸€ä¸ªæ—¶é—´æˆ³ç›®å½•
    latest_run = run_dirs[-1]
    
    # è°ƒç”¨ ns-export å¯¼å‡ºä¸ºæ ‡å‡†çš„ .ply é«˜æ–¯æ–‡ä»¶
    subprocess.run([
        "ns-export", "gaussian-splat", 
        "--load-config", str(latest_run/"config.yml"), 
        "--output-dir", str(work_dir)
    ], check=True, env=env)
    
    # åå¤„ç†åˆ‡å‰²
    raw_ply = work_dir / "point_cloud.ply"
    if not raw_ply.exists(): raw_ply = work_dir / "splat.ply" # å…¼å®¹æ—§ç‰ˆæœ¬æ–‡ä»¶å
    cleaned_ply = work_dir / "point_cloud_cleaned.ply"
    final_ply = raw_ply
    
    # å¦‚æœæ˜¯ç‰©ä½“æ¨¡å¼ï¼Œæ‰§è¡Œä¹‹å‰çš„â€œåˆ†ä½æ•°åˆ‡å‰²â€å‡½æ•°
    if (scene_type == "object" or cfg.force_spherical_culling) and raw_ply.exists():
        if perform_percentile_culling(
            raw_ply, 
            cfg.transforms_file, 
            cleaned_ply, 
            keep_percentile=cfg.keep_percentile # <--- æ–°å¢è¿™è¡Œ
        ):
            final_ply = cleaned_ply
    
    step3_duration = time.time() - step3_start

    # [Step 4] ç»“æœå›ä¼ 
    # å°†æœ€ç»ˆç»“æœå¤åˆ¶åˆ°è„šæœ¬æ‰€åœ¨ç›®å½•çš„ results æ–‡ä»¶å¤¹
    target_dir = Path(__file__).parent / "results"
    target_dir.mkdir(exist_ok=True)
    shutil.copy2(str(final_ply), str(target_dir / f"{cfg.project_name}.ply"))
    
    # æ‰“å°æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
    total_duration = time.time() - global_start_time
    print(f"\nğŸ‰ å…¨éƒ¨å®Œæˆï¼æ¨¡å‹å·²ä¿å­˜è‡³: {target_dir / f'{cfg.project_name}.ply'}")
    print(f"ğŸ“Š è€—æ—¶ç»Ÿè®¡:")
    print(f"   - é¢„å¤„ç† (COLMAP + AI): {format_duration(step1_duration)}")
    print(f"   - è®­ç»ƒ (Splatfacto):    {format_duration(step2_duration)}")
    print(f"   - å¯¼å‡ºä¸åå¤„ç†:         {format_duration(step3_duration)}")
    print(f"   - æ€»è€—æ—¶:               {format_duration(total_duration)}")
    
    return str(target_dir / f"{cfg.project_name}.ply")

# ç¨‹åºå…¥å£
if __name__ == "__main__":
    ##
    # 1. å®šä¹‰åŸºç¡€å‚æ•°
    video_file = Path("test.mp4")
    if len(sys.argv) > 1: video_file = Path(sys.argv[1])
    
    # 2. ã€å…³é”®ã€‘å®ä¾‹åŒ–é…ç½®å¯¹è±¡
    cfg = PipelineConfig(
        project_name="scene_ai_test_v2",
        video_path=video_file,
        max_images=100, # æˆ‘å¯ä»¥åœ¨è¿™é‡Œçµæ´»ä¿®æ”¹å‚æ•°ï¼Œä¸ç”¨æ”¹ä»£ç 
        enable_ai=True
    )
    
    # 3. ä¼ å…¥ Pipeline
    run_pipeline(cfg) # æ³¨æ„ï¼šè¿™é‡Œå…¥å‚å˜äº†ï¼Œåé¢ä¼šè®²






    if video_file.exists():
        run_pipeline(video_file, "scene_ai_test")
    else:
        print(f"âŒ æ‰¾ä¸åˆ°è§†é¢‘: {video_file}")