# ==============================================================================
# å¯¼å…¥æ ‡å‡†åº“å’Œç¬¬ä¸‰æ–¹åº“
# ==============================================================================
import subprocess  # [æ ‡å‡†åº“] ç”¨äºæ‰§è¡Œå¤–éƒ¨ç³»ç»Ÿå‘½ä»¤ï¼ˆå¦‚ ffmpeg, colmapï¼‰ï¼Œå®ç° Python ä¸æ“ä½œç³»ç»Ÿçš„äº¤äº’
import sys         # [æ ‡å‡†åº“] ç”¨äºè®¿é—®ä¸ Python è§£é‡Šå™¨ç´§å¯†ç›¸å…³çš„å˜é‡å’Œå‡½æ•°ï¼Œå¦‚ sys.argv, sys.path
import shutil      # [æ ‡å‡†åº“] é«˜çº§æ–‡ä»¶æ“ä½œåº“ï¼Œç”¨äºå¤åˆ¶ã€ç§»åŠ¨ã€åˆ é™¤æ–‡ä»¶å’Œç›®å½•
import os          # [æ ‡å‡†åº“] æä¾›æ“ä½œç³»ç»Ÿæ¥å£ï¼Œç”¨äºè·¯å¾„æ“ä½œã€ç¯å¢ƒå˜é‡è·å–ç­‰
import time        # [æ ‡å‡†åº“] ç”¨äºæ—¶é—´å¤„ç†ï¼Œè®¡ç®—ä»£ç è¿è¡Œè€—æ—¶
import datetime    # [æ ‡å‡†åº“] ç”¨äºå¤„ç†æ—¥æœŸå’Œæ—¶é—´æ ¼å¼
from pathlib import Path  # [Python è¿›é˜¶] é¢å‘å¯¹è±¡çš„æ–‡ä»¶ç³»ç»Ÿè·¯å¾„åº“ï¼Œæ¯” os.path æ›´ä¼˜é›…ã€æ˜“ç”¨
import json        # [æ ‡å‡†åº“] ç”¨äºè¯»å†™ JSON æ ¼å¼æ–‡ä»¶ï¼ˆå¦‚ transforms.json ç›¸æœºå‚æ•°æ–‡ä»¶ï¼‰
import numpy as np # [ç¬¬ä¸‰æ–¹åº“] ç§‘å­¦è®¡ç®—åº“ï¼Œç”¨äºå¤„ç†çŸ©é˜µã€å›¾åƒæ•°ç»„ï¼ˆHWCæ ¼å¼ï¼‰
import logging     # [æ ‡å‡†åº“] æ—¥å¿—ç³»ç»Ÿï¼Œç”¨äºæ§åˆ¶æ§åˆ¶å°è¾“å‡ºçº§åˆ«
import cv2         # [ç¬¬ä¸‰æ–¹åº“] OpenCVï¼Œç”¨äºå›¾åƒå¤„ç†ï¼ˆè¯»å–ã€å†™å…¥ã€å½¢æ€å­¦æ“ä½œã€è½®å»“æŸ¥æ‰¾ç­‰ï¼‰
import re          # [æ ‡å‡†åº“] æ­£åˆ™è¡¨è¾¾å¼ï¼Œè™½åœ¨å¼€å¤´å¼•å…¥ä½†å‰350è¡Œæš‚æœªæ˜¾å¼ç”¨åˆ°å¤æ‚çš„æ­£åˆ™

# ================= ğŸ§  AI ä¾èµ–å¼•å…¥ =================
# [å·¥ç¨‹åŒ–æ€è·¯] è½¯ä¾èµ–å¯¼å…¥ï¼šä¸è¦å› ä¸ºç¼ºå¤±éæ ¸å¿ƒåŠŸèƒ½çš„åº“è€Œå¯¼è‡´æ•´ä¸ªç¨‹åºå´©æºƒã€‚
# è¿™é‡Œä½¿ç”¨äº† try-except å—æ¥æ£€æµ‹æ˜¯å¦å®‰è£…äº† AI ç›¸å…³çš„åº“ã€‚
try:
    import dashscope  # [ç¬¬ä¸‰æ–¹åº“] é˜¿é‡Œäº‘ç™¾ç‚¼ SDKï¼Œç”¨äºè°ƒç”¨ Qwen-VL å¤šæ¨¡æ€å¤§æ¨¡å‹
    from dashscope import MultiModalConversation # å…·ä½“å¯¼å…¥å¤šæ¨¡æ€å¯¹è¯ç±»
    from ultralytics import SAM, YOLOWorld # [ç¬¬ä¸‰æ–¹åº“] Ultralytics åº“ï¼Œå°è£…äº† YOLOï¼ˆç›®æ ‡æ£€æµ‹ï¼‰å’Œ SAMï¼ˆåˆ†å‰²ä¸‡ç‰©ï¼‰
    HAS_AI = True     # [å˜é‡] æ ‡è®°ä½ï¼Œç”¨äºåç»­é€»è¾‘åˆ¤æ–­æ˜¯å¦å¯ç”¨ AI åŠŸèƒ½
except ImportError:
    HAS_AI = False
    # [ç”¨æˆ·äº¤äº’] å‹å¥½çš„é”™è¯¯æç¤ºï¼Œå‘ŠçŸ¥ç”¨æˆ·ç¼ºå¤±äº†ä»€ä¹ˆä»¥åŠå¦‚ä½•ä¿®å¤
    print("âš ï¸ [ç¯å¢ƒè­¦å‘Š] æœªæ£€æµ‹åˆ° dashscope æˆ– ultralytics åº“ã€‚")
    print("    -> æ™ºèƒ½åˆ†å‰²åŠŸèƒ½å°†è¢«ç¦ç”¨ã€‚è¯·è¿è¡Œ: pip install dashscope ultralytics")

# ğŸ”¥ è¯·åœ¨æ­¤å¤„å¡«å…¥ä½ çš„ API KEY (æˆ–è€…ç¡®ä¿ç¯å¢ƒå˜é‡ DASHSCOPE_API_KEY å·²å­˜åœ¨)
# os.environ["DASHSCOPE_API_KEY"] = "sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"

# ================= ğŸ”§ åŸºç¡€é…ç½® =================
# [å·¥ç¨‹åŒ–æ€è·¯] ç¯å¢ƒå˜é‡ä¼˜å…ˆçº§ç®¡ç†
# å¾ˆå¤šæœåŠ¡å™¨ä¸Šç³»ç»Ÿè‡ªå¸¦çš„ colmap ç‰ˆæœ¬è¾ƒè€ï¼Œè¿™é‡Œå¼ºåˆ¶å°†ç”¨æˆ·ç¼–è¯‘çš„é«˜ç‰ˆæœ¬è·¯å¾„æåˆ° PATH ç¯å¢ƒå˜é‡çš„æœ€å‰é¢
# sys_path = "/usr/local/bin" # [å˜é‡] æŒ‡å®šé«˜ä¼˜å…ˆçº§äºŒè¿›åˆ¶æ–‡ä»¶è·¯å¾„
# current_path = os.environ.get("PATH", "") # è·å–å½“å‰ PATH
# # æ£€æŸ¥ sys_path æ˜¯å¦å·²ç»åœ¨ PATH çš„é¦–ä½
# if sys_path not in current_path.split(os.pathsep)[0]:
#     print(f"âš¡ [ç¯å¢ƒä¿®æ­£] å¼ºåˆ¶è®¾ç½® PATH ä¼˜å…ˆçº§: {sys_path} -> Priority High")
#     # æ‹¼æ¥æ–°çš„ PATHï¼Œå°† sys_path æ”¾åœ¨æœ€å‰é¢
#     os.environ["PATH"] = f"{sys_path}{os.pathsep}{current_path}"

# è®¾ç½®æ—¥å¿—çº§åˆ«
# å±è”½ nerfstudio åº“ä¸­é Error çº§åˆ«çš„æ—¥å¿—ï¼Œé˜²æ­¢æ§åˆ¶å°è¢«åˆ·å±
logging.getLogger('nerfstudio').setLevel(logging.ERROR) 






# # å·¥ä½œåŒºé…ç½®
# # [Python è¿›é˜¶] ä½¿ç”¨ Path.home() è·å–ç”¨æˆ·ä¸»ç›®å½•ï¼Œå®ç°è·¨å¹³å°ï¼ˆWindows/Linuxï¼‰å…¼å®¹
# LINUX_WORK_ROOT = Path.home() / "braindance_workspace"
# # ğŸ”¥ æ–°å¢ï¼šè¯æ±‡æ ‘æ–‡ä»¶è·¯å¾„ï¼ŒCOLMAP è¿›è¡Œç‰¹å¾åŒ¹é…æ—¶éœ€è¦çš„é¢„è®­ç»ƒæ•°æ®
# VOCAB_TREE_PATH = LINUX_WORK_ROOT / "vocab_tree_flickr100k_words.bin" 
# SCENE_RADIUS_SCALE = 1.8  # [å˜é‡] åœºæ™¯åŠå¾„ç¼©æ”¾å› å­ï¼Œç”¨äºè®¡ç®—ç›¸æœºè¿‘å¹³é¢/è¿œå¹³é¢
# MAX_IMAGES = 180          # [å˜é‡] é™åˆ¶æœ€å¤§å¤„ç†å›¾ç‰‡æ•°é‡ï¼Œé˜²æ­¢æ˜¾å­˜çˆ†ç‚¸æˆ–è®­ç»ƒæ—¶é—´è¿‡é•¿

# # åˆ‡å‰²é…ç½®
# FORCE_SPHERICAL_CULLING = True # [å˜é‡] æ˜¯å¦å¼ºåˆ¶ä½¿ç”¨çƒå½¢è£å‰ªï¼ˆä¿ç•™ä¸­å¿ƒç‰©ä½“ï¼Œåˆ‡é™¤å‘¨å›´æ‚æ™¯ï¼‰
# KEEP_PERCENTILE = 0.9          # [å˜é‡] ä¿ç•™ 90% çš„ç‚¹äº‘å¯†åº¦ï¼Œå»é™¤ç¦»ç¾¤ç‚¹
from dataclasses import dataclass, field
from pathlib import Path
import os
import sys

@dataclass
class PipelineConfig:
    # 1. ã€å¿…å¡«é¡¹ã€‘ç”¨æˆ·åˆå§‹åŒ–æ—¶å¿…é¡»ç»™æˆ‘çš„
    project_name: str
    video_path: Path
    
    # 2. ã€é€‰å¡«é¡¹ã€‘æœ‰é»˜è®¤å€¼çš„é…ç½® (å¯¹åº”ä½ åŸä»£ç çš„å…¨å±€å˜é‡)
    work_root: Path = Path.home() / "braindance_workspace"
    max_images: int = 180
    force_spherical_culling: bool = True 
    scene_radius_scale: float = 1.8
    keep_percentile: float = 0.9
    enable_ai: bool = True  # æ–°å¢æ§åˆ¶å¼€å…³
    
    # 3. ã€è‡ªåŠ¨è®¡ç®—é¡¹ã€‘ç”¨æˆ·ä¸ç”¨ä¼ ï¼Œæˆ‘è‡ªå·±ç®—å‡ºæ¥çš„è·¯å¾„
    # field(init=False) çš„æ„æ€æ˜¯ï¼šè¿™ä¸ªå˜é‡å­˜åœ¨ï¼Œä½†åœ¨åˆå§‹åŒ–(__init__)æ—¶ä¸éœ€è¦ä½œä¸ºå‚æ•°ä¼ å…¥
    project_dir: Path = field(init=False)
    data_dir: Path = field(init=False)
    images_dir: Path = field(init=False)
    masks_dir: Path = field(init=False)
    transforms_file: Path = field(init=False)
    vocab_tree_path: Path = field(init=False)

    def __post_init__(self):
        """
        è¿™ä¸ªå‡½æ•°ä¼šåœ¨ç±»åˆå§‹åŒ–å®Œæˆä¹‹åï¼Œè‡ªåŠ¨æ‰§è¡Œï¼
        æˆ‘ä»¬åœ¨è¿™é‡Œé›†ä¸­å¤„ç†æ‰€æœ‰çš„è·¯å¾„æ‹¼æ¥å’Œç¯å¢ƒè®¾ç½®ã€‚
        """
        # --- A. è‡ªåŠ¨è®¡ç®—è·¯å¾„ (å†ä¹Ÿä¸ç”¨åœ¨ä¸»å‡½æ•°é‡Œå†™ä¸€éäº†) ---
        self.project_dir = self.work_root / self.project_name
        self.data_dir = self.project_dir / "data"
        self.images_dir = self.data_dir / "images"
        self.masks_dir = self.data_dir / "masks"
        self.transforms_file = self.data_dir / "transforms.json"
        
        # è¯æ±‡æ ‘è·¯å¾„ (å¯¹åº”åŸä»£ç  VOCAB_TREE_PATH)
        self.vocab_tree_path = self.work_root / "vocab_tree_flickr100k_words.bin"

        # --- B. ç¯å¢ƒä¿®æ­£ (å¯¹åº”åŸä»£ç çš„ PATH è®¾ç½®é€»è¾‘) ---
        # æŠŠè®¾ç½®ç¯å¢ƒå˜é‡çš„é€»è¾‘æ¬åˆ°è¿™é‡Œï¼Œä¿è¯ config ä¸€åŠ è½½ï¼Œç¯å¢ƒå°±æ˜¯å¯¹çš„
        # sys_path = "/usr/local/bin"
        # current_path = os.environ.get("PATH", "")
        # if sys_path not in current_path.split(os.pathsep)[0]:
        #     print(f"âš¡ [Config] è‡ªåŠ¨ä¼˜åŒ– PATH ä¼˜å…ˆçº§: {sys_path}")
        #     os.environ["PATH"] = f"{sys_path}{os.pathsep}{current_path}"
            
        # è®¾ç½® Setuptools ä¿®å¤ (å¯¹åº”åŸä»£ç  env["SETUPTOOLS_USE_DISTUTILS"])
        os.environ["SETUPTOOLS_USE_DISTUTILS"] = "stdlib"

# æ£€æŸ¥ plyfile åº“
# plyfile ç”¨äºè¯»å†™ .ply ç‚¹äº‘æ–‡ä»¶
try:
    from plyfile import PlyData, PlyElement
    HAS_PLYFILE = True
except ImportError:
    HAS_PLYFILE = False

# ================= ğŸ§  AI æ ¸å¿ƒé€»è¾‘å‡½æ•° =================

def get_central_object_prompt(images_dir: Path, sample_count=3):
    """
    [Step 1.1] ä½¿ç”¨ Qwen-VL-Plus å¤šå›¾åˆ†æï¼Œæå–ä¸­å¿ƒç‰©ä½“çš„æ–‡æœ¬æè¿°
    
    å‚æ•°:
        images_dir (Path): å›¾ç‰‡æ–‡ä»¶å¤¹è·¯å¾„
        sample_count (int): é‡‡æ ·å›¾ç‰‡æ•°é‡ï¼Œé»˜è®¤3å¼ ï¼ŒèŠ‚çœ Token å¹¶åŠ å¿«é€Ÿåº¦
    
    è¿”å›:
        prompt_text (str): å¤§æ¨¡å‹ç”Ÿæˆçš„ç‰©ä½“æè¿°æç¤ºè¯
    """
    # è·å– API Key
    api_key = os.environ.get("DASHSCOPE_API_KEY")
    if not api_key:
        print("âŒ æœªè®¾ç½® DASHSCOPE_API_KEYï¼Œæ— æ³•è°ƒç”¨å¤§æ¨¡å‹ã€‚")
        return None

    print(f"\nğŸ§  [AI åˆ†æ] æ­£åœ¨è°ƒç”¨ Qwen-VL-Plus åˆ†æåœºæ™¯...")
    
    # [Python è¿›é˜¶] ä½¿ç”¨ glob è·å–æ‰€æœ‰ jpg/png å›¾ç‰‡ï¼Œå¹¶æ’åºç¡®ä¿é¡ºåºä¸€è‡´
    image_files = sorted(list(images_dir.glob("*.jpg")) + list(images_dir.glob("*.png")))
    if not image_files: return None
    
    # [ç®—æ³•é€»è¾‘] å‡åŒ€é‡‡æ ·ï¼šä½¿ç”¨ numpy çš„ linspace åœ¨å›¾ç‰‡åºåˆ—ä¸­å‡åŒ€æŠ½å– sample_count å¼ å›¾
    # è¿™æ ·èƒ½è¦†ç›–ç‰©ä½“çš„ä¸åŒè§’åº¦ï¼Œæ¯”åªå–å‰ä¸‰å¼ æ›´ç¨³å¥
    indices = np.linspace(0, len(image_files) - 1, sample_count, dtype=int)
    sampled_imgs = [image_files[i] for i in indices]
    
    # æ„å»ºå¤šæ¨¡æ€æ¶ˆæ¯ä½“ (Dashscope SDK è¦æ±‚çš„æ ¼å¼)
    content = [{"image": str(img_path)} for img_path in sampled_imgs]
    content.append({
        "text": (
            "è¿™äº›æ˜¯ä¸€ä¸ªè§†é¢‘çš„æŠ½å¸§å›¾ç‰‡ã€‚è¯·åˆ†æç”»é¢ä¸­å¿ƒå§‹ç»ˆå­˜åœ¨çš„ã€æœ€ä¸»è¦çš„ä¸€ä¸ªç‰©ä½“æ˜¯ä»€ä¹ˆã€‚"
            "è¯·è¾“å‡ºä¸€ä¸ªé€‚åˆç”¨äºç‰©ä½“æ£€æµ‹æ¨¡å‹çš„è‹±æ–‡åè¯çŸ­è¯­ï¼ˆPromptï¼‰ã€‚"
            "âš ï¸ å…³é”®ç­–ç•¥ï¼šè¯·ä¼˜å…ˆæè¿°ã€è§†è§‰ç‰¹å¾ã€‘ï¼ˆé¢œè‰²ã€æè´¨ã€å½¢çŠ¶ï¼‰ï¼Œè€Œä¸æ˜¯ã€åŠŸèƒ½åç§°ã€‘ã€‚"
            "è¶Šç®€å•ã€è¶Š'åœŸ'çš„è¯ï¼Œæ£€æµ‹æ¨¡å‹è¶Šå®¹æ˜“è¯†åˆ«ã€‚"
            "ä¾‹å¦‚ï¼š"
            " - ä¸è¦è¯´ 'electric shaver' (ç”µåŠ¨å‰ƒé¡»åˆ€)ï¼Œè¯·è¯´ 'gray metal object' æˆ– 'device'ã€‚"
            " - ä¸è¦è¯´ 'portable charger' (å……ç”µå®)ï¼Œè¯·è¯´ 'white rectangular box'ã€‚"
            "è¦æ±‚ï¼šä¸¥æ ¼åªè¾“å‡ºè¿™ä¸ªè‹±æ–‡çŸ­è¯­ï¼Œä¸è¦åŒ…å«ä»»ä½•æ ‡ç‚¹ç¬¦å·ã€è§£é‡Šã€‚"
        )
    })
    
    # å°è£…ç”¨æˆ·æ¶ˆæ¯
    messages = [{"role": "user", "content": content}]

    try:
        # è°ƒç”¨é˜¿é‡Œäº‘ Qwen-VL-Plus æ¨¡å‹
        response = dashscope.MultiModalConversation.call(
            model='qwen-vl-plus', 
            messages=messages
        )
        
        # è§£æè¿”å›ç»“æœ
        if response.status_code == 200:
            # æå–æ–‡æœ¬å†…å®¹
            prompt_text = response.output.choices[0].message.content[0]["text"].strip()
            # [æ•°æ®æ¸…æ´—] å»æ‰å¯èƒ½å­˜åœ¨çš„æ ‡ç‚¹ç¬¦å·ï¼Œé˜²æ­¢å¹²æ‰° YOLO
            prompt_text = prompt_text.replace(".", "").replace('"', "").replace("'", "")
            # \033[92m æ˜¯ ANSI è½¬ä¹‰ç ï¼Œç”¨äºåœ¨æ§åˆ¶å°è¾“å‡ºç»¿è‰²æ–‡å­—
            print(f"    ğŸ¤– Qwen è®¤ä¸ºä¸­å¿ƒç‰©ä½“æ˜¯: [ \033[92m{prompt_text}\033[0m ]")
            return prompt_text
        else:
            print(f"âŒ Qwen è°ƒç”¨å¤±è´¥: {response.code} - {response.message}")
            return None
    except Exception as e:
        print(f"âŒ API è¿æ¥å¼‚å¸¸: {e}")
        return None

def clean_and_verify_mask(mask, img_name=""):
    """
    [å‡€åŒ–ç‰ˆ] Mask åå¤„ç†æ ¸å¿ƒç®—æ³•
    åŠŸèƒ½ï¼š
    1. å¼ºåˆ¶æ¸…æ´—ï¼šåªä¿ç•™ç”»é¢ä¸­æœ€å¤§çš„è¿é€šå— (å»é™¤å­¤ç«‹å™ªç‚¹)ã€‚
    2. ä¸¥æ ¼è´¨æ£€ï¼šæ¸…æ´—åå¦‚æœå½¢çŠ¶ä¾ç„¶æ¯›ç³™(ç²˜è¿é˜´å½±)ï¼Œåˆ™å‰”é™¤ã€‚
    3. è¾¹ç¼˜è…èš€ï¼šå‘å†…æ”¶ç¼© Maskï¼Œå»é™¤è¾¹ç¼˜æ‚è‰²ã€‚
    
    å‚æ•°:
        mask (numpy array): å•é€šé“äºŒå€¼å›¾åƒ (0æˆ–255)
        img_name (str): ç”¨äºæ—¥å¿—è¾“å‡ºçš„æ–‡ä»¶å
        
    è¿”å›:
        tuple: (æ˜¯å¦åˆæ ¼ bool, æ¸…æ´—åçš„å¹²å‡€Mask, åŸå›  str)
    """
    h, w = mask.shape # è·å–å›¾åƒé«˜å®½
    
    # --- 1. è¿é€šåŸŸåˆ†æ & å¼ºåˆ¶æ¸…æ´— (Cleaning) ---
    # [ç®—æ³•é€»è¾‘] è¿é€šç»„ä»¶åˆ†æ (Connected Components)
    # è¿™é‡Œçš„ connectivity=8 è¡¨ç¤ºåˆ¤æ–­åƒç´ ç›¸è¿æ—¶è€ƒè™‘å‘¨å›´8ä¸ªæ–¹å‘
    # stats åŒ…å«æ¯ä¸ªè¿é€šå—çš„ [å·¦ä¸Šè§’x, å·¦ä¸Šè§’y, å®½, é«˜, é¢ç§¯]
    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(mask, connectivity=8)
    
    # num_labels è‡³å°‘ä¸º 2 (èƒŒæ™¯0 + è‡³å°‘ä¸€ä¸ªå‰æ™¯å—)ï¼Œå¦‚æœå°äº2è¯´æ˜å…¨æ˜¯é»‘çš„
    if num_labels < 2: 
        return False, None, "ç©ºè’™ç‰ˆ"

    # [ç®—æ³•é€»è¾‘] å¯»æ‰¾æœ€å¤§å‰æ™¯å—
    # éå†æ‰€æœ‰æ ‡ç­¾ï¼ˆä»1å¼€å§‹ï¼Œè·³è¿‡0èƒŒæ™¯ï¼‰ï¼Œæ‰¾åˆ°é¢ç§¯æœ€å¤§çš„é‚£ä¸ª
    max_area = 0
    max_label = -1
    for i in range(1, num_labels):
        if stats[i, cv2.CC_STAT_AREA] > max_area:
            max_area = stats[i, cv2.CC_STAT_AREA]
            max_label = i
            
    # [å·¥ç¨‹åŒ–æ€è·¯] é˜ˆå€¼è¿‡æ»¤ï¼šå¦‚æœæœ€å¤§çš„å—å æ¯”ä¸åˆ°å…¨å›¾çš„ 0.5%ï¼Œé€šå¸¸æ˜¯å™ªç‚¹
    if max_area < (h * w * 0.005):
        return False, None, "ä¸»ä½“è¿‡å°ï¼Œç–‘ä¼¼å™ªç‚¹"

    # ğŸ”¥ æ ¸å¿ƒæ“ä½œï¼šé‡æ„ Mask
    # åªä¿ç•™ label ç­‰äº max_label çš„åƒç´ ï¼Œå…¶ä½™ç½®ä¸º 0ã€‚
    # è¿™æ­¥æ“ä½œèƒ½å®Œç¾å»é™¤å‘¨å›´çš„é£æº…å™ªç‚¹ã€‚
    cleaned_mask = (labels == max_label).astype(np.uint8) * 255

    # --- 2. å¯¹æ¸…æ´—åçš„ Mask è¿›è¡Œâ€œä½“æ£€â€ (Verification) ---
    
    # [ç®—æ³•é€»è¾‘] è½®å»“æå–
    # RETR_EXTERNAL åªå–æœ€å¤–å±‚è½®å»“
    contours, _ = cv2.findContours(cleaned_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if not contours: return False, None, "æ¸…æ´—åæ— è½®å»“"
    
    # å–æœ€å¤§è½®å»“
    main_cnt = max(contours, key=cv2.contourArea)
    
    # [ç®—æ³•é€»è¾‘] å®å¿ƒåº¦ (Solidity) è®¡ç®—
    # å‡¸åŒ… (Convex Hull) åƒæ˜¯ç”¨æ©¡çš®ç­‹åŒ…ä½ç‰©ä½“çš„å½¢çŠ¶ã€‚
    # å®å¿ƒåº¦ = è½®å»“é¢ç§¯ / å‡¸åŒ…é¢ç§¯ã€‚
    # æ­£å¸¸ç‰©ä½“å®å¿ƒåº¦é«˜ (~0.95)ï¼Œå¦‚æœæœ‰ç²˜è¿é˜´å½±ï¼Œè½®å»“ä¼šå¾ˆä¸è§„åˆ™ï¼Œå®å¿ƒåº¦ä¼šé™ä½ã€‚
    hull = cv2.convexHull(main_cnt)
    hull_area = cv2.contourArea(hull)
    if hull_area == 0: return False, None, "å‡¸åŒ…é¢ç§¯ä¸º0"
    
    solidity = max_area / hull_area
    
    # é˜ˆå€¼è®¾å®šï¼š0.88 (ç»éªŒå€¼ï¼Œä½äºæ­¤å€¼é€šå¸¸æ„å‘³ç€è¾¹ç¼˜éå¸¸æ¯›ç³™æˆ–æœ‰ç²˜è¿)
    if solidity < 0.88:
        return False, None, f"è¾¹ç¼˜ä¸¥é‡æ¯›ç³™/ç²˜è¿é˜´å½± (å®å¿ƒåº¦ {solidity:.2f})"

    # [ç®—æ³•é€»è¾‘] é•¿å®½æ¯”æ£€æŸ¥ (Aspect Ratio)
    # é˜²æ­¢æŠŠé•¿æ¡å½¢çš„æ¡Œå­ç¼éš™ã€å¢™è§’çº¿å½“æˆç‰©ä½“
    x, y, w_rect, h_rect = cv2.boundingRect(main_cnt)
    aspect_ratio = w_rect / h_rect
    if aspect_ratio > 4.5: # å…è®¸ä¸€å®šç¨‹åº¦çš„é•¿æ¡ï¼Œä½†è¶…è¿‡ 4.5 å€å°±å¤ªå¤¸å¼ äº†
        return False, None, f"å½¢çŠ¶å¼‚å¸¸ (é•¿å®½æ¯” {aspect_ratio:.1f})"

    # ğŸ”¥ æ–°å¢ï¼šè¾¹ç¼˜è…èš€ (Erosion)
    # [ç®—æ³•é€»è¾‘] è…èš€æ“ä½œ
    # å·ç§¯æ ¸ kernel åœ¨å›¾åƒä¸Šæ»‘åŠ¨ï¼Œåªæœ‰æ ¸å†…å…¨ä¸º 255 æ—¶æ‰ä¿ç•™ä¸­å¿ƒç‚¹ã€‚
    # æ•ˆæœæ˜¯è®©ç™½è‰²åŒºåŸŸå‘å†…æ”¶ç¼©ï¼Œåˆ‡æ‰ç‰©ä½“è¾¹ç¼˜å¯èƒ½å­˜åœ¨çš„â€œå…‰æ™•â€æˆ–èƒŒæ™¯æ‚è‰²ã€‚
    kernel_size = 3  # 3x3 çš„æ ¸ï¼Œå¤§çº¦æ”¶ç¼© 1 åƒç´ 
    kernel = np.ones((kernel_size, kernel_size), np.uint8)
    cleaned_mask = cv2.erode(cleaned_mask, kernel, iterations=1)
    
    return True, cleaned_mask, "åˆæ ¼"

def get_salient_box(img_path, margin_ratio=0.1):
    """
    [çº¯æœ¬åœ° CV ç®—æ³•] å½“ AI å¤±è´¥æ—¶ï¼Œä½¿ç”¨ä¼ ç»Ÿè§†è§‰ç®—æ³•è®¡ç®—'è§†è§‰æ˜¾è‘—åŒºåŸŸ'ã€‚
    åŸç†ï¼šåˆ©ç”¨æ‹‰æ™®æ‹‰æ–¯ç®—å­æ‰¾è¾¹ç¼˜ -> è†¨èƒ€è¿æ¥ -> æ‰¾æœ€å¤§å¤–æ¥çŸ©å½¢
    
    å‚æ•°:
        img_path: å›¾ç‰‡è·¯å¾„
        margin_ratio: ç»“æœæ¡†çš„æ‰©è¾¹æ¯”ä¾‹ (padding)
    """
    try:
        img = cv2.imread(str(img_path))
        if img is None: return None
        
        # 1. è½¬ç°åº¦å¹¶è®¡ç®—è¾¹ç¼˜ (Laplacian)
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        # [ç®—æ³•é€»è¾‘] æ‹‰æ™®æ‹‰æ–¯ç®—å­è®¡ç®—äºŒé˜¶å¯¼æ•°ï¼Œå¯¹è¾¹ç¼˜æå…¶æ•æ„Ÿ
        # CV_64F å…è®¸è´Ÿå€¼ï¼Œé˜²æ­¢æˆªæ–­
        laplacian = cv2.Laplacian(gray, cv2.CV_64F)
        laplacian = np.uint8(np.absolute(laplacian)) # å–ç»å¯¹å€¼è½¬å› uint8
        
        # 2. æ¨¡ç³Šä¸äºŒå€¼åŒ–
        # [ç®—æ³•é€»è¾‘] é«˜æ–¯æ¨¡ç³Šç”¨äºå¹³æ»‘çº¹ç†ï¼Œè®©é›¶æ•£çš„è¾¹ç¼˜èšé›†
        blurred = cv2.GaussianBlur(laplacian, (25, 25), 0)
        # [ç®—æ³•é€»è¾‘] åŠ¨æ€é˜ˆå€¼ï¼šåªä¿ç•™äº®åº¦å‰ 20% çš„åŒºåŸŸï¼ˆå³çº¹ç†æœ€ä¸°å¯Œçš„åœ°æ–¹ï¼‰
        threshold_val = np.percentile(blurred, 80) 
        _, binary = cv2.threshold(blurred, threshold_val, 255, cv2.THRESH_BINARY)
        
        # 3. æ‰¾æœ€å¤§è½®å»“
        contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        if not contours: return None
        
        # å‡è®¾çº¹ç†æœ€å¤æ‚çš„åŒºåŸŸå°±æ˜¯ä¸»ä½“
        max_cnt = max(contours, key=cv2.contourArea)
        x, y, w, h = cv2.boundingRect(max_cnt)
        
        # 4. åŠ ä¸Šå®‰å…¨è¾¹è· (Padding)
        H, W = img.shape[:2]
        pad_x = int(w * margin_ratio)
        pad_y = int(h * margin_ratio)
        
        # é™åˆ¶åæ ‡ä¸è¶…å‡ºå›¾åƒè¾¹ç•Œ
        x1 = max(0, x - pad_x)
        y1 = max(0, y - pad_y)
        x2 = min(W, x + w + pad_x)
        y2 = min(H, y + h + pad_y)
        
        # è¿”å› torch tensor æ ¼å¼ï¼Œé€‚é… YOLO/SAM çš„è¾“å…¥è¦æ±‚
        import torch
        return torch.tensor([[x1, y1, x2, y2]], dtype=torch.float32)
        
    except Exception as e:
        print(f"       âš ï¸ è§†è§‰é‡å¿ƒè®¡ç®—å¤±è´¥: {e}")
        return None

def run_ai_segmentation_pipeline(data_dir: Path):
    """
    [Step 1.2] æ‰§è¡Œ AI åˆ†å‰²æ€»æµæ°´çº¿
    é€»è¾‘æµç¨‹ï¼š
    1. å°è¯•ç”¨ Qwen åˆ†æç‰©ä½“ -> å¾—åˆ° Prompt
    2. åŠ è½½ YOLO å’Œ SAM æ¨¡å‹
    3. éå†æ¯ä¸€å¼ å›¾ï¼š
        a. YOLO æ ¹æ® Prompt æ‰¾æ¡†
        b. å¦‚æœæœ‰å¤šä¸ªæ¡†ï¼Œé€‰æœ€ä¸­å¿ƒçš„
        c. å¦‚æœæ²¡æ¡†ï¼Œç”¨ SAM ä¸­å¿ƒç‚¹æ¨¡å¼
        d. SAM ç”Ÿæˆ Mask
        e. clean_and_verify_mask æ¸…æ´— Mask
        f. å¦‚æœåˆæ ¼ -> ç”Ÿæˆé€æ˜ PNGï¼Œæ›´æ–° transforms.json
        g. å¦‚æœä¸åˆæ ¼ -> åˆ é™¤å›¾ç‰‡
    """
    if not HAS_AI: return False # å¦‚æœç¼ºå°‘åº“ï¼Œç›´æ¥è·³è¿‡
    
    # è·¯å¾„å®šä¹‰
    images_dir = data_dir / "images"
    masks_dir = data_dir / "masks"
    cfg.transforms_file = data_dir / "transforms.json" # COLMAP ç”Ÿæˆçš„ç›¸æœºä½å§¿æ–‡ä»¶

    if not cfg.transforms_file.exists():
        print("âš ï¸ æœªæ‰¾åˆ° transforms.jsonï¼Œæ— æ³•è¿›è¡Œ Mask å¤„ç†ã€‚")
        return False

    # ================= æ ¸å¿ƒä¿®æ”¹é€»è¾‘å¼€å§‹ =================
    print(f"\nâœ‚ï¸ [AI åˆ†å‰²] æ­£åœ¨åˆå§‹åŒ–...")

    # --- ç¬¬ä¸€å±‚ï¼šå°è¯•è°ƒç”¨å¤§æ¨¡å‹è·å–ç²¾å‡† Prompt ---
    text_prompt = None
    try:
        # è°ƒç”¨ä¹‹å‰å®šä¹‰çš„å‡½æ•°
        text_prompt = get_central_object_prompt(images_dir)
    except Exception as e:
        print(f"    âš ï¸ å¤§æ¨¡å‹è°ƒç”¨å‡ºé”™: {e}")

    # --- ç¬¬äºŒå±‚ï¼šå¦‚æœå¤§æ¨¡å‹å¤±è´¥ï¼Œä½¿ç”¨é€šç”¨ Prompt ---
    if not text_prompt:
        # [å·¥ç¨‹åŒ–æ€è·¯] é™çº§ç­–ç•¥ (Fallback)
        # å¦‚æœå¤§æ¨¡å‹æŒ‚äº†ï¼Œæˆ–è€… API æ²¡é’±äº†ï¼Œä½¿ç”¨é€šç”¨è¯ "central object"
        text_prompt = "central object; single object"
        print(f"    âš ï¸ æœªèƒ½è·å–ç²¾å‡†æè¿°ï¼Œé™çº§ä½¿ç”¨é€šç”¨ Prompt: '{text_prompt}'")
    else:
        print(f"    ğŸ¯ è·å–åˆ°ç²¾å‡† Prompt: '\033[92m{text_prompt}\033[0m'")

    masks_dir.mkdir(parents=True, exist_ok=True) # åˆ›å»º Mask ç›®å½•
    # ================= æ ¸å¿ƒä¿®æ”¹é€»è¾‘ç»“æŸ =================

    # 2. åŠ è½½æ¨¡å‹ (æ¨èç”¨ Large)
    print("    -> æ­£åœ¨åŠ è½½ SAM 2 Large æ¨¡å‹...")
    
    # ğŸ”¥ è‡ªåŠ¨è¿ç§» AI æ¨¡å‹æ–‡ä»¶
    # [å·¥ç¨‹åŒ–æ€è·¯] è‡ªåŠ¨éƒ¨ç½²ï¼šè„šæœ¬è¿è¡Œæ—¶æ£€æŸ¥å½“å‰ç›®å½•æ˜¯å¦æœ‰æ¨¡å‹æ–‡ä»¶ï¼Œå¦‚æœæœ‰åˆ™å¤åˆ¶åˆ°å·¥ä½œåŒº
    model_files = ["yolov8s-worldv2.pt", "sam2.1_l.pt"]
    for model_name in model_files:
        target_model_path = cfg.work_root / model_name
        local_model_path = Path(__file__).parent / model_name # è„šæœ¬æ‰€åœ¨ç›®å½•
        
        if not target_model_path.exists():
            if local_model_path.exists():
                print(f"    ğŸ“¦ æ£€æµ‹åˆ°æœ¬åœ°æ¨¡å‹ {model_name}ï¼Œæ­£åœ¨è¿ç§»è‡³å·¥ä½œåŒº...")
                shutil.copy2(str(local_model_path), str(target_model_path))
            else:
                # å¦‚æœéƒ½æ²¡æœ‰ï¼ŒUltralytics åº“ä¼šåœ¨è°ƒç”¨æ—¶è‡ªåŠ¨ä¸‹è½½
                print(f"    âš ï¸ æœªåœ¨è„šæœ¬ç›®å½•æ‰¾åˆ° {model_name}ï¼Œå°†å°è¯•è‡ªåŠ¨ä¸‹è½½...")

    try:
        # å®šä¹‰æ¨¡å‹è·¯å¾„
        yolo_path = cfg.work_root / "yolov8s-worldv2.pt"
        sam_path = cfg.work_root / "sam2.1_l.pt"
        
        # YOLO-World: å¼€æ”¾è¯æ±‡æ£€æµ‹æ¨¡å‹ï¼Œèƒ½â€œå¬æ‡‚â€æ–‡å­—å¹¶æ‰¾åˆ°æ¡†
        det_model = YOLOWorld(str(yolo_path) if yolo_path.exists() else "yolov8s-worldv2.pt") 
        # è®¾ç½® YOLO éœ€è¦å¯»æ‰¾çš„ç±»åˆ«
        det_model.set_classes([text_prompt])
        
        # SAM 2: åˆ†å‰²æ¨¡å‹ï¼Œæ ¹æ®æ¡†ï¼ˆBox Promptï¼‰æˆ–ç‚¹ï¼ˆPoint Promptï¼‰æŠ å›¾
        sam_model = SAM(str(sam_path) if sam_path.exists() else "sam2.1_l.pt") 
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return False

    # 3. è¯»å– transforms.json
    # [Python è¿›é˜¶] è¯»å– JSON åˆ°å­—å…¸
    with open(cfg.transforms_file, 'r') as f:
        meta = json.load(f)
    
    # [Python è¿›é˜¶] å»ºç«‹å“ˆå¸Œæ˜ å°„ (HashMap / Dict)
    # å°†æ–‡ä»¶åæ˜ å°„åˆ°å¸§æ•°æ®å¯¹è±¡ï¼Œåç»­æŸ¥æ‰¾å¤æ‚åº¦ä¸º O(1)ï¼Œé¿å…éå†åˆ—è¡¨
    # Path(f["file_path"]).name æå–å¦‚ "frame_0001.jpg"
    frames_map = {Path(f["file_path"]).name: f for f in meta["frames"]}
    
    image_files = sorted(list(images_dir.glob("*.jpg")) + list(images_dir.glob("*.png")))
    total_imgs = len(image_files)
    
    valid_frames_list = [] # å­˜æ”¾æ¸…æ´—åˆæ ¼åçš„å¸§æ•°æ®
    deleted_count = 0
    
    print(f"    -> å¼€å§‹å¤„ç† {total_imgs} å¼ å›¾ç‰‡...")

    # [Python è¿›é˜¶] enumerate ç”¨äºåŒæ—¶è·å–ç´¢å¼• i å’Œå…ƒç´  img_path
    for i, img_path in enumerate(image_files):
        # --- A. æ£€æµ‹ä¸åˆ†å‰² ---
        try:
            # 1. YOLO æ£€æµ‹
            # conf=0.05: åªè¦ç½®ä¿¡åº¦å¤§äº 5% å°±è®¤ä¸ºå¯èƒ½æœ‰ä¸œè¥¿
            det_results = det_model.predict(img_path, conf=0.05, verbose=False)
            
            # ============================================================
            # ğŸ•µï¸â€â™‚ï¸ [DEBUG æ¨¡å¼] è°ƒè¯•å¯è§†åŒ–ä»£ç å—
            # ============================================================
            debug_dir = data_dir / "debug_yolo_visuals"
            debug_dir.mkdir(parents=True, exist_ok=True)
            
            num_boxes = len(det_results[0].boxes)
            
            if num_boxes > 0:
                plotted_img = det_results[0].plot() # YOLO è‡ªå¸¦ç”»å›¾åŠŸèƒ½
                debug_path = debug_dir / f"debug_{img_path.name}"
                cv2.imwrite(str(debug_path), plotted_img)
                
                if i < 3: # åªæ‰“å°å‰3å¼ ï¼Œé˜²æ­¢åˆ·å±
                    print(f"\n    ğŸ‘€ [DEBUG] {img_path.name}: æ‰¾åˆ°äº† {num_boxes} ä¸ªç›®æ ‡")
            # ============================================================

            # è·å–æ£€æµ‹æ¡†åæ ‡ (xyxyæ ¼å¼: xmin, ymin, xmax, ymax)
            bboxes = det_results[0].boxes.xyxy.cpu() 

            # ============================================================
            # ğŸ”¥ æ ¸å¿ƒä¿®æ”¹ï¼šå¤šç›®æ ‡ç­›é€‰ (åªå–æœ€ä¸­é—´çš„ä¸€ä¸ª)
            # ============================================================
            if len(bboxes) > 1:
                import torch
                # è·å–åŸå›¾å°ºå¯¸
                img_h, img_w = det_results[0].orig_shape[:2]
                # è®¡ç®—å±å¹•ä¸­å¿ƒåæ ‡
                screen_center = torch.tensor([img_w / 2.0, img_h / 2.0])
                
                min_dist = float('inf') # åˆå§‹åŒ–æœ€å°è·ç¦»ä¸ºæ— ç©·å¤§
                best_idx = 0
                
                # éå†æ¯ä¸ªæ¡†ï¼Œè®¡ç®—å…¶ä¸­å¿ƒç‚¹åˆ°å±å¹•ä¸­å¿ƒçš„æ¬§æ°è·ç¦»
                for idx, box in enumerate(bboxes):
                    box_center_x = (box[0] + box[2]) / 2.0
                    box_center_y = (box[1] + box[3]) / 2.0
                    
                    dist = torch.sqrt((box_center_x - screen_center[0])**2 + (box_center_y - screen_center[1])**2)
                    
                    if dist < min_dist:
                        min_dist = dist
                        best_idx = idx
                
                # æ›´æ–° bboxesï¼Œåªä¿ç•™æœ€ä¸­å¿ƒçš„ä¸€ä¸ª
                # unsqueeze(0) ç”¨äºä¿æŒç»´åº¦ä¸º [1, 4]ï¼Œè€Œä¸æ˜¯å˜æˆ [4]
                bboxes = bboxes[best_idx].unsqueeze(0) 

            # ============================================================
            # ğŸ”¥ æ ¸å¿ƒä¿®æ”¹ï¼šä»â€œæ­»æ¡†â€æ”¹ä¸ºâ€œæ™ºèƒ½ä¸­å¿ƒç‚¹æ‰©æ•£â€
            # ============================================================
            
            use_point_prompt = False
            
            # å¦‚æœ YOLO æ²¡æ‰¾åˆ°ä»»ä½•ä¸œè¥¿
            if len(bboxes) == 0:
                print(f"       âš ï¸ YOLO æœªè¯†åˆ«åˆ°ç‰©ä½“ï¼Œåˆ‡æ¢ä¸º [SAM ä¸­å¿ƒç‚¹æ¨¡å¼]")
                h, w = det_results[0].orig_shape[:2]
                
                # [ç®—æ³•é€»è¾‘] ç›²çŒœä¸­å¿ƒï¼šå‡è®¾ç‰©ä½“åœ¨ç”»é¢æ­£ä¸­å¤®
                # ç»™ SAM ä¸€ä¸ªä¸­å¿ƒç‚¹æç¤ºï¼Œè®©å®ƒå°è¯•å‘å¤–æ‰©æ•£åˆ†å‰²
                use_point_prompt = True
            
            # 3. æ‰§è¡Œ SAM åˆ†å‰²
            if use_point_prompt:
                # æ„é€ ä¸€ä¸ªä½äºä¸­å¿ƒçš„æå°æ¡†ï¼Œæ¨¡æ‹Ÿç‚¹å‡»æ•ˆæœ
                cx, cy = w / 2, h / 2
                margin = 5 
                bboxes = torch.tensor([[cx-margin, cy-margin, cx+margin, cy+margin]], device=det_model.device)
                
                # è°ƒç”¨ SAM
                sam_results = sam_model(img_path, bboxes=bboxes, verbose=False)
            else:
                # ä½¿ç”¨ YOLO ç¡®å®šçš„æ¡†è°ƒç”¨ SAM
                sam_results = sam_model(img_path, bboxes=bboxes, verbose=False)
            
            # è·å– SAM ç»“æœ
            if sam_results[0].masks is not None:
                # masks.data æ˜¯ [N, H, W] çš„ tensor
                all_masks = sam_results[0].masks.data.cpu().numpy()
                # [Python è¿›é˜¶] np.any(axis=0) å°†æ‰€æœ‰æ£€æµ‹åˆ°çš„ mask åˆå¹¶ï¼ˆé€»è¾‘æˆ–ï¼‰
                final_mask = np.any(all_masks, axis=0).astype(np.uint8) * 255
            else:
                final_mask = np.zeros(det_results[0].orig_shape[:2], dtype=np.uint8)

            # -------------------------------------------------
            # ğŸ”¥ æ ¸å¿ƒä¿®æ”¹ï¼šè°ƒç”¨æ¸…æ´—å‡½æ•°è¿›è¡Œè´¨æ£€ ğŸ”¥
            # -------------------------------------------------
            # è¿™é‡Œè°ƒç”¨äº†å‰é¢å®šä¹‰çš„ clean_and_verify_mask
            is_good, cleaned_mask, reason = clean_and_verify_mask(final_mask, img_path.name)

            if is_good:
                # âœ… åˆæ ¼é€»è¾‘
                original_img = cv2.imread(str(img_path))
                if original_img is not None:
                    # [ç®—æ³•é€»è¾‘] ç¾½åŒ– (Feathering)
                    # å¯¹ Mask è¿›è¡Œé«˜æ–¯æ¨¡ç³Šï¼Œä½¿è¾¹ç¼˜åŠé€æ˜ï¼Œé¿å…åˆæˆæ—¶å‡ºç°é”¯é½¿
                    mask_blurred = cv2.GaussianBlur(cleaned_mask, (5, 5), 0)
                    
                    # å½’ä¸€åŒ– Alpha é€šé“ (0.0 - 1.0)
                    alpha_channel = mask_blurred.astype(np.float32) / 255.0
                    img_float = original_img.astype(np.float32)
                    
                    # [ç®—æ³•é€»è¾‘] é¢„ä¹˜ Alpha (Premultiplied Alpha)
                    # æ ‡å‡†çš„å›¾å½¢å­¦æ“ä½œï¼šRGB = RGB * Alpha
                    # è¿™æ ·èƒŒæ™¯åŒºåŸŸå°±ä¼šå˜æˆçº¯é»‘ (0,0,0)
                    b, g, r = cv2.split(img_float)
                    b = b * alpha_channel
                    g = g * alpha_channel
                    r = r * alpha_channel
                    
                    # åˆå¹¶é€šé“ç”Ÿæˆ BGRA å›¾ç‰‡
                    img_bgra = cv2.merge([
                        b.astype(np.uint8), 
                        g.astype(np.uint8), 
                        r.astype(np.uint8), 
                        mask_blurred # Alpha é€šé“
                    ])
                    
                    # ä¿å­˜ä¸º PNG (JPG ä¸æ”¯æŒé€æ˜é€šé“)
                    new_img_path = img_path.with_suffix('.png')
                    cv2.imwrite(str(new_img_path), img_bgra)
                    
                    # åˆ é™¤æ—§çš„ JPG æ–‡ä»¶ä»¥èŠ‚çœç©ºé—´å’Œé¿å…æ··æ·†
                    if img_path.suffix.lower() == '.jpg':
                        try: img_path.unlink()
                        except: pass
                        
                    final_img_path_name = new_img_path.name
                else:
                    final_img_path_name = img_path.name

                # ä¿å­˜ Mask ä¾›è°ƒè¯•æˆ–è®­ç»ƒä½¿ç”¨
                cv2.imwrite(str(masks_dir / f"{img_path.stem}.png"), cleaned_mask)

                # æ›´æ–° JSON å…ƒæ•°æ®
                if img_path.name in frames_map:
                    frame_data = frames_map[img_path.name]
                    # ä¿®æ”¹æ–‡ä»¶è·¯å¾„æŒ‡å‘æ–°çš„ PNG
                    frame_data["file_path"] = f"images/{final_img_path_name}" 
                    valid_frames_list.append(frame_data)

            else:
                # âŒ ä¸åˆæ ¼é€»è¾‘
                print(f"       ğŸ—‘ï¸ [å‰”é™¤] {img_path.name}: {reason}")
                # [å·¥ç¨‹åŒ–æ€è·¯] ç‰©ç†åˆ é™¤è´¨é‡å·®çš„æ•°æ®ï¼Œé˜²æ­¢è¿›å…¥è®­ç»ƒæµç¨‹æ±¡æŸ“æ¨¡å‹
                img_path.unlink() 
                deleted_count += 1
                # valid_frames_list ä¸­ä¸æ·»åŠ è¯¥å¸§ï¼Œç›¸å½“äºåœ¨ transforms.json ä¸­ä¹Ÿåˆ é™¤äº†

        except Exception as e:
            # å¼‚å¸¸å¤„ç†ï¼šå•ä¸ªå›¾ç‰‡å¤„ç†å¤±è´¥ä¸å½±å“æ•´ä½“æµç¨‹
            print(f"       âŒ å¤„ç†å‡ºé”™ {img_path.name}: {e}")
            continue

        # è¿›åº¦æ¡æ‰“å°
        if i % 10 == 0:
            # end="\r" å®ç°å•è¡Œåˆ·æ–°
            print(f"       è¿›åº¦: {i}/{total_imgs} (å·²å‰”é™¤ {deleted_count} å¼ )...", end="\r")

    # 4. ç»“ç®—ä¸æ›´æ–°
    print(f"\n\nğŸ“Š ç­›é€‰æŠ¥å‘Š:")
    print(f"   - åŸå§‹æ€»æ•°: {total_imgs}")
    print(f"   - å‰”é™¤æ•°é‡: {deleted_count} ({deleted_count/total_imgs:.1%})")
    print(f"   - å‰©ä½™å¯ç”¨: {len(valid_frames_list)}")

    if len(valid_frames_list) == 0:
        print("âŒ é”™è¯¯ï¼šæ‰€æœ‰å›¾ç‰‡éƒ½è¢«å‰”é™¤äº†ï¼è¯·æ£€æŸ¥æç¤ºè¯æˆ–æ‹æ‘„è´¨é‡ã€‚")
        return False

    # 5. é‡å†™ transforms.json
    # [å…³é”®æ­¥éª¤] ç”¨æ¸…æ´—åçš„ valid_frames_list è¦†ç›–åŸå§‹æ•°æ®
    # è¿™æ · Nerfstudio è®­ç»ƒæ—¶å°±åªä¼šè¯»å–åˆ°å¹²å‡€ã€å¸¦æœ‰é€æ˜é€šé“çš„å›¾ç‰‡
    meta["frames"] = valid_frames_list
    with open(cfg.transforms_file, 'w') as f:
        json.dump(meta, f, indent=4)
        
    print(f"    âœ… transforms.json å·²æ›´æ–°ï¼Œæ•°æ®é›†å·²æ¸…æ´—å®Œæ¯•ã€‚")
    return True

# ================= è¾…åŠ©å·¥å…· =================

def format_duration(seconds):

    """
    [è¾…åŠ©å‡½æ•°] å°†ç§’æ•°è½¬æ¢ä¸ºæ˜“è¯»çš„ HH:MM:SS æ ¼å¼
    """
    # [æ ‡å‡†åº“] datetime.timedelta è‡ªåŠ¨å¤„ç†æ—¶é—´æ¢ç®—ï¼ˆå¦‚ 3661ç§’ -> 1:01:01ï¼‰
    return str(datetime.timedelta(seconds=int(seconds)))

class ImageProcessor:
    def __init__(self, config: PipelineConfig):
        self.cfg = config

    def smart_filter_blurry_images(self, image_folder, keep_ratio=0.85):
        """
        [å›¾åƒæ¸…æ´—ç®—æ³•] æ··åˆç­–ç•¥æ¨¡ç³Šæ£€æµ‹
        """
        print(f"\nğŸ§  [æ™ºèƒ½æ¸…æ´—] æ­£åœ¨åˆ†æå›¾ç‰‡è´¨é‡ (æ··åˆç­–ç•¥ç‰ˆ)...")
        image_dir = Path(image_folder)
        images = sorted([p for p in image_dir.iterdir() if p.suffix.lower() in ['.jpg', '.jpeg', '.png']])
        if not images: return
        
        trash_dir = image_dir.parent / "trash_smart"
        trash_dir.mkdir(exist_ok=True)
        
        # --- è¿™é‡Œçš„ä»£ç ä¿æŒä¸å˜ï¼Œç›´åˆ° good_images è®¡ç®—å®Œæ¯• ---
        img_scores = []
        for i, img_path in enumerate(images):
            img = cv2.imread(str(img_path))
            if img is None: continue
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            h, w = gray.shape
            grid_h, grid_w = h // 3, w // 3
            max_grid_score = 0
            for r in range(3):
                for c in range(3):
                    roi = gray[r*grid_h:(r+1)*grid_h, c*grid_w:(c+1)*grid_w]
                    score = cv2.Laplacian(roi, cv2.CV_64F).var()
                    if score > max_grid_score: max_grid_score = score
            img_scores.append((img_path, max_grid_score))
            if i % 50 == 0: print(f"  -> åˆ†æä¸­... {i}/{len(images)}", end="\r")
        
        scores = [s[1] for s in img_scores]
        if not scores: return
        quality_threshold = np.percentile(scores, (1 - keep_ratio) * 100)
        
        good_images = []
        for img_path, score in img_scores:
            if score < quality_threshold:
                shutil.move(str(img_path), str(trash_dir / img_path.name))
            else:
                good_images.append(img_path)
        
        # ======================================================
        # ğŸ”¥ æ ¸å¿ƒä¿®æ”¹åœ¨è¿™é‡Œ ğŸ”¥
        # ======================================================
        # ä»é…ç½®å¯¹è±¡ä¸­è¯»å–æœ€å¤§å›¾ç‰‡æ•°é‡
        max_imgs = self.cfg.max_images  
        
        # ä½¿ç”¨ max_imgs æ›¿ä»£åŸæ¥çš„ max_images
        if len(good_images) > max_imgs:
            print(f"    âš ï¸ å›¾ç‰‡è¿‡å¤š ({len(good_images)} å¼ ), æ­£åœ¨é™é‡‡æ ·è‡³ {max_imgs} å¼ ...")
            # np.linspace ç”Ÿæˆå‡åŒ€åˆ†å¸ƒçš„ç´¢å¼•
            indices_to_keep = set(np.linspace(0, len(good_images) - 1, max_imgs, dtype=int))
            for idx, img_path in enumerate(good_images):
                if idx not in indices_to_keep:
                    shutil.move(str(img_path), str(trash_dir / img_path.name))
                    
        print(f"âœ¨ æ¸…æ´—ç»“æŸï¼Œå‰©ä½™ {len(list(image_dir.glob('*')))} å¼ ã€‚")

def analyze_and_calculate_adaptive_collider(json_path, force_cull=False, radius_scale=1.8):
    """
    [3D åœºæ™¯ç†è§£ç®—æ³•] è§£æç›¸æœºè½¨è¿¹ï¼Œè‡ªåŠ¨åˆ¤æ–­åœºæ™¯ç±»å‹å¹¶è®¡ç®—åŒ…å›´ç›’ (Collider)
    é€»è¾‘ï¼š
    1. è¯»å– transforms.json è·å–æ‰€æœ‰ç›¸æœºä½å§¿ã€‚
    2. è®¡ç®—æ‰€æœ‰ç›¸æœºçš„è§†çº¿å‘é‡ä¸â€œç›¸æœºä¸­å¿ƒ-åœºæ™¯ä¸­å¿ƒâ€å‘é‡çš„ç‚¹ç§¯ã€‚
    3. å¦‚æœå¤§éƒ¨åˆ†ç›¸æœºéƒ½çœ‹å‘ä¸­å¿ƒ -> Object Mode (ç‰©ä½“æ¨¡å¼)ã€‚
    4. å¦‚æœç›¸æœºå‘å››é¢å…«æ–¹çœ‹ -> Scene Mode (åœºæ™¯æ¨¡å¼)ã€‚
    """
    print(f"\nğŸ¤– [AI åˆ†æ] è§£æç›¸æœºè½¨è¿¹...")
    try:
        with open(json_path, 'r') as f: data = json.load(f)
        frames = data["frames"]
        if not frames: return [], "unknown"
        
        has_mask = "mask_path" in frames[0]
        if has_mask:
            print("    -> æ£€æµ‹åˆ° Mask æ•°æ®ï¼å°†å¯ç”¨ç‰©ä½“èšç„¦æ¨¡å¼ã€‚")
        
        # [çº¿æ€§ä»£æ•°] æå–æ‰€æœ‰ç›¸æœºçš„ä½ç§» (Translation)
        # transform_matrix æ˜¯ 4x4 çŸ©é˜µï¼Œ[:3, 3] æ˜¯ XYZ åæ ‡
        positions = [np.array(f["transform_matrix"])[:3, 3] for f in frames]
        
        # æå–ç›¸æœºçš„å‰å‘å‘é‡ (Forward Vector)
        # åœ¨ OpenCV/Colmap å®šä¹‰ä¸­ï¼Œ+Z è½´é€šå¸¸æ˜¯ç›¸æœºçœ‹å‘çš„æ–¹å‘ï¼Œæˆ–è€… -Zï¼Œéœ€æ ¹æ®å…·ä½“åæ ‡ç³»åˆ¤å®š
        # è¿™é‡Œå‡è®¾ -Z æ˜¯å‰æ–¹ (NeRF å¸¸ç”¨çº¦å®š)
        forward_vectors = [np.array(f["transform_matrix"])[:3, :3] @ np.array([0, 0, -1]) for f in frames]
        
        # è®¡ç®—æ‰€æœ‰ç›¸æœºä½ç½®çš„å‡ ä½•ä¸­å¿ƒ
        center = np.mean(positions, axis=0)
        
        # è®¡ç®—æ¯ä¸ªç›¸æœºä½ç½®æŒ‡å‘åœºæ™¯ä¸­å¿ƒçš„å‘é‡
        vec_to_center = center - positions
        # å½’ä¸€åŒ–å‘é‡ (é™¤ä»¥æ¨¡é•¿)
        vec_to_center /= (np.linalg.norm(vec_to_center, axis=1, keepdims=True) + 1e-6)
        
        # [æ ¸å¿ƒç®—æ³•] è®¡ç®—â€œè§†çº¿â€ä¸â€œæŒ‡å‘ä¸­å¿ƒå‘é‡â€çš„å¯¹é½ç¨‹åº¦
        # ç‚¹ç§¯ > 0 è¡¨ç¤ºæ–¹å‘åŸºæœ¬ä¸€è‡´ï¼ˆå¤¹è§’å°äº90åº¦ï¼‰
        # å¦‚æœ ratio > 0.6ï¼Œè¯´æ˜è¶…è¿‡ 60% çš„ç›¸æœºéƒ½çœ‹å‘ä¸­å¿ƒåŒºåŸŸ
        ratio = np.sum(np.sum(forward_vectors * vec_to_center, axis=1) > 0) / len(frames)
        
        # ç»¼åˆåˆ¤å®šï¼šå‘å¿ƒç‡é«˜ OR å¼ºåˆ¶å¼€å¯çƒå½¢è£å‰ª OR æœ‰ Mask
        is_object_mode = ratio > 0.6 or force_cull or has_mask

        if is_object_mode:
            # ç‰©ä½“æ¨¡å¼ï¼šè®¾ç½®ç´§å‡‘çš„ Near/Far Plane
            dists = [np.linalg.norm(p) for p in positions] # ç›¸æœºåˆ°åŸç‚¹çš„è·ç¦»
            avg_dist = np.mean(dists)
            
            scene_radius = 1.0 * radius_scale  # åœºæ™¯åŠå¾„
            
            # è®¡ç®— Near Plane (è¿‘å¹³é¢)ï¼šä¸èƒ½å¤ªè¿‘ï¼Œå¦åˆ™ä¼šåˆ‡æ‰ç›¸æœºå‰çš„ç‰©ä½“
            calc_near = max(0.05, min(dists) - scene_radius)
            # è®¡ç®— Far Plane (è¿œå¹³é¢)ï¼šåªè¦åŒ…ä½ç‰©ä½“å³å¯
            calc_far = avg_dist + scene_radius
            
            # è¿”å› nerfstudio éœ€è¦çš„è®­ç»ƒå‚æ•°
            return ["--pipeline.model.enable-collider", "True", 
                    "--pipeline.model.collider-params", "near_plane", str(round(calc_near, 2)), 
                    "far_plane", str(round(calc_far, 2))], "object"
        else:
            # åœºæ™¯æ¨¡å¼ï¼šç©ºé—´å¾ˆå¤§ï¼ŒFar Plane è®¾è¿œä¸€ç‚¹
            return ["--pipeline.model.enable-collider", "True", 
                    "--pipeline.model.collider-params", "near_plane", "0.05", "far_plane", "100.0"], "scene"
    except:
        return [], "unknown"

def perform_percentile_culling(ply_path, json_path, output_path, keep_percentile=0.9):
    """
    [ç‚¹äº‘åå¤„ç†] åŸºäºç»Ÿè®¡åˆ†ä½æ•°çš„æš´åŠ›åˆ‡å‰²
    åŠŸèƒ½ï¼šå»é™¤ Gaussian Splatting è®­ç»ƒåäº§ç”Ÿåœ¨è¿œå¤„çš„èƒŒæ™¯ä¼ªå½±ã€‚
    ä¾èµ–ï¼šplyfile åº“
    """
    # æ£€æŸ¥ä¾èµ–
    if not HAS_PLYFILE: return False
    print(f"\nâœ‚ï¸ [åå¤„ç†] æ­£åœ¨æ‰§è¡Œã€åˆ†ä½æ•°æš´åŠ›åˆ‡å‰²ã€‘...")
    try:
        # 1. è®¡ç®—åœºæ™¯ä¸­å¿ƒ
        with open(json_path, 'r') as f: frames = json.load(f)["frames"]
        cam_pos = np.array([np.array(f["transform_matrix"])[:3, 3] for f in frames])
        center = np.mean(cam_pos, axis=0)
        
        # 2. è¯»å– PLY ç‚¹äº‘æ•°æ®
        plydata = PlyData.read(str(ply_path))
        vertex = plydata['vertex']
        # å †å  x,y,z åæ ‡
        points = np.stack([vertex['x'], vertex['y'], vertex['z']], axis=1)
        
        # 3. è®¡ç®—æ‰€æœ‰ç‚¹åˆ°ä¸­å¿ƒçš„è·ç¦»
        dists_pts = np.linalg.norm(points - center, axis=1)

        # [ç®—æ³•é€»è¾‘] ç¡®å®šé˜ˆå€¼åŠå¾„
        # 2. âœ… è¿™é‡Œä¿®æ”¹ï¼šä½¿ç”¨ä¼ å…¥çš„å‚æ•° keep_percentile
        threshold_radius = np.percentile(dists_pts, keep_percentile * 100)
        
        # 4. è¯»å–ä¸é€æ˜åº¦ (Opacity) å¹¶è¿‡æ»¤
        # Gaussian Splatting å­˜å‚¨çš„ opacity é€šå¸¸ç»è¿‡ sigmoid æ¿€æ´»ï¼Œéœ€è¦è¿˜åŸ
        # è¿™é‡Œ simplified: å‡è®¾ vertex['opacity'] æ˜¯ logit
        opacities = 1 / (1 + np.exp(-vertex['opacity']))
        
        # è”åˆæ©ç ï¼š(åœ¨åŠå¾„å†…) AND (ä¸é€æ˜åº¦ > 0.05)
        mask = (dists_pts < threshold_radius) & (opacities > 0.05)
        filtered_vertex = vertex[mask]
        
        # 5. å†™å…¥æ–°æ–‡ä»¶
        PlyData([PlyElement.describe(filtered_vertex, 'vertex')]).write(str(output_path))
        return True
    except Exception as e:
        print(f"âŒ åˆ‡å‰²å¤±è´¥: {e}")
        return False


# ==============================================================================
# æ¨¡å—åŒ–ï¼šGLOMAP ä½å§¿è§£ç®—ç±» (ç¯å¢ƒéš”ç¦»å¢å¼ºç‰ˆ)
# ==============================================================================
class GlomapRunner:
    def __init__(self, cfg: PipelineConfig):
        self.cfg = cfg
        
        # 1. æŸ¥æ‰¾ COLMAP (ä¼˜å…ˆä½¿ç”¨ Conda ç¯å¢ƒè‡ªå¸¦çš„ï¼)
        self.colmap_exe = shutil.which("colmap")
        if not self.colmap_exe:
            if os.path.exists("/usr/local/bin/colmap"):
                self.colmap_exe = "/usr/local/bin/colmap"
        
        # 2. æŸ¥æ‰¾ GLOMAP
        self.glomap_exe = shutil.which("glomap")
        if not self.glomap_exe:
            if os.path.exists("/usr/local/bin/glomap"):
                self.glomap_exe = "/usr/local/bin/glomap"

        if not self.colmap_exe or not self.glomap_exe:
            raise FileNotFoundError("âŒ ç¼ºå°‘ colmap æˆ– glomap å¯æ‰§è¡Œæ–‡ä»¶")

        print(f"    -> ğŸ¯ é”å®šå¼•æ“: COLMAP={self.colmap_exe}")
        print(f"    -> ğŸ¯ é”å®šå¼•æ“: GLOMAP={self.glomap_exe}")
        
        self.env = os.environ.copy()
        self.env["SETUPTOOLS_USE_DISTUTILS"] = "stdlib"

    def run(self):
        """æ‰§è¡Œ GLOMAP å®Œæ•´æµç¨‹"""
        print(f"\nğŸ“ [2/4] GLOMAP ä½å§¿è§£ç®— (Global Mapping)")

        # è·¯å¾„å‡†å¤‡
        raw_images_dir = self.cfg.project_dir / "raw_images"
        dest_images_dir = self.cfg.images_dir
        dest_images_dir.mkdir(parents=True, exist_ok=True)
        for img in raw_images_dir.glob("*"):
            if not (dest_images_dir / img.name).exists():
                shutil.copy2(str(img), str(dest_images_dir / img.name))

        colmap_output_dir = self.cfg.data_dir / "colmap"
        colmap_output_dir.mkdir(parents=True, exist_ok=True)
        database_path = colmap_output_dir / "database.db"
        sparse_dir = colmap_output_dir / "sparse"

        try:
            # æ¸…ç†
            if database_path.exists(): database_path.unlink()
            if sparse_dir.exists(): shutil.rmtree(sparse_dir)
            sparse_dir.mkdir(parents=True, exist_ok=True)
            if self.cfg.transforms_file.exists(): self.cfg.transforms_file.unlink()

            # Step 1: ç‰¹å¾æå–
            self._run_cmd([
                self.colmap_exe, "feature_extractor",
                "--database_path", str(database_path),
                "--image_path", str(raw_images_dir),
                "--ImageReader.camera_model", "OPENCV",
                "--ImageReader.single_camera", "1"
            ], "Step 1: ç‰¹å¾æå– (COLMAP)")

            # Step 2: é¡ºåºåŒ¹é…
            self._run_cmd([
                self.colmap_exe, "sequential_matcher",
                "--database_path", str(database_path),
                "--SequentialMatching.overlap", "25"
            ], "Step 2: é¡ºåºåŒ¹é… (COLMAP)")

            # Step 3: å…¨å±€é‡å»º
            print(f"    -> ğŸš€ å¯åŠ¨ GLOMAP å¼•æ“...")
            self._run_cmd([
                self.glomap_exe, "mapper",
                "--database_path", str(database_path),
                "--image_path", str(raw_images_dir),
                "--output_path", str(sparse_dir)
            ], "Step 3: å…¨å±€æ˜ å°„ (GLOMAP)")

            # Step 4: ç›®å½•ä¿®æ­£
            self._fix_directory_structure(sparse_dir)

            # Step 5: ç”Ÿæˆ json
            self._run_cmd([
                "ns-process-data", "images",
                "--data", str(dest_images_dir),
                "--output-dir", str(self.cfg.data_dir),
                "--skip-colmap",
                "--skip-image-processing",
                "--num-downscales", "0"
            ], "ç”Ÿæˆ transforms.json")

            # Step 6: æ£€æŸ¥
            if self._check_quality(raw_images_dir):
                print(f"    âœ¨ GLOMAP æµç¨‹æˆåŠŸï¼")
                return True

        except Exception as e:
            print(f"    âŒ GLOMAP æµç¨‹å¤±è´¥: {e}")
            return False
        return False

    def _run_cmd(self, cmd, desc):
        """å†…éƒ¨å·¥å…·ï¼šæ‰§è¡Œå‘½ä»¤ (å«ç¯å¢ƒéš”ç¦»é€»è¾‘)"""
        print(f"ğŸš€ {desc}...")
        
        # ğŸ”¥ ç¯å¢ƒéš”ç¦»é€»è¾‘ ğŸ”¥
        cmd_env = self.env.copy()
        exe_path = cmd[0]
        # å¦‚æœæ˜¯ç³»ç»Ÿç¨‹åº (/usr/local/bin/glomap)ï¼Œæ¸…é™¤ LD_LIBRARY_PATH é˜²æ­¢ Conda å¹²æ‰°
        if exe_path.startswith("/usr") or exe_path.startswith("/bin"):
            if "LD_LIBRARY_PATH" in cmd_env:
                del cmd_env["LD_LIBRARY_PATH"]

        try:
            process = subprocess.Popen(
                cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, env=cmd_env
            )
            for line in process.stdout:
                if any(k in line for k in ["Error", "Warning", "Elapsed", "image pairs"]):
                    print(f"    | {line.strip()}")
            process.wait()
            if process.returncode != 0:
                raise subprocess.CalledProcessError(process.returncode, cmd)
        except subprocess.CalledProcessError as e:
            print(f"âŒ å‘½ä»¤æ‰§è¡Œå´©æºƒ: {cmd[0]} (ä»£ç  {e.returncode})")
            raise e

    def _fix_directory_structure(self, sparse_root):
        target_dir_0 = sparse_root / "0"
        target_dir_0.mkdir(parents=True, exist_ok=True)
        required_files = ["cameras.bin", "images.bin", "points3D.bin"]
        required_files_txt = ["cameras.txt", "images.txt", "points3D.txt"]
        model_found = False
        for root, dirs, files in os.walk(sparse_root):
            if all(f in files for f in required_files):
                src = Path(root)
                if src != target_dir_0:
                    for f in required_files:
                        if (target_dir_0/f).exists(): (target_dir_0/f).unlink()
                        shutil.move(str(src/f), str(target_dir_0/f))
                model_found = True
                break
            if all(f in files for f in required_files_txt):
                src = Path(root)
                if src != target_dir_0:
                    for f in required_files_txt:
                        if (target_dir_0/f).exists(): (target_dir_0/f).unlink()
                        shutil.move(str(src/f), str(target_dir_0/f))
                model_found = True
                break
        if not model_found: raise RuntimeError("GLOMAP æœªç”Ÿæˆæœ‰æ•ˆçš„ç¨€ç–æ¨¡å‹æ–‡ä»¶ï¼")

    def _check_quality(self, raw_images_dir):
        if not self.cfg.transforms_file.exists(): return False
        with open(self.cfg.transforms_file, 'r') as f: meta = json.load(f)
        reg_count = len(meta["frames"])
        total_count = len(list(raw_images_dir.glob("*.jpg")) + list(raw_images_dir.glob("*.png")))
        ratio = reg_count / total_count if total_count > 0 else 0
        print(f"    ğŸ“Š åŒ¹é…ç‡: {ratio:.2%} ({reg_count}/{total_count})")
        return ratio > 0.2

# ==============================================================================
# æ¨¡å—åŒ–ï¼šAI è¯­ä¹‰åˆ†å‰²ç±»
# ==============================================================================
class AISegmentor:
    def __init__(self, cfg: PipelineConfig):
        self.cfg = cfg
        self.data_dir = cfg.data_dir
        self.images_dir = cfg.images_dir
        self.masks_dir = cfg.masks_dir

    def run(self):
        """æ‰§è¡Œ AI åˆ†å‰²æ€»æµæ°´çº¿ (å¯¹åº”åŸ run_ai_segmentation_pipeline)"""
        if not HAS_AI or not self.cfg.enable_ai:
            print("â© è·³è¿‡ AI åˆ†å‰² (æœªå¯ç”¨æˆ–ç¼ºå°‘ä¾èµ–)")
            return False
            
        if not self.cfg.transforms_file.exists():
            print("âš ï¸ transforms.json ä¸å­˜åœ¨ï¼Œæ— æ³•è¿›è¡Œ AI åˆ†å‰²")
            return False

        print(f"\nâœ‚ï¸ [AI åˆ†å‰²] æ­£åœ¨åˆå§‹åŒ–...")
        self.masks_dir.mkdir(parents=True, exist_ok=True)

        # 1. è·å–æç¤ºè¯
        text_prompt = self._get_prompt()
        
        # 2. åŠ è½½æ¨¡å‹
        try:
            # è‡ªåŠ¨è¿ç§»æ¨¡å‹æ–‡ä»¶é€»è¾‘
            self._ensure_model_exists("yolov8s-worldv2.pt")
            self._ensure_model_exists("sam2.1_l.pt")
            
            yolo_path = self.cfg.work_root / "yolov8s-worldv2.pt"
            sam_path = self.cfg.work_root / "sam2.1_l.pt"
            
            print("    -> æ­£åœ¨åŠ è½½ AI æ¨¡å‹...")
            det_model = YOLOWorld(str(yolo_path))
            det_model.set_classes([text_prompt])
            sam_model = SAM(str(sam_path))
        except Exception as e:
            print(f"âŒ AI æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False

        # 3. è¯»å–å…ƒæ•°æ®
        with open(self.cfg.transforms_file, 'r') as f: meta = json.load(f)
        frames_map = {Path(f["file_path"]).name: f for f in meta["frames"]}
        
        image_files = sorted(list(self.images_dir.glob("*.jpg")) + list(self.images_dir.glob("*.png")))
        valid_frames_list = []
        deleted_count = 0
        
        print(f"    -> å¼€å§‹å¤„ç† {len(image_files)} å¼ å›¾ç‰‡...")

        # 4. å¾ªç¯å¤„ç†
        for i, img_path in enumerate(image_files):
            try:
                # YOLO æ£€æµ‹
                det_results = det_model.predict(img_path, conf=0.05, verbose=False)
                bboxes = det_results[0].boxes.xyxy.cpu()
                
                # ç­›é€‰ä¸­å¿ƒæ¡† (é€»è¾‘ä¸ä¹‹å‰ç›¸åŒï¼Œè¿™é‡Œç®€åŒ–å±•ç¤º)
                if len(bboxes) > 1:
                    bboxes = self._pick_center_box(bboxes, det_results[0].orig_shape)
                
                # SAM åˆ†å‰²
                if len(bboxes) == 0:
                    # ä¸­å¿ƒç‚¹æ¨¡å¼
                    h, w = det_results[0].orig_shape[:2]
                    cx, cy, margin = w / 2, h / 2, 5
                    bboxes = [[cx-margin, cy-margin, cx+margin, cy+margin]]
                
                sam_results = sam_model(img_path, bboxes=bboxes, verbose=False)
                
                # åˆå¹¶ Mask
                if sam_results[0].masks is not None:
                    final_mask = np.any(sam_results[0].masks.data.cpu().numpy(), axis=0).astype(np.uint8) * 255
                else:
                    final_mask = np.zeros(det_results[0].orig_shape[:2], dtype=np.uint8)

                # æ¸…æ´— Mask (è°ƒç”¨å†…éƒ¨æ–¹æ³•)
                is_good, cleaned_mask, reason = self._clean_and_verify_mask(final_mask)
                
                if is_good:
                    final_name = self._save_transparent_png(img_path, cleaned_mask)
                    if img_path.name in frames_map:
                        frame_data = frames_map[img_path.name]
                        frame_data["file_path"] = f"images/{final_name}"
                        valid_frames_list.append(frame_data)
                else:
                    print(f"       ğŸ—‘ï¸ [å‰”é™¤] {img_path.name}: {reason}")
                    img_path.unlink()
                    deleted_count += 1

            except Exception as e:
                print(f"       âŒ é”™è¯¯ {img_path.name}: {e}")
                continue
            
            if i % 10 == 0: print(f"       è¿›åº¦: {i}/{len(image_files)}...", end="\r")

        # 5. æ›´æ–° json
        if valid_frames_list:
            meta["frames"] = valid_frames_list
            with open(self.cfg.transforms_file, 'w') as f: json.dump(meta, f, indent=4)
            print(f"\n    âœ… AI å¤„ç†å®Œæˆï¼Œå‰©ä½™å¯ç”¨: {len(valid_frames_list)}")
            return True
        else:
            print("\nâŒ é”™è¯¯ï¼šæ‰€æœ‰å›¾ç‰‡éƒ½è¢«å‰”é™¤äº†")
            return False

    def _get_prompt(self):
        """åŸ get_central_object_prompt çš„å°è£…"""
        # è¿™é‡Œä½ å¯ä»¥è°ƒç”¨ä¹‹å‰å®šä¹‰çš„å…¨å±€å‡½æ•° get_central_object_prompt(self.images_dir)
        # æˆ–è€…æŠŠé‚£æ®µä»£ç æ¬è¿›æ¥ã€‚ä¸ºäº†çœäº‹ï¼Œå»ºè®®ç›´æ¥è°ƒç”¨ç°æœ‰çš„å…¨å±€å‡½æ•°ï¼š
        try:
            prompt = get_central_object_prompt(self.images_dir)
            return prompt if prompt else "central object"
        except:
            return "central object"

    def _ensure_model_exists(self, model_name):
        target = self.cfg.work_root / model_name
        local = Path(__file__).parent / model_name
        if not target.exists() and local.exists():
            shutil.copy2(str(local), str(target))

    def _pick_center_box(self, bboxes, img_shape):
        """ç­›é€‰æœ€ä¸­å¿ƒçš„æ¡†"""
        import torch
        img_h, img_w = img_shape[:2]
        screen_center = torch.tensor([img_w / 2.0, img_h / 2.0])
        min_dist = float('inf')
        best_idx = 0
        for idx, box in enumerate(bboxes):
            cx = (box[0] + box[2]) / 2.0
            cy = (box[1] + box[3]) / 2.0
            dist = torch.sqrt((cx - screen_center[0])**2 + (cy - screen_center[1])**2)
            if dist < min_dist:
                min_dist = dist
                best_idx = idx
        return bboxes[best_idx].unsqueeze(0)

    def _clean_and_verify_mask(self, mask):
        """åŸ clean_and_verify_mask çš„å°è£…"""
        # ç›´æ¥è°ƒç”¨ä¹‹å‰çš„å…¨å±€å‡½æ•°å³å¯
        return clean_and_verify_mask(mask)

    def _save_transparent_png(self, img_path, mask):
        """åˆæˆå¹¶ä¿å­˜ PNG"""
        img = cv2.imread(str(img_path))
        mask_blurred = cv2.GaussianBlur(mask, (5, 5), 0)
        alpha = mask_blurred.astype(np.float32) / 255.0
        img_float = img.astype(np.float32)
        b, g, r = cv2.split(img_float)
        img_bgra = cv2.merge([
            (b * alpha).astype(np.uint8),
            (g * alpha).astype(np.uint8),
            (r * alpha).astype(np.uint8),
            mask_blurred
        ])
        new_path = img_path.with_suffix('.png')
        cv2.imwrite(str(new_path), img_bgra)
        if img_path.suffix.lower() == '.jpg':
            try: img_path.unlink()
            except: pass
        return new_path.name
        

# ==============================================================================
# æ¨¡å—åŒ–ï¼šNerfstudio è®­ç»ƒå¼•æ“ç±»
# ==============================================================================
class NerfstudioEngine:
    def __init__(self, cfg: PipelineConfig):
        self.cfg = cfg
        self.output_dir = cfg.project_dir / "outputs"
        # å‡†å¤‡ç¯å¢ƒå˜é‡
        self.env = os.environ.copy()
        self.env["QT_QPA_PLATFORM"] = "offscreen"
        self.env["SETUPTOOLS_USE_DISTUTILS"] = "stdlib"

    def train(self):
        """æ‰§è¡Œ splatfacto è®­ç»ƒ"""
        print(f"\nğŸ”¥ [4/4] å¼€å§‹è®­ç»ƒ (Splatfacto)")
        
        # 1. è®¡ç®—åœºæ™¯å‚æ•° (Collider) - ç›´æ¥è°ƒç”¨ä¹‹å‰çš„å…¨å±€å‡½æ•°
        collider_args, scene_type = analyze_and_calculate_adaptive_collider(
            self.cfg.transforms_file,
            force_cull=self.cfg.force_spherical_culling,
            radius_scale=self.cfg.scene_radius_scale
        )
        self.scene_type = scene_type # å­˜ä¸‹æ¥ç»™å¯¼å‡ºæ­¥éª¤ç”¨

        # 2. ç»„è£…å‘½ä»¤
        cmd = [
            "ns-train", "splatfacto",
            "--data", str(self.cfg.data_dir),
            "--output-dir", str(self.output_dir),
            "--experiment-name", self.cfg.project_name,
            "--pipeline.model.random-init", "False",
            "--pipeline.model.background-color", "random",
            "--pipeline.model.cull-alpha-thresh", "0.05",
            "--pipeline.model.stop-split-at", "10000",
            *collider_args,
            "--max-num-iterations", "15000",
            "--vis", "viewer+tensorboard",
            "--viewer.quit-on-train-completion", "True",
            "nerfstudio-data",
            "--downscale-factor", "1",
            "--auto-scale-poses", "False"
        ]
        
        # 3. æ‰§è¡Œ
        subprocess.run(cmd, check=True, env=self.env)

    def export(self):
        """å¯¼å‡º ply å¹¶è¿›è¡Œåå¤„ç†"""
        print(f"\nğŸ’¾ æ­£åœ¨å¯¼å‡º...")
        # æ‰¾åˆ°æœ€æ–°çš„ config.yml
        search_path = self.output_dir / self.cfg.project_name / "splatfacto"
        try:
            run_dirs = sorted(list(search_path.glob("*")))
            config_path = run_dirs[-1] / "config.yml"
        except IndexError:
            print("âŒ æœªæ‰¾åˆ°è®­ç»ƒç»“æœ config.yml")
            return None

        # å¯¼å‡ºå‘½ä»¤
        subprocess.run([
            "ns-export", "gaussian-splat",
            "--load-config", str(config_path),
            "--output-dir", str(self.cfg.project_dir)
        ], check=True, env=self.env)

        # åå¤„ç†ï¼šç‚¹äº‘åˆ‡å‰²
        raw_ply = self.cfg.project_dir / "point_cloud.ply"
        if not raw_ply.exists(): raw_ply = self.cfg.project_dir / "splat.ply"
        cleaned_ply = self.cfg.project_dir / "point_cloud_cleaned.ply"
        final_ply = raw_ply

        # åˆ¤æ–­æ˜¯å¦éœ€è¦åˆ‡å‰² (ç‰©ä½“æ¨¡å¼ or å¼ºåˆ¶åˆ‡å‰²)
        need_cull = (self.scene_type == "object" or self.cfg.force_spherical_culling)
        
        if need_cull and raw_ply.exists():
            # è°ƒç”¨ä¹‹å‰çš„å…¨å±€å‡½æ•°
            success = perform_percentile_culling(
                raw_ply, 
                self.cfg.transforms_file, 
                cleaned_ply,
                keep_percentile=self.cfg.keep_percentile
            )
            if success:
                final_ply = cleaned_ply

        # å¤åˆ¶ç»“æœåˆ° results ç›®å½•
        results_dir = Path(__file__).parent / "results"
        results_dir.mkdir(exist_ok=True)
        target_path = results_dir / f"{self.cfg.project_name}.ply"
        shutil.copy2(str(final_ply), str(target_path))
        
        return target_path

# ================= ä¸»æµç¨‹ =================
def run_pipeline(cfg: PipelineConfig):
    global_start_time = time.time()
    print(f"\nğŸš€ [BrainDance Engine] å¯åŠ¨ä»»åŠ¡: {cfg.project_name}")
    
    # 1. å®ä¾‹åŒ–æ‰€æœ‰æ¨¡å—
    img_processor = ImageProcessor(cfg)
    # colmap_runner = ColmapRunner(cfg)
    glomap_runner = GlomapRunner(cfg) 
    ai_segmentor = AISegmentor(cfg)
    nerf_engine = NerfstudioEngine(cfg)

    # ==========================================
    # Step 1: æ•°æ®å‡†å¤‡
    # ==========================================
    # åˆå§‹åŒ–ç›®å½•
    if cfg.project_dir.exists(): shutil.rmtree(cfg.project_dir, ignore_errors=True)
    cfg.project_dir.mkdir(parents=True, exist_ok=True)
    shutil.copy(str(cfg.video_path), str(cfg.project_dir / cfg.video_path.name))
    
    # æŠ½å¸§ (è¿™é‡Œé€»è¾‘ç®€å•ï¼Œç›´æ¥å†™è¿™é‡Œä¹Ÿè¡Œï¼Œæˆ–è€…å°è£…è¿› ImageProcessor)
    temp_dir = cfg.project_dir / "temp_extract"
    temp_dir.mkdir(parents=True, exist_ok=True)
    subprocess.run(["ffmpeg", "-y", "-i", str(cfg.project_dir / cfg.video_path.name), 
                    "-vf", "fps=10", "-q:v", "2", 
                    str(temp_dir / "frame_%05d.jpg")], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    
    # æ¸…æ´—
    img_processor.smart_filter_blurry_images(temp_dir, keep_ratio=0.85)
    
    # ç§»åŠ¨å›¾ç‰‡åˆ° raw_images
    raw_images_dir = cfg.project_dir / "raw_images"
    raw_images_dir.mkdir(parents=True, exist_ok=True)
    # ç®€å•çš„ç§»åŠ¨é€»è¾‘ä¿ç•™åœ¨è¿™é‡Œï¼Œæˆ–è€…ä¹Ÿå¯ä»¥ç§»å…¥ ImageProcessor
    all_imgs = sorted(list(temp_dir.glob("*")))
    limit = cfg.max_images
    if len(all_imgs) > limit:
        indices = np.linspace(0, len(all_imgs)-1, limit, dtype=int)
        all_imgs = [all_imgs[i] for i in sorted(list(set(indices)))]
    for img in all_imgs: shutil.copy2(str(img), str(raw_images_dir / img.name))
    shutil.rmtree(temp_dir)

    # ==========================================
    # Step 2: GOLMAP
    # ==========================================
    if not glomap_runner.run():
        print("âŒ Pipeline ä¸­æ–­ï¼šGLOMAP å¤±è´¥")
        return

    # ==========================================
    # Step 3: AI
    # ==========================================
    # è¿™é‡Œä¼šè‡ªåŠ¨åˆ¤æ–­æ˜¯å¦å¼€å¯ï¼Œå†…éƒ¨å·²å¤„ç†å¼‚å¸¸
    ai_segmentor.run()

    # ==========================================
    # Step 4 & 5: è®­ç»ƒä¸å¯¼å‡º
    # ==========================================
    try:
        nerf_engine.train()
        final_path = nerf_engine.export()
        print(f"\nğŸ‰ ä»»åŠ¡å®Œæˆï¼ç»“æœä½äº: {final_path}")
    except Exception as e:
        print(f"âŒ è®­ç»ƒ/å¯¼å‡ºé˜¶æ®µå¤±è´¥: {e}")

    print(f"â±ï¸ æ€»è€—æ—¶: {format_duration(time.time() - global_start_time)}")


# ç¨‹åºå…¥å£
if __name__ == "__main__":
    script_dir = Path(__file__).resolve().parent
    video_file = script_dir / "test.mp4" 
    if len(sys.argv) > 1: video_file = Path(sys.argv[1])
    
    if not video_file.exists():
        print(f"âŒ æ‰¾ä¸åˆ°è§†é¢‘: {video_file}")
        sys.exit(1)

    # å®ä¾‹åŒ–é…ç½®
    cfg = PipelineConfig(
        project_name="glomap_test_v1", # æ”¹ä¸ªåå­—
        video_path=video_file,
        max_images=100,
        enable_ai=True
    )
    
    # è¿è¡Œæµæ°´çº¿
    run_pipeline(cfg)