import subprocess
import sys
import shutil
import os
import time
import base64
import json
import logging
import numpy as np
import cv2
from pathlib import Path

# ================= ğŸ”§ ç”¨æˆ·é…ç½®åŒºåŸŸ =================
OPENAI_API_KEY = ""  # å¦‚æœæœ‰ Keyï¼Œå¡«å…¥åå¼€å¯ GPT-4o è¯­ä¹‰åˆ†æ
LINUX_WORK_ROOT = Path.home() / "braindance_workspace"
MODEL_YOLO = 'yolov8x-worldv2.pt'
MODEL_SAM = 'sam_l.pt'

# ================= ğŸ“¦ åº“å¯¼å…¥ =================
logging.getLogger('nerfstudio').setLevel(logging.ERROR)
logging.getLogger('ultralytics').setLevel(logging.ERROR)

try:
    from plyfile import PlyData, PlyElement
    has_plyfile = True
except ImportError:
    has_plyfile = False
    print("âš ï¸ æœªå®‰è£… plyfile åº“ï¼Œå°†è·³è¿‡ PLY åå¤„ç†æ¸…æ´—ã€‚å»ºè®®: pip install plyfile")

try:
    from openai import OpenAI
    has_openai = True if OPENAI_API_KEY and OPENAI_API_KEY.startswith("sk-") else False
except: has_openai = False

try:
    from ultralytics import YOLOWorld, SAM
    has_ultralytics = True
except: has_ultralytics = False

# ================= ğŸ§¹ æ–°å¢ï¼šåŸºäºç›¸æœºçš„ PLY æ¸…æ´—ç®—æ³• =================

def clean_ply_based_on_cameras(ply_path, json_path, output_path):
    """
    è¯»å– PLY å’Œç›¸æœºå‚æ•°ï¼Œåˆ é™¤ä½äºç›¸æœºè½¨è¿¹åŒ…å›´ç›’ä¹‹å¤–çš„å™ªç‚¹ã€‚
    """
    if not has_plyfile: return False
    
    print(f"\nğŸ§¹ [åå¤„ç†] æ­£åœ¨åŸºäºç›¸æœºè½¨è¿¹æ¸…æ´—ç‚¹äº‘å™ªå£°...")
    
    # 1. è¯»å–ç›¸æœºæ•°æ®
    try:
        with open(json_path, 'r') as f:
            frames = json.load(f)["frames"]
        
        positions = []
        for frame in frames:
            c2w = np.array(frame["transform_matrix"])
            positions.append(c2w[:3, 3])
        
        positions = np.array(positions)
        center = np.mean(positions, axis=0)
        
        # è®¡ç®—æ‰€æœ‰ç›¸æœºåˆ°ä¸­å¿ƒçš„è·ç¦»
        dists = np.linalg.norm(positions - center, axis=1)
        max_cam_radius = np.max(dists)
        avg_cam_radius = np.mean(dists)
        
        print(f"    -> ç›¸æœºç¾¤ç»Ÿè®¡: ä¸­å¿ƒ={center[:3]}, å¹³å‡åŠå¾„={avg_cam_radius:.2f}, æœ€å¤§åŠå¾„={max_cam_radius:.2f}")

    except Exception as e:
        print(f"âš ï¸ è¯»å–ç›¸æœºæ•°æ®å¤±è´¥: {e}")
        return False

    # 2. è¯»å– PLY æ–‡ä»¶
    try:
        plydata = PlyData.read(str(ply_path))
        vertex = plydata['vertex']
        
        # æå–ç‚¹çš„ä½ç½® (x, y, z)
        x = vertex['x']
        y = vertex['y']
        z = vertex['z']
        points = np.stack([x, y, z], axis=1)
        
        # æå–ä¸é€æ˜åº¦ (opacity) - 3DGS å­˜å‚¨çš„æ˜¯ logit(opacity)
        # é€šå¸¸ opacity < 0.05 ä¹Ÿæ˜¯ä¸å¯è§çš„å™ªç‚¹
        opacities = 1 / (1 + np.exp(-vertex['opacity'])) # sigmoid
        
        original_count = len(points)
        
        # --- æ ¸å¿ƒé€»è¾‘ï¼šå®šä¹‰ä¿ç•™åŒºåŸŸ ---
        # ç­–ç•¥ï¼šä¿ç•™ [ä¸­å¿ƒç‚¹] åˆ° [æœ€å¤§ç›¸æœºåŠå¾„ * 1.2] èŒƒå›´å†…çš„ç‚¹
        # 1.2 æ˜¯ä¸€ä¸ªå®‰å…¨ç³»æ•°ï¼Œé˜²æ­¢åˆ‡æ‰è¾¹ç¼˜
        
        # è®¡ç®—æ¯ä¸ªç‚¹åˆ°ä¸­å¿ƒçš„è·ç¦»
        point_dists = np.linalg.norm(points - center, axis=1)
        
        # åˆ¤å®šæ¡ä»¶ 1: è·ç¦»è¿‡æ»¤ (åªä¿ç•™ç›¸æœºåŒ…å›´åœˆå†…çš„ç‚¹ + ä¸€ç‚¹ä½™é‡)
        # æ³¨æ„ï¼šè¿™ä¸»è¦é€‚ç”¨äºâ€œç‰©ä½“æ¨¡å¼â€ã€‚å¦‚æœæ˜¯åœºæ™¯æ¨¡å¼ï¼Œè¿™ä¸ªé€»è¾‘ä¼šè¢«è·³è¿‡ã€‚
        is_object_mode = True # é»˜è®¤å‡è®¾ç‰©ä½“æ¨¡å¼ï¼Œå¦‚æœä½ æœ‰å‰é¢çš„ scene_type å˜é‡æ›´å¥½
        
        # è¿™é‡Œæˆ‘ä»¬ç”¨ç®€å•çš„å¯å‘å¼ï¼šå¦‚æœç›¸æœºèšåˆåº¦é«˜ï¼ˆç‰©ä½“ï¼‰ï¼Œå°±åˆ‡ï¼›å¦åˆ™æ”¾å®½
        # ä¸ºäº†å®‰å…¨ï¼Œæˆ‘ä»¬ç”¨ max_cam_radius * 1.5 ä½œä¸ºç•Œé™ã€‚
        # ä»»ä½•æ¯”ç›¸æœºè¿˜è¦è¿œ 1.5 å€çš„ç‚¹ï¼Œé€šå¸¸éƒ½æ˜¯èƒŒæ™¯æ¼‚æµ®ç‰©ã€‚
        radius_mask = point_dists < (max_cam_radius * 1.5)
        
        # åˆ¤å®šæ¡ä»¶ 2: é€æ˜åº¦è¿‡æ»¤ (åˆ é™¤æå…¶ç¨€è–„çš„ç‚¹)
        opacity_mask = opacities > 0.02 
        
        # åˆå¹¶æ©ç 
        final_mask = radius_mask & opacity_mask
        
        # åº”ç”¨è¿‡æ»¤
        filtered_vertex = vertex[final_mask]
        new_count = len(filtered_vertex)
        
        print(f"    -> åŸå§‹ç‚¹æ•°: {original_count}")
        print(f"    -> å‰©ä½™ç‚¹æ•°: {new_count} (åˆ é™¤äº† {original_count - new_count} ä¸ªå™ªç‚¹)")
        
        # 3. ä¿å­˜æ–°çš„ PLY
        ply_element = PlyElement.describe(filtered_vertex, 'vertex')
        PlyData([ply_element]).write(str(output_path))
        print(f"âœ… æ¸…æ´—å®Œæˆï¼å·²ä¿å­˜è‡³: {output_path}")
        return True

    except Exception as e:
        print(f"âŒ PLY æ¸…æ´—å¤±è´¥: {e}")
        return False

# ================= ğŸ§  AI & å‡ ä½•åˆ†ææ¨¡å— (ä¿ç•™) =================

class SmartMasker:
    def __init__(self):
        if not has_ultralytics: return
        print(f"\nğŸ¦¾ [AI å¼•æ“] åŠ è½½ YOLO+SAM (RTX 5070)...")
        self.detector = YOLOWorld(MODEL_YOLO)
        self.segmentor = SAM(MODEL_SAM)

    def generate_mask(self, image_path, prompt):
        try:
            self.detector.set_classes([prompt])
            det = self.detector.predict(image_path, conf=0.15, verbose=False)
            if len(det[0].boxes) == 0: return None
            sam = self.segmentor(image_path, bboxes=det[0].boxes.xyxy, verbose=False)
            if len(sam[0].masks) == 0: return None
            
            final = np.zeros(sam[0].orig_shape[:2], dtype=np.uint8)
            for m in sam[0].masks.data.cpu().numpy():
                m_u8 = (m * 255).astype(np.uint8)
                if m_u8.shape != final.shape: m_u8 = cv2.resize(m_u8, (final.shape[1], final.shape[0]))
                final = cv2.bitwise_or(final, m_u8)
            return final
        except: return None

def analyze_scene_geometry(json_path):
    try:
        with open(json_path, 'r') as f: frames = json.load(f)["frames"]
        if not frames: return "scene", False, "none"
        pos = [np.array(f["transform_matrix"])[:3, 3] for f in frames]
        fwds = [np.array(f["transform_matrix"])[:3, :3] @ np.array([0,0,-1]) for f in frames]
        center = np.mean(pos, axis=0)
        vecs = center - np.array(pos)
        vecs /= (np.linalg.norm(vecs, axis=1, keepdims=True) + 1e-6)
        ratio = np.sum(np.sum(np.array(fwds) * vecs, axis=1) > 0) / len(frames)
        
        # è¿”å›ï¼šç±»å‹, æ˜¯å¦éœ€æŠ å›¾, ä¸»ä½“å, æ˜¯å¦å¯ç”¨æ¿€è¿›åå¤„ç†
        if ratio > 0.6: return "object", True, "object", True
        else: return "scene", False, "none", False
    except: return "scene", False, "none", False

def analyze_with_gpt4o(images_dir):
    if not has_openai: return None
    print(f"\nğŸ§  [GPT-4o] åˆ†æä¸­...")
    client = OpenAI(api_key=OPENAI_API_KEY)
    imgs = sorted(list(images_dir.glob("*.jpg")))
    if not imgs: return None
    samples = imgs[::max(1, len(imgs)//6)][:6]
    
    content = [{"type": "text", "text": "Analyze for 3D Gaussian Splatting."}]
    for p in samples:
        with open(p, "rb") as f: b64 = base64.b64encode(f.read()).decode('utf-8')
        content.append({"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{b64}","detail":"low"}})
    
    content.append({"type": "text", "text": """Return JSON: {"type": "object"|"scene", "subject": string, "masking_needed": bool}"""})
    try:
        res = client.chat.completions.create(model="gpt-4o", messages=[{"role":"user","content":content}], response_format={"type":"json_object"})
        ret = json.loads(res.choices[0].message.content)
        # GPT ä¸ç›´æ¥å†³å®šåå¤„ç†åŠ›åº¦ï¼Œæˆ‘ä»¬åœ¨åé¢é€»è¾‘åˆ¤æ–­
        return ret
    except: return None

# ================= ğŸš€ ä¸»æµç¨‹ =================

def run_pipeline(video_path, project_name):
    print(f"\nğŸš€ [BrainDance Engine 6.0] å¯åŠ¨ä»»åŠ¡: {project_name}")
    
    video_src = Path(video_path).resolve()
    work_dir = LINUX_WORK_ROOT / project_name
    data_dir = work_dir / "data"
    output_dir = work_dir / "outputs"
    transforms_file = data_dir / "transforms.json"
    env = os.environ.copy()
    env["QT_QPA_PLATFORM"] = "offscreen" 

    # [Step 1] æ•°æ®å¤„ç†
    if transforms_file.exists():
        print(f"\nâ© [æ–­ç‚¹ç»­ä¼ ] å·²æœ‰ COLMAP æ•°æ®")
    else:
        if work_dir.exists(): shutil.rmtree(work_dir)
        work_dir.mkdir(parents=True)
        data_dir.mkdir(parents=True)
        shutil.copy(str(video_src), str(work_dir / video_src.name))

        print(f"\nğŸ¥ [1/3] COLMAP è§£ç®—")
        img_dir = data_dir / "images"
        img_dir.mkdir(parents=True, exist_ok=True)
        subprocess.run(["ffmpeg", "-y", "-i", str(work_dir / video_src.name), "-vf", "scale=1920:-1,fps=4", "-q:v", "2", str(img_dir / "frame_%05d.jpg")], check=True)
        
        res = subprocess.run(["ns-process-data", "images", "--data", str(img_dir), "--output-dir", str(data_dir), "--verbose"], check=True, env=env, capture_output=True, text=True)
        if "COLMAP only found poses" in res.stdout: raise RuntimeError("COLMAP å¤±è´¥")

    # [Step 2] æ™ºèƒ½è®­ç»ƒ
    search_path = output_dir / project_name / "splatfacto"
    run_dirs = sorted(list(search_path.glob("*"))) if search_path.exists() else []

    # æ ‡è®°å˜é‡ï¼šæ˜¯å¦å¯ç”¨æ¿€è¿›çš„åå¤„ç†æ¸…æ´—
    enable_aggressive_cleaning = False 

    if run_dirs:
        print(f"\nâ© [è®­ç»ƒè·³è¿‡] å·²å®Œæˆ")
        # å¦‚æœè·³è¿‡è®­ç»ƒï¼Œæˆ‘ä»¬éœ€è¦é‡æ–°åˆ¤æ–­ä¸€ä¸‹ç±»å‹æ¥å†³å®šæ˜¯å¦æ¸…æ´—
        type_res, _, _, enable_aggressive_cleaning = analyze_scene_geometry(transforms_file)
    else:
        img_dir = data_dir / "images"
        gpt_res = analyze_with_gpt4o(img_dir)
        
        if gpt_res:
            stype, need_mask, subj = gpt_res["type"], gpt_res["masking_needed"], gpt_res["subject"]
            enable_aggressive_cleaning = (stype == "object") # ç‰©ä½“æ¨¡å¼å¯ç”¨æ¸…æ´—
        else:
            stype, need_mask, subj, enable_aggressive_cleaning = analyze_scene_geometry(transforms_file)

        collider_params = ["near_plane", "0.05", "far_plane", "100.0"] if stype == "scene" else ["near_plane", "2.0", "far_plane", "6.0"]
        
        if stype == "object" and need_mask and has_ultralytics:
            print(f"ğŸ’¡ [AI] ç‰©ä½“æ¨¡å¼ ('{subj}') -> å¯åŠ¨ SAM æŠ å›¾...")
            masks_dir = data_dir / "masks"
            masks_dir.mkdir(exist_ok=True)
            if not any(masks_dir.iterdir()):
                masker = SmartMasker()
                imgs = sorted(list(img_dir.glob("*.jpg")))
                for i, p in enumerate(imgs):
                    m = masker.generate_mask(str(p), subj)
                    if m is not None: cv2.imwrite(str(masks_dir / (p.stem+".png")), m)
                    print(f"    ç”Ÿæˆè’™ç‰ˆ {i+1}/{len(imgs)}", end="\r")
                print("")
                with open(transforms_file,'r') as f: d=json.load(f)
                for fr in d["frames"]:
                    mp = masks_dir/(Path(fr["file_path"]).stem+".png")
                    if mp.exists(): fr["mask_path"] = f"masks/{mp.name}"
                with open(transforms_file,'w') as f: json.dump(d,f,indent=4)

        print(f"\nğŸ§  [2/3] å¼€å§‹è®­ç»ƒ...")
        subprocess.run([
            "ns-train", "splatfacto", "--data", str(data_dir), "--output-dir", str(output_dir), 
            "--experiment-name", project_name, "--pipeline.model.random-init", "False", 
            "--pipeline.model.cull-alpha-thresh", "0.005", 
            "--pipeline.model.enable-collider", "True", "--pipeline.model.collider-params", *collider_params,
            "--max-num-iterations", "15000", "--vis", "viewer+tensorboard", 
            "--viewer.quit-on-train-completion", "True", "colmap"
        ], check=True, env=env)

    # [Step 3] å¯¼å‡º
    print(f"\nğŸ’¾ [3/3] å¯¼å‡ºç»“æœ")
    if not run_dirs: run_dirs = sorted(list(search_path.glob("*")))
    if not run_dirs: return None
    subprocess.run(["ns-export", "gaussian-splat", "--load-config", str(run_dirs[-1]/"config.yml"), "--output-dir", str(work_dir)], check=True, env=env)
    time.sleep(5)

    # [Step 3.5] åŸºäºç›¸æœºè½¨è¿¹çš„åå¤„ç†æ¸…æ´— (New!)
    raw_ply = work_dir / "point_cloud.ply"
    cleaned_ply = work_dir / "point_cloud_cleaned.ply"
    
    final_ply_to_copy = raw_ply # é»˜è®¤ç”¨åŸç‰ˆ
    
    # åªæœ‰åœ¨ç‰©ä½“æ¨¡å¼(ä¸” has_plyfile)ä¸‹æ‰æ‰§è¡Œæ¸…æ´—ï¼Œé˜²æ­¢æŠŠæˆ¿é—´åˆ‡ç¢
    if enable_aggressive_cleaning and has_plyfile and raw_ply.exists():
        if clean_ply_based_on_cameras(raw_ply, transforms_file, cleaned_ply):
            final_ply_to_copy = cleaned_ply # æ¸…æ´—æˆåŠŸï¼Œæ›¿æ¢ä¸ºæ¸…æ´—ç‰ˆ

    # [Step 4] å›ä¼ 
    print(f"\nğŸ“¦ [IO åŒæ­¥] å›ä¼ ç»“æœ...")
    target_dir = Path(__file__).parent / "results"
    target_dir.mkdir(exist_ok=True)
    
    # WebGL Pose
    try:
        with open(transforms_file,'r') as f: d=json.load(f)
        frames = [{"file_path": fr["file_path"], "pose_matrix_c2w": fr["transform_matrix"]} for fr in d["frames"]]
        with open(target_dir/"webgl_poses.json",'w') as f: json.dump({"camera_model": d.get("camera_model","OPENCV"), "frames": frames}, f, indent=4)
    except: pass

    final_dst = target_dir / f"{project_name}.ply"
    if final_ply_to_copy.exists():
        shutil.copy(str(final_ply_to_copy), str(final_dst))
        shutil.rmtree(work_dir)
        print(f"âœ… å®Œæˆï¼æ¨¡å‹å·²ä¿å­˜è‡³: {final_dst}")
        return str(final_dst)
    
    return None

if __name__ == "__main__":
    script_dir = Path(__file__).resolve().parent
    video_file = script_dir / "test.mp4" 
    if len(sys.argv) > 1: video_file = Path(sys.argv[1])
    if video_file.exists(): run_pipeline(video_file, "scene_auto_sync")
    else: print(f"âŒ æ‰¾ä¸åˆ°è§†é¢‘")