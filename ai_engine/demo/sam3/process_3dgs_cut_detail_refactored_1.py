# ==============================================================================
# å¯¼å…¥æ ‡å‡†åº“å’Œç¬¬ä¸‰æ–¹åº“
# ==============================================================================
import subprocess
import sys
import shutil
import os
import time
import datetime
from pathlib import Path
import json
import numpy as np
import torch
import logging
import cv2
import re

# ================= ğŸ§  AI ä¾èµ–å¼•å…¥ =================
try:
    import dashscope
    from dashscope import MultiModalConversation
    from ultralytics import SAM, YOLOWorld
    HAS_AI = True
except ImportError:
    HAS_AI = False
    print("âš ï¸ [ç¯å¢ƒè­¦å‘Š] æœªæ£€æµ‹åˆ° dashscope æˆ– ultralytics åº“ã€‚")

# ================= ğŸ”§ åŸºç¡€é…ç½® =================
logging.getLogger('nerfstudio').setLevel(logging.ERROR) 

from dataclasses import dataclass, field

@dataclass
class PipelineConfig:
    project_name: str
    video_path: Path
    
    work_root: Path = Path.home() / "braindance_workspace"
    max_images: int = 180
    force_spherical_culling: bool = True 
    scene_radius_scale: float = 1.8
    keep_percentile: float = 0.9
    enable_ai: bool = True
    
    project_dir: Path = field(init=False)
    data_dir: Path = field(init=False)
    images_dir: Path = field(init=False)
    masks_dir: Path = field(init=False)
    transforms_file: Path = field(init=False)
    vocab_tree_path: Path = field(init=False)

    # SAM3 æ¨¡å‹è·¯å¾„é…ç½®
    model_root: Path = Path("/home/ltx/workspace/ai/sam3") 

    def __post_init__(self):
        self.project_dir = self.work_root / self.project_name
        self.data_dir = self.project_dir / "data"
        self.images_dir = self.data_dir / "images"
        self.masks_dir = self.data_dir / "masks"
        self.transforms_file = self.data_dir / "transforms.json"
        self.vocab_tree_path = self.work_root / "vocab_tree_flickr100k_words.bin"

        self.model_root.mkdir(parents=True, exist_ok=True)
        os.environ["SETUPTOOLS_USE_DISTUTILS"] = "stdlib"

# æ£€æŸ¥ plyfile
try:
    from plyfile import PlyData, PlyElement
    HAS_PLYFILE = True
except ImportError:
    HAS_PLYFILE = False

# ================= ğŸ§  AI æ ¸å¿ƒé€»è¾‘å‡½æ•° =================

def get_central_object_prompt(images_dir: Path, sample_count=7):
    """ä½¿ç”¨ Qwen-VL-Plus æå– Prompt"""
    api_key = os.environ.get("DASHSCOPE_API_KEY")
    if not api_key:
        print("âŒ æœªè®¾ç½® DASHSCOPE_API_KEY")
        return None

    print(f"\nğŸ§  [AI åˆ†æ] æ­£åœ¨è°ƒç”¨ Qwen-VL-Plus åˆ†æåœºæ™¯...")
    image_files = sorted(list(images_dir.glob("*.jpg")) + list(images_dir.glob("*.png")))
    if not image_files: return None
    
    indices = np.linspace(0, len(image_files) - 1, sample_count, dtype=int)
    sampled_imgs = [image_files[i] for i in indices]
    
    content = [{"image": str(img_path)} for img_path in sampled_imgs]
    content.append({
        "text": (
            "è¿™äº›æ˜¯ä¸€ä¸ªè§†é¢‘çš„æŠ½å¸§å›¾ç‰‡ã€‚è¯·åˆ†æç”»é¢ä¸­å¿ƒå§‹ç»ˆå­˜åœ¨çš„ã€æœ€ä¸»è¦çš„ä¸€ä¸ªç‰©ä½“æ˜¯ä»€ä¹ˆã€‚"
            "æˆ‘æ­£åœ¨ä½¿ç”¨ SAM 3 (Segment Anything Model 3) è¿›è¡ŒåŸºäºæ–‡æœ¬çš„è§†é¢‘è·Ÿè¸ªã€‚"
            "è¯·è¾“å‡ºä¸€ä¸ªã€æŒ‡ä»£æ€§æ˜ç¡®ã€‘çš„è‹±æ–‡çŸ­è¯­ (Referring Expression)ã€‚"
            "âš ï¸ å…³é”®ç­–ç•¥ï¼š"
            "1. å¿…é¡»åŒ…å«è§†è§‰ç‰¹å¾ï¼ˆé¢œè‰²ã€æè´¨ï¼‰ã€‚"
            "2. æè¿°ç‰©ä½“æœ¬èº«ï¼Œä¸è¦æè¿°åŠŸèƒ½ã€‚"
            "3. ä¿æŒç®€çŸ­ï¼Œç›´æ¥è¾“å‡ºè‹±æ–‡çŸ­è¯­ï¼Œä¸è¦æ ‡ç‚¹ç¬¦å·ã€‚"
        )
    })
    
    messages = [{"role": "user", "content": content}]

    try:
        response = dashscope.MultiModalConversation.call(model='qwen-vl-plus', messages=messages)
        if response.status_code == 200:
            prompt_text = response.output.choices[0].message.content[0]["text"].strip()
            prompt_text = prompt_text.replace(".", "").replace('"', "").replace("'", "")
            print(f"    ğŸ¤– Qwen è®¤ä¸ºä¸­å¿ƒç‰©ä½“æ˜¯: [ \033[92m{prompt_text}\033[0m ]")
            return prompt_text
        else:
            print(f"âŒ Qwen è°ƒç”¨å¤±è´¥: {response.code}")
            return None
    except Exception as e:
        print(f"âŒ API è¿æ¥å¼‚å¸¸: {e}")
        return None

def clean_and_verify_mask(mask, img_name=""):
    """
    [ç»å…¸ç‰ˆ] è…èš€ (Erosion) æ¨¡å¼
    """
    h, w = mask.shape
    
    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(mask, connectivity=8)
    if num_labels < 2: return False, None, "Empty Mask"

    max_area = 0
    max_label = -1
    for i in range(1, num_labels):
        if stats[i, cv2.CC_STAT_AREA] > max_area:
            max_area = stats[i, cv2.CC_STAT_AREA]
            max_label = i
            
    if max_area < (h * w * 0.005): return False, None, "Too Small/Noise"
    if max_area > (h * w * 0.90): return False, None, f"Too Large ({max_area/(h*w):.0%})"

    cleaned_mask = (labels == max_label).astype(np.uint8) * 255

    contours, _ = cv2.findContours(cleaned_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if not contours: return False, None, "No Contour"
    main_cnt = max(contours, key=cv2.contourArea)

    hull = cv2.convexHull(main_cnt)
    hull_area = cv2.contourArea(hull)
    if hull_area == 0: return False, None, "Hull Area 0"
    
    # ğŸ”¥ æ ¸å¿ƒï¼šä½¿ç”¨è…èš€ (Erosion) è€Œä¸æ˜¯è†¨èƒ€
    kernel_size = 3 
    kernel = np.ones((kernel_size, kernel_size), np.uint8)
    cleaned_mask = cv2.erode(cleaned_mask, kernel, iterations=1)

    return True, cleaned_mask, "OK"

def run_ai_segmentation_pipeline(data_dir: Path):
    """
    SAM 3 + Premultiplied Alpha
    """
    if not HAS_AI: return False
    
    logging.getLogger("ultralytics").setLevel(logging.ERROR)
    
    images_dir = data_dir / "images"
    masks_dir = data_dir / "masks"
    debug_dir = data_dir / "debug_combo"
    debug_dir.mkdir(parents=True, exist_ok=True)
    masks_dir.mkdir(parents=True, exist_ok=True)

    cfg.transforms_file = data_dir / "transforms.json" 
    if not cfg.transforms_file.exists(): return False

    print(f"\nâœ‚ï¸ [æ™ºèƒ½åˆ†å‰²] åˆå§‹åŒ– (YOLO-World + SAM 3 Multi-Point)...")
    try:
        text_prompt = get_central_object_prompt(images_dir)
        if " on " in text_prompt: text_prompt = text_prompt.split(" on ")[0]
    except: text_prompt = "object"
    if not text_prompt: text_prompt = "object"
    print(f"    ğŸ¯ æ ¸å¿ƒ Prompt: '\033[92m{text_prompt}\033[0m'")

    yolo_path = cfg.model_root / "yolov8s-worldv2.pt"
    sam_path = cfg.model_root / "sam3.pt"
    
    try:
        det_model = YOLOWorld(str(yolo_path) if yolo_path.exists() else "yolov8s-worldv2.pt")
        det_model.set_classes([text_prompt])
        sam_model = SAM(str(sam_path))
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return False

    with open(cfg.transforms_file, 'r') as f: meta = json.load(f)
    frames_map = {Path(f["file_path"]).name: f for f in meta["frames"]}
    valid_frames_list = []
    
    image_files = sorted(list(images_dir.glob("*.jpg")) + list(images_dir.glob("*.png")))
    total_imgs = len(image_files)
    
    print(f"    -> å¼€å§‹å¤„ç† {total_imgs} å¸§...")
    start_time = time.time()

    for i, img_path in enumerate(image_files):
        elapsed = time.time() - start_time
        fps = (i + 1) / (elapsed + 1e-6)
        process_success = False 
        
        try:
            original_img = cv2.imread(str(img_path))
            if original_img is None: raise ValueError("æ— æ³•è¯»å–å›¾ç‰‡")
            h_real, w_real = original_img.shape[:2]

            # --- Step 1: YOLO ---
            det_results = det_model.predict(img_path, conf=0.05, verbose=False) 
            bboxes = det_results[0].boxes.xyxy.cpu()
            
            final_box = None
            is_fallback = False 
            
            if len(bboxes) > 0:
                center_x, center_y = w_real / 2, h_real / 2
                min_dist = float('inf')
                for box in bboxes:
                    bx = (box[0] + box[2]) / 2
                    by = (box[1] + box[3]) / 2
                    dist = (bx - center_x)**2 + (by - center_y)**2
                    if dist < min_dist:
                        min_dist = dist
                        final_box = box.unsqueeze(0)
            
            # --- Step 2: SAM 3 ---
            final_mask = None
            if final_box is not None:
                sam_results = sam_model(img_path, bboxes=final_box, verbose=False)
            else:
                is_fallback = True
                h_img, w_img = det_results[0].orig_shape[:2]
                cx, cy = w_img / 2, h_img / 2
                margin = 5  
                fallback_box = torch.tensor([[cx-margin, cy-margin, cx+margin, cy+margin]], device=det_model.device)
                sam_results = sam_model(img_path, bboxes=fallback_box, verbose=False)

            if sam_results[0].masks is not None:
                masks_data = sam_results[0].masks.data.cpu().numpy()
                if masks_data.shape[0] > 0:
                    areas = np.sum(masks_data, axis=(1, 2))
                    largest_idx = np.argmax(areas)
                    final_mask = masks_data[largest_idx].astype(np.uint8) * 255
            
            if final_mask is None:
                final_mask = np.zeros((h_real, w_real), dtype=np.uint8)

            # --- Step 3: æ¸…æ´— ---
            status_icon = "ğŸŸ¢" if not is_fallback else "ğŸ”µ"
            print(f"       [{i+1}/{total_imgs}] {img_path.name} | {status_icon} | âš¡ {fps:.1f} fps          ", end="\r")

            is_good, cleaned_mask, reason = clean_and_verify_mask(final_mask, img_path.name)

            if is_good:
                if cleaned_mask.shape[:2] != original_img.shape[:2]:
                    cleaned_mask = cv2.resize(cleaned_mask, (w_real, h_real), interpolation=cv2.INTER_NEAREST)
                
                # ğŸ”¥ Premultiplied Alpha + Feathering ğŸ”¥
                mask_blurred = cv2.GaussianBlur(cleaned_mask, (5, 5), 0)
                alpha = mask_blurred.astype(np.float32) / 255.0
                img_float = original_img.astype(np.float32)
                
                b, g, r = cv2.split(img_float)
                b = b * alpha
                g = g * alpha
                r = r * alpha
                
                img_bgra = cv2.merge([
                    b.astype(np.uint8),
                    g.astype(np.uint8),
                    r.astype(np.uint8),
                    mask_blurred
                ])
                
                new_img_path = img_path.with_suffix('.png')
                cv2.imwrite(str(new_img_path), img_bgra)
                cv2.imwrite(str(masks_dir / f"{img_path.stem}.png"), cleaned_mask)
                
                if img_path.name in frames_map:
                    frame_data = frames_map[img_path.name]
                    frame_data["file_path"] = f"images/{new_img_path.name}"
                    valid_frames_list.append(frame_data)
                process_success = True

        except Exception as e:
            print(f"\nâŒ Frame {i} Error: {e}")
            process_success = False 

        finally:
            if img_path.exists() and img_path.suffix.lower() == '.jpg':
                if process_success:
                    try: img_path.unlink() 
                    except: pass
                else:
                    try: img_path.unlink()
                    except: pass

    print(f"\n\nğŸ“Š å®Œæˆã€‚å‰©ä½™å¯ç”¨: {len(valid_frames_list)}")
    if len(valid_frames_list) == 0: return False

    meta["frames"] = valid_frames_list
    with open(cfg.transforms_file, 'w') as f: json.dump(meta, f, indent=4)
    return True

# ================= è¾…åŠ©å·¥å…· =================

def format_duration(seconds):
    return str(datetime.timedelta(seconds=int(seconds)))

class ImageProcessor:
    def __init__(self, config: PipelineConfig):
        self.cfg = config

    def smart_filter_blurry_images(self, image_folder, keep_ratio=0.85):
        print(f"\nğŸ§  [æ™ºèƒ½æ¸…æ´—] æ­£åœ¨åˆ†æå›¾ç‰‡è´¨é‡...")
        image_dir = Path(image_folder)
        images = sorted([p for p in image_dir.iterdir() if p.suffix.lower() in ['.jpg', '.jpeg', '.png']])
        if not images: return
        
        trash_dir = image_dir.parent / "trash_smart"
        trash_dir.mkdir(exist_ok=True)
        
        img_scores = []
        for i, img_path in enumerate(images):
            img = cv2.imread(str(img_path))
            if img is None: continue
            
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            h, w = gray.shape
            grid_h, grid_w = h // 3, w // 3
            max_grid_score = 0
            for r in range(3):
                for c in range(3):
                    roi = gray[r*grid_h:(r+1)*grid_h, c*grid_w:(c+1)*grid_w]
                    score = cv2.Laplacian(roi, cv2.CV_64F).var()
                    if score > max_grid_score: max_grid_score = score
            img_scores.append((img_path, max_grid_score))
            if i % 50 == 0: print(f"  -> åˆ†æä¸­... {i}/{len(images)}", end="\r")
        
        scores = [s[1] for s in img_scores]
        if not scores: return
        quality_threshold = np.percentile(scores, (1 - keep_ratio) * 100)
        
        good_images = []
        for img_path, score in img_scores:
            if score < quality_threshold:
                shutil.move(str(img_path), str(trash_dir / img_path.name))
            else:
                good_images.append(img_path)
        
        max_imgs = self.cfg.max_images  
        if len(good_images) > max_imgs:
            print(f"    âš ï¸ å›¾ç‰‡è¿‡å¤š ({len(good_images)} å¼ ), æ­£åœ¨é™é‡‡æ ·è‡³ {max_imgs} å¼ ...")
            indices_to_keep = set(np.linspace(0, len(good_images) - 1, max_imgs, dtype=int))
            for idx, img_path in enumerate(good_images):
                if idx not in indices_to_keep:
                    shutil.move(str(img_path), str(trash_dir / img_path.name))
                    
        print(f"âœ¨ æ¸…æ´—ç»“æŸï¼Œå‰©ä½™ {len(list(image_dir.glob('*')))} å¼ ã€‚")

def analyze_and_calculate_adaptive_collider(json_path, force_cull=False, radius_scale=1.8):
    """
    åœºæ™¯ç†è§£ç®—æ³•
    """
    print(f"\nğŸ¤– [AI åˆ†æ] è§£æç›¸æœºè½¨è¿¹...")
    try:
        with open(json_path, 'r') as f: data = json.load(f)
        frames = data["frames"]
        if not frames: return [], "unknown"

        has_mask = "mask_path" in frames[0]
        positions = [np.array(f["transform_matrix"])[:3, 3] for f in frames]
        forward_vectors = [np.array(f["transform_matrix"])[:3, :3] @ np.array([0, 0, -1]) for f in frames]
        center = np.mean(positions, axis=0)
        vec_to_center = center - positions
        vec_to_center /= (np.linalg.norm(vec_to_center, axis=1, keepdims=True) + 1e-6)
        ratio = np.sum(np.sum(forward_vectors * vec_to_center, axis=1) > 0) / len(frames)
        
        is_object_mode = ratio > 0.6 or force_cull or has_mask

        if is_object_mode:
            dists = [np.linalg.norm(p) for p in positions]
            avg_dist = np.mean(dists)
            scene_radius = 1.0 * radius_scale
            # ä¿æŠ¤ç¬”å°–ï¼šç¡®ä¿è¿‘å¹³é¢è¶³å¤Ÿå°
            calc_near = max(0.01, min(dists) - scene_radius) 
            calc_far = avg_dist + scene_radius
            
            print(f"    -> ç‰©ä½“æ¨¡å¼: Near={calc_near:.2f}, Far={calc_far:.2f}")
            return ["--pipeline.model.enable-collider", "True", 
                    "--pipeline.model.collider-params", "near_plane", str(round(calc_near, 2)), 
                    "far_plane", str(round(calc_far, 2))], "object"
        else:
            return ["--pipeline.model.enable-collider", "True", 
                    "--pipeline.model.collider-params", "near_plane", "0.05", "far_plane", "100.0"], "scene"
    except Exception as e:
        print(f"    âš ï¸ åˆ†æå¤±è´¥: {e}")
        return [], "unknown"

def perform_percentile_culling(ply_path, json_path, output_path, keep_percentile=0.9):
    if not HAS_PLYFILE: return False
    print(f"\nâœ‚ï¸ [åå¤„ç†] æ­£åœ¨æ‰§è¡Œã€åˆ†ä½æ•°æš´åŠ›åˆ‡å‰²ã€‘...")
    try:
        with open(json_path, 'r') as f: frames = json.load(f)["frames"]
        cam_pos = np.array([np.array(f["transform_matrix"])[:3, 3] for f in frames])
        center = np.mean(cam_pos, axis=0)
        
        plydata = PlyData.read(str(ply_path))
        vertex = plydata['vertex']
        points = np.stack([vertex['x'], vertex['y'], vertex['z']], axis=1)
        
        dists_pts = np.linalg.norm(points - center, axis=1)
        threshold_radius = np.percentile(dists_pts, keep_percentile * 100)
        
        opacities = 1 / (1 + np.exp(-vertex['opacity']))
        mask = (dists_pts < threshold_radius) & (opacities > 0.05)
        filtered_vertex = vertex[mask]
        
        PlyData([PlyElement.describe(filtered_vertex, 'vertex')]).write(str(output_path))
        return True
    except Exception as e:
        print(f"âŒ åˆ‡å‰²å¤±è´¥: {e}")
        return False

# ==============================================================================
# ç±»: ColmapRunner (å›å½’ç»å…¸ï¼šä½¿ç”¨æ ‡å‡† COLMAP è€Œä¸æ˜¯ GLOMAP)
# ------------------------------------------------------------------------------
# åŸå› ï¼šCOLMAP çš„ Incremental Mapper å¯¹ç»†å¾®ç‰©ä½“ï¼ˆå¦‚ç¬”å°–ï¼‰çš„é‡å»ºèƒ½åŠ›è¿œå¼ºäº GLOMAP
# ==============================================================================
class ColmapRunner:
    def __init__(self, cfg: PipelineConfig):
        self.cfg = cfg
        self.colmap_exe = shutil.which("colmap") or "/usr/local/bin/colmap"
        if not os.path.exists(self.colmap_exe):
            raise FileNotFoundError("âŒ ç¼ºå°‘ colmap å¯æ‰§è¡Œæ–‡ä»¶")
        
        print(f"    -> ğŸ¯ é”å®šå¼•æ“: COLMAP={self.colmap_exe} (å›å½’é«˜ç²¾åº¦æ¨¡å¼)")
        self.env = os.environ.copy()
        self.env["SETUPTOOLS_USE_DISTUTILS"] = "stdlib"

    def run(self):
        print(f"\nğŸ“ [2/4] COLMAP ä½å§¿è§£ç®— (High Precision)")
        raw_images_dir = self.cfg.project_dir / "raw_images"
        dest_images_dir = self.cfg.images_dir
        dest_images_dir.mkdir(parents=True, exist_ok=True)
        for img in raw_images_dir.glob("*"):
            if not (dest_images_dir / img.name).exists():
                shutil.copy2(str(img), str(dest_images_dir / img.name))

        colmap_output_dir = self.cfg.data_dir / "colmap"
        colmap_output_dir.mkdir(parents=True, exist_ok=True)
        database_path = colmap_output_dir / "database.db"
        sparse_dir = colmap_output_dir / "sparse"

        try:
            if database_path.exists(): database_path.unlink()
            if sparse_dir.exists(): shutil.rmtree(sparse_dir)
            sparse_dir.mkdir(parents=True, exist_ok=True)
            if self.cfg.transforms_file.exists(): self.cfg.transforms_file.unlink()

            # Step 1: ç‰¹å¾æå–
            self._run_cmd([self.colmap_exe, "feature_extractor", "--database_path", str(database_path), "--image_path", str(raw_images_dir), "--ImageReader.camera_model", "OPENCV", "--ImageReader.single_camera", "1"], "Step 1: ç‰¹å¾æå–")
            
            # Step 2: é¡ºåºåŒ¹é…
            self._run_cmd([self.colmap_exe, "sequential_matcher", "--database_path", str(database_path), "--SequentialMatching.overlap", "25"], "Step 2: é¡ºåºåŒ¹é…")
            
            # Step 3: å¢é‡æ˜ å°„ (Incremental Mapper) - è¿™å°±æ˜¯æ‰¾å›ç¬”å°–çš„å…³é”®ï¼
            sparse_0 = sparse_dir / "0"
            sparse_0.mkdir(parents=True, exist_ok=True)
            self._run_cmd([self.colmap_exe, "mapper", "--database_path", str(database_path), "--image_path", str(raw_images_dir), "--output_path", str(sparse_dir)], "Step 3: å¢é‡æ˜ å°„ (COLMAP)")

            # Step 4: è½¬ json
            # COLMAP è¾“å‡ºé€šå¸¸åœ¨ sparse/0 ä¸­ï¼Œnerfstudio èƒ½è‡ªåŠ¨è¯†åˆ«
            self._run_cmd(["ns-process-data", "images", "--data", str(dest_images_dir), "--output-dir", str(self.cfg.data_dir), "--skip-colmap", "--skip-image-processing", "--num-downscales", "0"], "ç”Ÿæˆ transforms.json")

            return self._check_quality(raw_images_dir)
        except Exception as e:
            print(f"âŒ COLMAP æµç¨‹å¤±è´¥: {e}")
            return False

    def _run_cmd(self, cmd, desc):
        print(f"ğŸš€ {desc}...")
        cmd_env = self.env.copy()
        if cmd[0].startswith("/usr") or cmd[0].startswith("/bin"):
            if "LD_LIBRARY_PATH" in cmd_env: del cmd_env["LD_LIBRARY_PATH"]
        subprocess.run(cmd, check=True, env=cmd_env, stdout=subprocess.DEVNULL) # ç®€åŒ–è¾“å‡º

    def _check_quality(self, raw_images_dir):
        if not self.cfg.transforms_file.exists(): return False
        with open(self.cfg.transforms_file, 'r') as f: meta = json.load(f)
        ratio = len(meta["frames"]) / len(list(raw_images_dir.glob("*")))
        print(f"    ğŸ“Š åŒ¹é…ç‡: {ratio:.2%}")
        return ratio > 0.2

class AISegmentor:
    def __init__(self, cfg: PipelineConfig):
        self.cfg = cfg
    def run(self):
        return run_ai_segmentation_pipeline(self.cfg.data_dir)

class NerfstudioEngine:
    def __init__(self, cfg: PipelineConfig):
        self.cfg = cfg
        self.output_dir = cfg.project_dir / "outputs"
        self.env = os.environ.copy()
        self.env["QT_QPA_PLATFORM"] = "offscreen"
        self.env["SETUPTOOLS_USE_DISTUTILS"] = "stdlib"
        self.scene_type = "object" 

    def train(self):
        print(f"\nğŸ”¥ [4/4] å¼€å§‹è®­ç»ƒ (Splatfacto)")
        
        collider_args, scene_type = analyze_and_calculate_adaptive_collider(
            self.cfg.transforms_file,
            force_cull=self.cfg.force_spherical_culling,
            radius_scale=self.cfg.scene_radius_scale
        )
        self.scene_type = scene_type 

        cmd = [
            "ns-train", "splatfacto",
            "--data", str(self.cfg.data_dir),
            "--output-dir", str(self.output_dir),
            "--experiment-name", self.cfg.project_name,
            
            # ğŸ”¥ å…³é”®é…ç½®ï¼šå…³é—­ random-initï¼Œä¾é  COLMAP çš„é«˜ç²¾åº¦ç‚¹äº‘
            "--pipeline.model.random-init", "False", 
            
            "--pipeline.model.background-color", "random",
            "--pipeline.model.cull-alpha-thresh", "0.05",
            "--pipeline.model.stop-split-at", "25000",
            *collider_args,
            "--max-num-iterations", "15000",
            "--vis", "viewer+tensorboard",
            "--viewer.quit-on-train-completion", "True",
            "nerfstudio-data",
            "--downscale-factor", "1",
            "--auto-scale-poses", "False"
        ]
        
        subprocess.run(cmd, check=True, env=self.env)

    def export(self):
        print(f"\nğŸ’¾ [å¯¼å‡º] æ­£åœ¨è½¬æ¢æ¨¡å‹æ ¼å¼...")
        base_dir = self.output_dir / self.cfg.project_name / "splatfacto"
        try:
            runs = sorted([p for p in base_dir.iterdir() if p.is_dir()])
            config_path = runs[-1] / "config.yml"
        except IndexError:
            print("âŒ æœªæ‰¾åˆ°è®­ç»ƒè®°å½•")
            return None

        subprocess.run([
            "ns-export", "gaussian-splat",
            "--load-config", str(config_path),
            "--output-dir", str(self.cfg.project_dir)
        ], check=True, env=self.env)

        raw_ply = self.cfg.project_dir / "splat.ply"
        if not raw_ply.exists():
            raw_ply = self.cfg.project_dir / "point_cloud.ply"
            
        cleaned_ply = self.cfg.project_dir / f"{self.cfg.project_name}_clean.ply"
        final_ply = raw_ply

        need_cull = (self.scene_type == "object" or self.cfg.force_spherical_culling)
        
        if need_cull and raw_ply.exists():
            print(f"    -> æ£€æµ‹åˆ°ç‰©ä½“æ¨¡å¼ï¼Œæ‰§è¡Œç‚¹äº‘æ¸…æ´—...")
            success = perform_percentile_culling(
                raw_ply, 
                self.cfg.transforms_file, 
                cleaned_ply,
                keep_percentile=self.cfg.keep_percentile
            )
            if success:
                final_ply = cleaned_ply

        script_dir = Path(__file__).parent
        results_dir = script_dir / "results"
        results_dir.mkdir(exist_ok=True)
        
        target_path = results_dir / f"{self.cfg.project_name}.ply"
        
        if final_ply.exists():
            shutil.copy2(str(final_ply), str(target_path))
            print(f"    ğŸ“¦ å·²å¤åˆ¶ç»“æœåˆ°: {target_path}")
            return target_path
        else:
            print("âŒ å¯¼å‡ºå¤±è´¥ï¼Œæ‰¾ä¸åˆ° PLY æ–‡ä»¶")
            return None

# ================= ä¸»æµç¨‹ =================
def run_pipeline(cfg: PipelineConfig):
    global_start_time = time.time()
    print(f"\nğŸš€ [BrainDance Engine] å¯åŠ¨ä»»åŠ¡: {cfg.project_name}")
    
    img_processor = ImageProcessor(cfg)
    colmap_runner = ColmapRunner(cfg) # ä½¿ç”¨ COLMAP 
    ai_segmentor = AISegmentor(cfg)
    nerf_engine = NerfstudioEngine(cfg)

    # Step 1: å‡†å¤‡
    if cfg.project_dir.exists(): shutil.rmtree(cfg.project_dir, ignore_errors=True)
    cfg.project_dir.mkdir(parents=True, exist_ok=True)
    
    temp_dir = cfg.project_dir / "temp_extract"
    temp_dir.mkdir(parents=True, exist_ok=True)
    subprocess.run(["ffmpeg", "-y", "-i", str(cfg.video_path), "-vf", "fps=10", "-q:v", "2", str(temp_dir / "frame_%05d.jpg")], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    
    img_processor.smart_filter_blurry_images(temp_dir)
    
    raw_images_dir = cfg.project_dir / "raw_images"
    raw_images_dir.mkdir(parents=True, exist_ok=True)
    all_imgs = sorted(list(temp_dir.glob("*")))
    limit = cfg.max_images
    if len(all_imgs) > limit:
        indices = np.linspace(0, len(all_imgs)-1, limit, dtype=int)
        all_imgs = [all_imgs[i] for i in sorted(list(set(indices)))]
    for img in all_imgs: shutil.copy2(str(img), str(raw_images_dir / img.name))
    shutil.rmtree(temp_dir)

    # Step 2: COLMAP (æ…¢ä½†å‡†)
    if not colmap_runner.run(): return

    # Step 3: AI (ç”Ÿæˆå¹²å‡€çš„é€æ˜å›¾)
    ai_segmentor.run()
    
    try:
        # å¼ºåˆ¶æ¸…ç†ç¼“å­˜
        output_cache = cfg.project_dir / "outputs"
        if output_cache.exists(): shutil.rmtree(output_cache)
        
        nerf_engine.train()
        final_path = nerf_engine.export()
        if final_path:
            print(f"\nğŸ‰ ä»»åŠ¡å®Œæˆï¼ç»“æœä½äº: \033[92m{final_path}\033[0m")
        else:
            print("\nâŒ ä»»åŠ¡å®Œæˆä½†å¯¼å‡ºå¤±è´¥")
    except Exception as e:
        print(f"âŒ å¤±è´¥: {e}")

    print(f"â±ï¸ æ€»è€—æ—¶: {format_duration(time.time() - global_start_time)}")

if __name__ == "__main__":
    script_dir = Path(__file__).resolve().parent
    video_file = script_dir / "test.mp4" 
    if len(sys.argv) > 1: video_file = Path(sys.argv[1])
    
    if not video_file.exists():
        print(f"âŒ æ‰¾ä¸åˆ°è§†é¢‘: {video_file}")
        sys.exit(1)

    cfg = PipelineConfig(
        project_name="process_3dgs_final", # æ”¹ä¸ªåé˜²å†²çª
        video_path=video_file,
        max_images=100, 
        enable_ai=True
    )
    
    run_pipeline(cfg)