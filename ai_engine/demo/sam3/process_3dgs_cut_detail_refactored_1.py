# ==============================================================================
# å¯¼å…¥æ ‡å‡†åº“å’Œç¬¬ä¸‰æ–¹åº“
# ==============================================================================
import subprocess  # [æ ‡å‡†åº“] ç”¨äºæ‰§è¡Œå¤–éƒ¨ç³»ç»Ÿå‘½ä»¤ï¼ˆå¦‚ ffmpeg, colmapï¼‰ï¼Œå®ç° Python ä¸æ“ä½œç³»ç»Ÿçš„äº¤äº’
import sys         # [æ ‡å‡†åº“] ç”¨äºè®¿é—®ä¸ Python è§£é‡Šå™¨ç´§å¯†ç›¸å…³çš„å˜é‡å’Œå‡½æ•°ï¼Œå¦‚è·å–å‘½ä»¤è¡Œå‚æ•° sys.argv
import shutil      # [æ ‡å‡†åº“] é«˜çº§æ–‡ä»¶æ“ä½œåº“ï¼Œæä¾›å¤åˆ¶(copy)ã€ç§»åŠ¨(move)ã€åˆ é™¤ç›®å½•æ ‘ç­‰åŠŸèƒ½
import os          # [æ ‡å‡†åº“] æä¾›æ“ä½œç³»ç»Ÿæ¥å£ï¼Œç”¨äºæ–‡ä»¶è·¯å¾„æ“ä½œã€è¯»å–ç¯å¢ƒå˜é‡ã€æ–‡ä»¶å­˜åœ¨æ€§æ£€æŸ¥ç­‰
import time        # [æ ‡å‡†åº“] ç”¨äºæ—¶é—´å¤„ç†ï¼Œè®¡ç®—ä»£ç è¿è¡Œè€—æ—¶ï¼ˆæ€§èƒ½åˆ†æï¼‰
import datetime    # [æ ‡å‡†åº“] ç”¨äºå°†ç§’æ•°è½¬æ¢ä¸ºäººç±»å¯è¯»çš„æ—¥æœŸå’Œæ—¶é—´æ ¼å¼ (HH:MM:SS)
from pathlib import Path  # [æ ‡å‡†åº“] é¢å‘å¯¹è±¡çš„æ–‡ä»¶ç³»ç»Ÿè·¯å¾„åº“ï¼Œæ¯” os.path æ›´ç›´è§‚ï¼Œæ”¯æŒ .parent, .name ç­‰é“¾å¼è°ƒç”¨
import json        # [æ ‡å‡†åº“] ç”¨äºè¯»å†™ JSON æ ¼å¼æ–‡ä»¶ï¼Œè¿™é‡Œä¸»è¦ç”¨äºå¤„ç†ç›¸æœºä½å§¿æ–‡ä»¶ transforms.json
import numpy as np # [ç¬¬ä¸‰æ–¹åº“] Python ç§‘å­¦è®¡ç®—çš„æ ¸å¿ƒåº“ï¼Œç”¨äºå¤„ç†çŸ©é˜µè¿ç®—ã€å›¾åƒæ•°ç»„ï¼ˆHWCæ ¼å¼ï¼‰
# åœ¨ import numpy as np ä¸‹æ–¹ç¡®è®¤æ·»åŠ 
import torch
import logging     # [æ ‡å‡†åº“] æ—¥å¿—ç³»ç»Ÿï¼Œç”¨äºæ§åˆ¶æ§åˆ¶å°è¾“å‡ºçº§åˆ«ï¼Œå±è”½ä¸å¿…è¦çš„è­¦å‘Šä¿¡æ¯
import cv2         # [ç¬¬ä¸‰æ–¹åº“] OpenCV (Open Source Computer Vision)ï¼Œç”¨äºå›¾åƒè¯»å–ã€å½¢æ€å­¦æ“ä½œã€è½®å»“æŸ¥æ‰¾ç­‰
import re          # [æ ‡å‡†åº“] æ­£åˆ™è¡¨è¾¾å¼åº“ï¼Œç”¨äºå¤„ç†å¤æ‚çš„å­—ç¬¦ä¸²åŒ¹é…å’Œæå–

# ================= ğŸ§  AI ä¾èµ–å¼•å…¥ =================
# [å·¥ç¨‹åŒ–æ€è·¯] è½¯ä¾èµ–å¯¼å…¥ï¼šä¸ºäº†ä¿è¯ç¨‹åºçš„å¥å£®æ€§ï¼Œä¸è¦å› ä¸ºç¼ºå¤± AI ç›¸å…³çš„åº“ï¼ˆéæ ¸å¿ƒåŠŸèƒ½ï¼‰è€Œå¯¼è‡´æ•´ä¸ªç¨‹åºå´©æºƒã€‚
# è¿™é‡Œä½¿ç”¨äº† try-except å—æ¥æ£€æµ‹æ˜¯å¦å®‰è£…äº† AI ç›¸å…³çš„åº“ã€‚
try:
    import dashscope  # [ç¬¬ä¸‰æ–¹åº“] é˜¿é‡Œäº‘ç™¾ç‚¼ SDKï¼Œç”¨äºè°ƒç”¨ Qwen-VL (é€šä¹‰åƒé—®è§†è§‰ç‰ˆ) å¤šæ¨¡æ€å¤§æ¨¡å‹
    from dashscope import MultiModalConversation # å…·ä½“å¯¼å…¥å¤šæ¨¡æ€å¯¹è¯ç±»ï¼Œç”¨äºå‘é€å›¾ç‰‡å’Œæ–‡æœ¬ç»™å¤§æ¨¡å‹
    from ultralytics import SAM, YOLOWorld # [ç¬¬ä¸‰æ–¹åº“] Ultralytics åº“ï¼Œå°è£…äº†æœ€å…ˆè¿›çš„è§†è§‰æ¨¡å‹ï¼šYOLO (ç›®æ ‡æ£€æµ‹) å’Œ SAM (åˆ†å‰²ä¸‡ç‰©)
    HAS_AI = True     # [å…¨å±€å˜é‡] æ ‡è®°ä½ï¼Œè®¾ç½®ä¸º Trueï¼Œåç»­é€»è¾‘ä¼šæ ¹æ®è¿™ä¸ªå˜é‡å†³å®šæ˜¯å¦å¼€å¯æ™ºèƒ½åˆ†å‰²
except ImportError:
    HAS_AI = False    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œæ ‡è®°ä¸º False
    # [ç”¨æˆ·äº¤äº’] å‹å¥½çš„é”™è¯¯æç¤ºï¼Œå‘ŠçŸ¥ç”¨æˆ·ç¼ºå¤±äº†ä»€ä¹ˆåº“ä»¥åŠå¦‚ä½•é€šè¿‡ pip å®‰è£…ä¿®å¤
    print("âš ï¸ [ç¯å¢ƒè­¦å‘Š] æœªæ£€æµ‹åˆ° dashscope æˆ– ultralytics åº“ã€‚")
    print("    -> æ™ºèƒ½åˆ†å‰²åŠŸèƒ½å°†è¢«ç¦ç”¨ã€‚è¯·è¿è¡Œ: pip install dashscope ultralytics")

# ğŸ”¥ è¯·åœ¨æ­¤å¤„å¡«å…¥ä½ çš„ API KEY (æˆ–è€…ç¡®ä¿ç¯å¢ƒå˜é‡ DASHSCOPE_API_KEY å·²å­˜åœ¨)
# os.environ["DASHSCOPE_API_KEY"] = "sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"

# ================= ğŸ”§ åŸºç¡€é…ç½® =================
# è®¾ç½®æ—¥å¿—çº§åˆ«
# å±è”½ nerfstudio åº“ä¸­é Error çº§åˆ«çš„æ—¥å¿—ï¼Œé˜²æ­¢æ§åˆ¶å°è¢«å¤§é‡çš„ Warning/Info åˆ·å±ï¼Œå½±å“è§‚å¯Ÿè¿›åº¦
logging.getLogger('nerfstudio').setLevel(logging.ERROR) 

from dataclasses import dataclass, field # [æ ‡å‡†åº“] ç”¨äºå¿«é€Ÿåˆ›å»ºâ€œæ•°æ®ç±»â€ï¼Œå‡å°‘æ ·æ¿ä»£ç ï¼ˆå¦‚ __init__ï¼‰
from pathlib import Path
import os
import sys

# ==============================================================================
# ç±»: PipelineConfig (æµæ°´çº¿é…ç½®ç±»)
# ------------------------------------------------------------------------------
# [ä¾èµ–åº“]: dataclasses, pathlib, os
# [åŠŸèƒ½]: ç®¡ç†æ•´ä¸ªå·¥ç¨‹çš„æ‰€æœ‰é…ç½®é¡¹å’Œè·¯å¾„ã€‚
#         å®ƒåˆ©ç”¨ __post_init__ ç‰¹æ€§ï¼Œåœ¨åˆå§‹åŒ–åè‡ªåŠ¨è®¡ç®—å‡ºæ‰€æœ‰æ´¾ç”Ÿè·¯å¾„ï¼Œ
#         é¿å…äº†åœ¨ä»£ç å„å¤„æ‰‹åŠ¨æ‹¼æ¥è·¯å¾„å¯¼è‡´çš„æ··ä¹±å’Œæ‹¼å†™é”™è¯¯ã€‚
# ==============================================================================
@dataclass
class PipelineConfig:
    # 1. ã€å¿…å¡«é¡¹ã€‘ç”¨æˆ·åˆå§‹åŒ–æ—¶å¿…é¡»ä¼ å…¥çš„å‚æ•°
    project_name: str    # é¡¹ç›®åç§°ï¼Œå°†ç”¨ä½œæ–‡ä»¶å¤¹å
    video_path: Path     # è¾“å…¥è§†é¢‘çš„è·¯å¾„
    
    # 2. ã€é€‰å¡«é¡¹ã€‘æœ‰é»˜è®¤å€¼çš„é…ç½® (å¯¹åº”åŸä»£ç çš„å…¨å±€å˜é‡)
    work_root: Path = Path.home() / "braindance_workspace" # å·¥ä½œæ ¹ç›®å½•ï¼Œé»˜è®¤ä¸ºç”¨æˆ·ä¸»ç›®å½•ä¸‹çš„ braindance_workspace
    max_images: int = 180              # æœ€å¤§å›¾ç‰‡æ•°é‡é™åˆ¶ï¼Œé˜²æ­¢æ˜¾å­˜çˆ†ç‚¸æˆ–è®­ç»ƒæ—¶é—´è¿‡é•¿
    force_spherical_culling: bool = True # æ˜¯å¦å¼ºåˆ¶å¼€å¯çƒå½¢è£å‰ªï¼ˆåˆ‡é™¤è¿œå¤„çš„æ‚æ™¯ï¼‰
    scene_radius_scale: float = 1.8    # åœºæ™¯åŠå¾„ç¼©æ”¾å› å­ï¼Œç”¨äºè®¡ç®— Nerfstudio çš„ collider (ç¢°æ’ä½“) å‚æ•°
    keep_percentile: float = 0.9       # ç‚¹äº‘æ¸…æ´—æ—¶çš„ä¿ç•™æ¯”ä¾‹ (ä¿ç•™ 90% çš„ç‚¹)
    enable_ai: bool = True             # AI åŠŸèƒ½çš„æ€»å¼€å…³
    
    # 3. ã€è‡ªåŠ¨è®¡ç®—é¡¹ã€‘ç”¨æˆ·ä¸éœ€è¦ä¼ ï¼Œç”±ç¨‹åºè‡ªåŠ¨è®¡ç®—å‡ºæ¥çš„è·¯å¾„
    # field(init=False) å‘Šè¯‰ dataclassï¼šè¿™ä¸ªå˜é‡è™½ç„¶æ˜¯ç±»çš„å±æ€§ï¼Œä½†åœ¨ __init__ æ„é€ å‡½æ•°ä¸­ä¸éœ€è¦ä½œä¸ºå‚æ•°ä¼ å…¥
    project_dir: Path = field(init=False)      # é¡¹ç›®ä¸»ç›®å½•
    data_dir: Path = field(init=False)         # æ•°æ®å­˜æ”¾ç›®å½• (nerfstudio æ ¼å¼)
    images_dir: Path = field(init=False)       # å›¾ç‰‡å­˜æ”¾ç›®å½•
    masks_dir: Path = field(init=False)        # æ©ç (Mask)å­˜æ”¾ç›®å½•
    transforms_file: Path = field(init=False)  # ç›¸æœºä½å§¿æ–‡ä»¶ (transforms.json) è·¯å¾„
    vocab_tree_path: Path = field(init=False)  # COLMAP è¯æ±‡æ ‘æ–‡ä»¶è·¯å¾„

    # sam3
    model_root: Path = Path("/home/ltx/workspace/ai/sam3") 


    def __post_init__(self):
        """
        [é­”æ³•æ–¹æ³•] è¿™ä¸ªå‡½æ•°ä¼šåœ¨ç±»åˆå§‹åŒ–(__init__)å®Œæˆä¹‹åï¼Œè‡ªåŠ¨æ‰§è¡Œï¼
        æˆ‘ä»¬åœ¨è¿™é‡Œé›†ä¸­å¤„ç†æ‰€æœ‰çš„è·¯å¾„æ‹¼æ¥å’Œç¯å¢ƒè®¾ç½®ï¼Œå®ç°"é…ç½®å³é€»è¾‘"ã€‚
        """
        # --- A. è‡ªåŠ¨è®¡ç®—è·¯å¾„ ---
        # è·¯å¾„æ‹¼æ¥ï¼šwork_root / project_name
        self.project_dir = self.work_root / self.project_name
        # æ•°æ®ç›®å½•ï¼šproject_dir / data
        self.data_dir = self.project_dir / "data"
        # å›¾ç‰‡ç›®å½•ï¼šdata_dir / images
        self.images_dir = self.data_dir / "images"
        # æ©ç ç›®å½•ï¼šdata_dir / masks
        self.masks_dir = self.data_dir / "masks"
        # ä½å§¿æ–‡ä»¶ï¼šdata_dir / transforms.json
        self.transforms_file = self.data_dir / "transforms.json"
        
        # è¯æ±‡æ ‘è·¯å¾„ (ç”¨äº COLMAP ç‰¹å¾åŒ¹é…åŠ é€Ÿ)
        self.vocab_tree_path = self.work_root / "vocab_tree_flickr100k_words.bin"

        # [æ–°å¢] ç¡®ä¿æ¨¡å‹ç›®å½•å­˜åœ¨
        self.model_root.mkdir(parents=True, exist_ok=True)
        self.project_dir = self.work_root / self.project_name

        # --- B. ç¯å¢ƒä¿®æ­£ ---
        # è®¾ç½® Setuptools çš„ç¯å¢ƒå˜é‡ï¼Œè§£å†³ Python 3.12+ ä¸­ distutils è¢«ç§»é™¤å¯¼è‡´çš„å…¼å®¹æ€§é—®é¢˜
        os.environ["SETUPTOOLS_USE_DISTUTILS"] = "stdlib"

        # éªŒè¯ä¸€ä¸‹æ¨¡å‹ç›®å½•æ˜¯å¦å­˜åœ¨ï¼Œæ–¹ä¾¿è°ƒè¯•
        if not self.model_root.exists():
            print(f"âš ï¸ è­¦å‘Š: æ¨¡å‹ç›®å½•ä¸å­˜åœ¨ -> {self.model_root}")
        

# [å¯é€‰ä¾èµ–æ£€æµ‹] æ£€æŸ¥ plyfile åº“
# plyfile ç”¨äºè¯»å†™ .ply æ ¼å¼çš„ç‚¹äº‘æ–‡ä»¶ï¼Œå¦‚æœæ²¡å®‰è£…ï¼Œåç»­çš„ç‚¹äº‘æ¸…æ´—åŠŸèƒ½ä¼šå¤±æ•ˆ
try:
    from plyfile import PlyData, PlyElement # [ç¬¬ä¸‰æ–¹åº“] ç”¨äº PLY æ–‡ä»¶è¯»å†™
    HAS_PLYFILE = True
except ImportError:
    HAS_PLYFILE = False # æ ‡è®°ä¸ºä¸å¯ç”¨

# ================= ğŸ§  AI æ ¸å¿ƒé€»è¾‘å‡½æ•° =================

# ==============================================================================
# å‡½æ•°: get_central_object_prompt
# ------------------------------------------------------------------------------
# [ä¾èµ–åº“]: dashscope (é˜¿é‡Œäº‘ SDK), os, pathlib, numpy
# [åŠŸèƒ½]: ä½¿ç”¨å¤šæ¨¡æ€å¤§æ¨¡å‹ (Qwen-VL-Plus) æ™ºèƒ½åˆ†æå›¾ç‰‡å†…å®¹ã€‚
#         å®ƒä¼šè¯»å–æ–‡ä»¶å¤¹ä¸­çš„å‡ å¼ é‡‡æ ·å›¾ç‰‡ï¼Œè¯¢é—® AI "ç”»é¢ä¸­å¿ƒçš„ç‰©ä½“æ˜¯ä»€ä¹ˆï¼Ÿ"ï¼Œ
#         å¹¶è¦æ±‚ AI è¿”å›ä¸€ä¸ªé€‚åˆ YOLO ç›®æ ‡æ£€æµ‹çš„è‹±æ–‡æç¤ºè¯ (Prompt)ã€‚
# ==============================================================================
def get_central_object_prompt(images_dir: Path, sample_count=7):
    """
    [Step 1.1] ä½¿ç”¨ Qwen-VL-Plus å¤šå›¾åˆ†æï¼Œæå–ä¸­å¿ƒç‰©ä½“çš„æ–‡æœ¬æè¿°
    
    å‚æ•°:
        images_dir (Path): å­˜æ”¾å›¾ç‰‡çš„æ–‡ä»¶å¤¹è·¯å¾„
        sample_count (int): é‡‡æ ·å›¾ç‰‡æ•°é‡ï¼Œé»˜è®¤3å¼ ã€‚å°‘é‡‡å‡ å¼ å¯ä»¥èŠ‚çœ Token è´¹ç”¨å¹¶åŠ å¿«é€Ÿåº¦
    
    è¿”å›:
        prompt_text (str): å¤§æ¨¡å‹ç”Ÿæˆçš„ç‰©ä½“æè¿°æç¤ºè¯ (ä¾‹å¦‚ "red apple")ï¼Œå¦‚æœå¤±è´¥è¿”å› None
    """
    # 1. è·å–ç¯å¢ƒå˜é‡ä¸­çš„ API Keyï¼Œè¿™æ˜¯è°ƒç”¨é˜¿é‡Œäº‘æœåŠ¡çš„å‡­è¯
    api_key = os.environ.get("DASHSCOPE_API_KEY")
    if not api_key:
        print("âŒ æœªè®¾ç½® DASHSCOPE_API_KEYï¼Œæ— æ³•è°ƒç”¨å¤§æ¨¡å‹ã€‚")
        return None

    print(f"\nğŸ§  [AI åˆ†æ] æ­£åœ¨è°ƒç”¨ Qwen-VL-Plus åˆ†æåœºæ™¯...")
    
    # 2. [Python è¿›é˜¶] ä½¿ç”¨ glob è·å–æ‰€æœ‰ jpg/png å›¾ç‰‡
    # sorted() ç¡®ä¿å›¾ç‰‡æŒ‰æ–‡ä»¶åé¡ºåºæ’åˆ—ï¼Œé¿å…æ¯æ¬¡è¿è¡Œé¡ºåºä¸ä¸€è‡´
    image_files = sorted(list(images_dir.glob("*.jpg")) + list(images_dir.glob("*.png")))
    if not image_files: return None # å¦‚æœæ–‡ä»¶å¤¹æ˜¯ç©ºçš„ï¼Œç›´æ¥è¿”å›
    
    # 3. [ç®—æ³•é€»è¾‘] å‡åŒ€é‡‡æ · (Uniform Sampling)
    # æˆ‘ä»¬ä¸åªå–å‰ä¸‰å¼ ï¼Œè€Œæ˜¯ä½¿ç”¨ numpy çš„ linspace åœ¨æ•´ä¸ªåºåˆ—ä¸­å‡åŒ€æŠ½å– sample_count å¼ å›¾
    # è¿™æ ·èƒ½è¦†ç›–ç‰©ä½“çš„ä¸åŒè§’åº¦ï¼ˆä¾‹å¦‚ï¼šæ­£é¢ã€ä¾§é¢ã€èƒŒé¢ï¼‰ï¼Œè®© AI çš„åˆ¤æ–­æ›´å‡†ç¡®
    indices = np.linspace(0, len(image_files) - 1, sample_count, dtype=int)
    sampled_imgs = [image_files[i] for i in indices]
    
    # 4. æ„å»ºå¤šæ¨¡æ€æ¶ˆæ¯ä½“ (Dashscope SDK è¦æ±‚çš„ç‰¹å®š JSON æ ¼å¼)
    # è¿™é‡Œçš„ list comprehension (åˆ—è¡¨æ¨å¯¼å¼) å°†æ¯å¼ å›¾ç‰‡è·¯å¾„è½¬ä¸ºå­—å…¸æ ¼å¼ {"image": "path/to/img.jpg"}
    content = [{"image": str(img_path)} for img_path in sampled_imgs]
    
    # è¿½åŠ æ–‡æœ¬æç¤º (Prompt Engineering)
    # è¿™é‡Œçš„ Prompt ç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼ŒæŒ‡ç¤º AIï¼š
    # - å…³æ³¨ç”»é¢ä¸­å¿ƒ
    # - è¾“å‡ºé€‚åˆæ£€æµ‹æ¨¡å‹çš„è¯ (ä¸è¦å¤æ‚çš„å½¢å®¹è¯)
    # - ä¼˜å…ˆæè¿°è§†è§‰ç‰¹å¾ (é¢œè‰²ã€å½¢çŠ¶) è€Œä¸æ˜¯åŠŸèƒ½åç§°
    content.append({
        "text": (
            "è¿™äº›æ˜¯ä¸€ä¸ªè§†é¢‘çš„æŠ½å¸§å›¾ç‰‡ã€‚è¯·åˆ†æç”»é¢ä¸­å¿ƒå§‹ç»ˆå­˜åœ¨çš„ã€æœ€ä¸»è¦çš„ä¸€ä¸ªç‰©ä½“æ˜¯ä»€ä¹ˆã€‚"
            "æˆ‘æ­£åœ¨ä½¿ç”¨ SAM 3 (Segment Anything Model 3) è¿›è¡ŒåŸºäºæ–‡æœ¬çš„è§†é¢‘è·Ÿè¸ªã€‚"
            "è¯·è¾“å‡ºä¸€ä¸ªã€æŒ‡ä»£æ€§æ˜ç¡®ã€‘çš„è‹±æ–‡çŸ­è¯­ (Referring Expression)ã€‚"
            
            "âš ï¸ å…³é”®ç­–ç•¥ï¼š"
            "1. å¿…é¡»åŒ…å«è§†è§‰ç‰¹å¾ï¼ˆé¢œè‰²ã€æè´¨ï¼‰ã€‚SAM 3 éœ€è¦ä¾é é¢œè‰²å’Œçº¹ç†å°†ç‰©ä½“ä»èƒŒæ™¯ä¸­åˆ†ç¦»ã€‚"
            "   - âŒ å Prompt: 'cup' (å®¹æ˜“æŠŠæ¡Œå­ä¹Ÿåˆ†è¿›å»)"
            "   - âœ… å¥½ Prompt: 'white ceramic cup' (ç™½è‰²é™¶ç“·æ¯)"
            "2. æè¿°ç‰©ä½“æœ¬èº«ï¼Œä¸è¦æè¿°åŠŸèƒ½ã€‚"
            "   - âŒ å Prompt: 'cleaning tool'"
            "   - âœ… å¥½ Prompt: 'blue plastic bottle'"
            "3. ä¿æŒç®€çŸ­ï¼Œç›´æ¥è¾“å‡ºè‹±æ–‡çŸ­è¯­ï¼Œä¸è¦æ ‡ç‚¹ç¬¦å·ã€‚"
        )
    })
    
    # å°è£…ä¸ºç”¨æˆ·æ¶ˆæ¯
    messages = [{"role": "user", "content": content}]

    try:
        # 5. [ç½‘ç»œè¯·æ±‚] è°ƒç”¨é˜¿é‡Œäº‘ Qwen-VL-Plus æ¨¡å‹
        # è¿™æ˜¯ä¸€ä¸ªåŒæ­¥è°ƒç”¨ï¼Œç¨‹åºä¼šåœ¨è¿™é‡Œç­‰å¾…æœåŠ¡å™¨è¿”å›ç»“æœ
        response = dashscope.MultiModalConversation.call(
            model='qwen-vl-plus', 
            messages=messages
        )
        
        # 6. è§£æè¿”å›ç»“æœ
        if response.status_code == 200:
            # æå– AI å›å¤çš„æ–‡æœ¬å†…å®¹
            prompt_text = response.output.choices[0].message.content[0]["text"].strip()
            # [æ•°æ®æ¸…æ´—] å»æ‰å¯èƒ½å­˜åœ¨çš„æ ‡ç‚¹ç¬¦å·ï¼ˆå¥å·ã€å¼•å·ï¼‰ï¼Œé˜²æ­¢å¹²æ‰° YOLO æ¨¡å‹è§£æ
            prompt_text = prompt_text.replace(".", "").replace('"', "").replace("'", "")
            
            # \033[92m æ˜¯ ANSI è½¬ä¹‰ç ï¼Œç”¨äºåœ¨æ§åˆ¶å°è¾“å‡ºç»¿è‰²é«˜äº®æ–‡å­—ï¼Œ\033[0m æ˜¯é‡ç½®é¢œè‰²
            print(f"    ğŸ¤– Qwen è®¤ä¸ºä¸­å¿ƒç‰©ä½“æ˜¯: [ \033[92m{prompt_text}\033[0m ]")
            return prompt_text
        else:
            # å¦‚æœçŠ¶æ€ç ä¸æ˜¯ 200ï¼Œè¯´æ˜ API è°ƒç”¨å‡ºé”™ (å¦‚æ¬ è´¹ã€ç½‘ç»œé”™è¯¯)
            print(f"âŒ Qwen è°ƒç”¨å¤±è´¥: {response.code} - {response.message}")
            return None
    except Exception as e:
        # æ•è·æ‰€æœ‰å¯èƒ½çš„å¼‚å¸¸ (å¦‚ç½‘ç»œæ–­è¿)
        print(f"âŒ API è¿æ¥å¼‚å¸¸: {e}")
        return None

# ==============================================================================
# å‡½æ•°: clean_and_verify_mask
# ------------------------------------------------------------------------------
# [ä¾èµ–åº“]: cv2 (OpenCV), numpy
# [åŠŸèƒ½]: å¯¹ AI ç”Ÿæˆçš„åˆ†å‰²æ©ç  (Mask) è¿›è¡Œ"ä½“æ£€"å’Œ"å‡€åŒ–"ã€‚
#         AI ç”Ÿæˆçš„ Mask å¾€å¾€æœ‰å™ªç‚¹ã€è¾¹ç¼˜æ¯›ç³™æˆ–åŒ…å«èƒŒæ™¯æ‚ç‰©ã€‚
#         æ­¤å‡½æ•°åˆ©ç”¨å½¢æ€å­¦å’Œè¿é€šåŸŸç®—æ³•ï¼Œå¼ºåˆ¶åªä¿ç•™ä¸»ä½“ï¼Œå¹¶å»é™¤è´¨é‡å·®çš„ Maskã€‚
# ==============================================================================
def clean_and_verify_mask(mask, img_name=""):
    """
    [V4 ä¼˜åŒ–ç‰ˆ] é’ˆå¯¹ç»†é•¿ç‰©ä½“(ç¬”)ä¼˜åŒ–ï¼Œå¢åŠ â€œèƒŒæ™¯è¯¯æ€â€æ‹¦æˆª
    """
    h, w = mask.shape
    
    # 1. è¿é€šåŸŸåˆ†æ
    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(mask, connectivity=8)
    if num_labels < 2: return False, None, "Empty Mask"

    # å¯»æ‰¾æœ€å¤§å‰æ™¯å—
    max_area = 0
    max_label = -1
    for i in range(1, num_labels):
        if stats[i, cv2.CC_STAT_AREA] > max_area:
            max_area = stats[i, cv2.CC_STAT_AREA]
            max_label = i
            
    # é˜ˆå€¼è¿‡æ»¤ 1ï¼šå¤ªå° (å™ªç‚¹)
    if max_area < (h * w * 0.005):
        return False, None, "Too Small/Noise"

    # ğŸ”¥ é˜ˆå€¼è¿‡æ»¤ 2 [æ–°å¢]ï¼šå¤ªå¤§ (è¯´æ˜å‰²åˆ°äº†æ¡Œå­/èƒŒæ™¯)
    # å¦‚æœç‰©ä½“å ç”»é¢è¶…è¿‡ 65%ï¼Œå¯¹äºä¸€æ”¯ç¬”æ¥è¯´æ˜¯ä¸å¯èƒ½çš„ï¼Œè‚¯å®šæ˜¯èƒŒæ™¯
    if max_area > (h * w * 0.65):
        return False, None, f"Too Large (Background? {max_area/(h*w):.0%})"

    cleaned_mask = (labels == max_label).astype(np.uint8) * 255

    # 2. å‡ ä½•ç‰¹å¾è´¨æ£€
    contours, _ = cv2.findContours(cleaned_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if not contours: return False, None, "No Contour"
    main_cnt = max(contours, key=cv2.contourArea)

    # å®å¿ƒåº¦æ£€æŸ¥ (æ”¾å®½ä¸€ç‚¹ç‚¹)
    hull = cv2.convexHull(main_cnt)
    hull_area = cv2.contourArea(hull)
    if hull_area == 0: return False, None, "Hull Area 0"
    solidity = max_area / hull_area
    if solidity < 0.75: # ä» 0.88 æ”¾å®½åˆ° 0.75ï¼Œå…è®¸ç¬”æœ‰ä¸€äº›ç¼ºå£
        return False, None, f"Rough Edges ({solidity:.2f})"

    # ğŸ”¥ é•¿å®½æ¯”æ£€æŸ¥ [é‡è¦ä¿®æ”¹]
    x, y, w_rect, h_rect = cv2.boundingRect(main_cnt)
    if h_rect == 0: return False, None, "Height 0"
    
    aspect_ratio = w_rect / h_rect
    # å¦‚æœç«–ç€æ”¾ï¼Œw/h å¯èƒ½ä¼šå¾ˆå°ï¼Œæˆ‘ä»¬è¦çœ‹é•¿è¾¹æ¯”çŸ­è¾¹
    real_ratio = max(aspect_ratio, 1/aspect_ratio)
    
    # ä» 4.5 æå‡åˆ° 15.0ï¼Œå…è®¸ç»†é•¿çš„ç¬”é€šè¿‡
    if real_ratio > 15.0: 
        return False, None, f"Bad Ratio ({real_ratio:.1f})"

    # 3. è¾¹ç¼˜è…èš€
    kernel = np.ones((3, 3), np.uint8)
    cleaned_mask = cv2.erode(cleaned_mask, kernel, iterations=1)

    return True, cleaned_mask, "OK"

# ==============================================================================
# å‡½æ•°: get_salient_box
# ------------------------------------------------------------------------------
# [ä¾èµ–åº“]: cv2, numpy, torch
# [åŠŸèƒ½]: [çº¯æœ¬åœ° CV ç®—æ³•] å½“ AI (YOLO) è¯†åˆ«å¤±è´¥æ—¶ï¼Œä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ (Fallback)ã€‚
#         å®ƒä¸ä¾èµ–ç¥ç»ç½‘ç»œï¼Œè€Œæ˜¯åˆ©ç”¨ä¼ ç»Ÿçš„å›¾åƒå¤„ç†ç®—æ³•å¯»æ‰¾ç”»é¢ä¸­"çº¹ç†æœ€å¤æ‚"çš„åŒºåŸŸã€‚
#         åŸç†ï¼šæ‹‰æ™®æ‹‰æ–¯è¾¹ç¼˜æ£€æµ‹ -> è†¨èƒ€è¿æ¥ -> æ‰¾æœ€å¤§å¤–æ¥çŸ©å½¢ã€‚
# ==============================================================================
def get_salient_box(img_path, margin_ratio=0.1):
    try:
        # è¯»å–å›¾ç‰‡
        img = cv2.imread(str(img_path))
        if img is None: return None
        
        # 1. è½¬ç°åº¦å¹¶è®¡ç®—è¾¹ç¼˜ (Laplacian Edge Detection)
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        
        # [ç®—æ³•é€»è¾‘] æ‹‰æ™®æ‹‰æ–¯ç®—å­è®¡ç®—å›¾åƒäº®åº¦çš„äºŒé˜¶å¯¼æ•°ã€‚
        # å¯¼æ•°å˜åŒ–å¤§çš„åœ°æ–¹å°±æ˜¯è¾¹ç¼˜ã€‚CV_64F å…è®¸ä½¿ç”¨æµ®ç‚¹æ•°å­˜å‚¨è´Ÿå€¼ï¼Œé˜²æ­¢è®¡ç®—è¿‡ç¨‹ä¸­æ•°æ®æˆªæ–­ã€‚
        laplacian = cv2.Laplacian(gray, cv2.CV_64F)
        laplacian = np.uint8(np.absolute(laplacian)) # å–ç»å¯¹å€¼å¹¶è½¬å› 8ä½æ•´æ•° (0-255)
        
        # 2. æ¨¡ç³Šä¸äºŒå€¼åŒ–
        # [ç®—æ³•é€»è¾‘] é«˜æ–¯æ¨¡ç³Š (Gaussian Blur) ç”¨äºå¹³æ»‘æ‰ç»†å°çš„å™ªç‚¹çº¹ç†ï¼Œè®©çœŸæ­£æ˜æ˜¾çš„è¾¹ç¼˜èšé›†åœ¨ä¸€èµ·ã€‚
        blurred = cv2.GaussianBlur(laplacian, (25, 25), 0)
        
        # [ç®—æ³•é€»è¾‘] åŠ¨æ€é˜ˆå€¼ (Percentile Thresholding)
        # æˆ‘ä»¬ä¸çŸ¥é“è¾¹ç¼˜çš„å…·ä½“æ•°å€¼æ˜¯å¤šå°‘ï¼Œæ‰€ä»¥ä½¿ç”¨ç™¾åˆ†ä½æ•°ã€‚
        # è¿™é‡Œåªä¿ç•™äº®åº¦æ’åœ¨å‰ 20% çš„åŒºåŸŸï¼ˆå³çº¹ç†æœ€ä¸°å¯Œçš„åœ°æ–¹ï¼‰ï¼Œè§†ä¸ºæ„Ÿå…´è¶£åŒºåŸŸã€‚
        threshold_val = np.percentile(blurred, 80) 
        _, binary = cv2.threshold(blurred, threshold_val, 255, cv2.THRESH_BINARY)
        
        # 3. æ‰¾æœ€å¤§è½®å»“
        # åœ¨äºŒå€¼åŒ–å›¾åƒä¸­æ‰¾è½®å»“
        contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        if not contours: return None
        
        # å‡è®¾çº¹ç†æœ€å¤æ‚çš„åŒºåŸŸï¼ˆè½®å»“é¢ç§¯æœ€å¤§ï¼‰å°±æ˜¯ä¸»ä½“
        max_cnt = max(contours, key=cv2.contourArea)
        x, y, w, h = cv2.boundingRect(max_cnt) # è·å–è¾¹ç•Œæ¡†
        
        # 4. åŠ ä¸Šå®‰å…¨è¾¹è· (Padding)
        # ä¼ ç»Ÿç®—æ³•æ‰¾å‡ºçš„æ¡†å¾€å¾€æ¯”è¾ƒç´§ï¼Œæˆ‘ä»¬æŒ‰æ¯”ä¾‹ (margin_ratio) å‘å¤–æ‰©ä¸€ç‚¹ï¼Œç»™ SAM ç•™å‡ºä½™åœ°ã€‚
        H, W = img.shape[:2]
        pad_x = int(w * margin_ratio)
        pad_y = int(h * margin_ratio)
        
        # è®¡ç®—æ‰©å……åçš„åæ ‡ï¼Œå¹¶é™åˆ¶åœ¨å›¾åƒè¾¹ç•Œå†… (min/max)
        x1 = max(0, x - pad_x)
        y1 = max(0, y - pad_y)
        x2 = min(W, x + w + pad_x)
        y2 = min(H, y + h + pad_y)
        
        # è¿”å› torch tensor æ ¼å¼ï¼Œå› ä¸ºåç»­çš„ SAM æ¨¡å‹éœ€è¦ Tensor è¾“å…¥
        import torch
        return torch.tensor([[x1, y1, x2, y2]], dtype=torch.float32)
        
    except Exception as e:
        print(f"       âš ï¸ è§†è§‰é‡å¿ƒè®¡ç®—å¤±è´¥: {e}")
        return None

# ==============================================================================
# å‡½æ•°: run_ai_segmentation_pipeline (SAM 3 å‡çº§ç‰ˆ)
# ------------------------------------------------------------------------------
# [ä¿®æ”¹è¯´æ˜]: 
# 1. ç§»é™¤äº† YOLO-World æ¨¡å‹ï¼Œç›´æ¥ä½¿ç”¨ SAM 3 çš„æ–‡æœ¬æç¤ºåŠŸèƒ½ã€‚
# 2. å¯ç”¨äº† SAM 3 çš„åºåˆ—å¤„ç†èƒ½åŠ›ï¼Œè¾“å…¥æ•´ä¸ªæ–‡ä»¶å¤¹è¿›è¡Œæ¨ç†ã€‚
# ==============================================================================
def run_ai_segmentation_pipeline(data_dir: Path):
    """
    é»„é‡‘ç»„åˆ V4: å¤šç‚¹è§¦æ§ä¿åº• + å¼ºåŠ›èƒŒæ™¯æŠ‘åˆ¶ + æ¯”ä¾‹æ”¾å®½
    """
    if not HAS_AI: return False
    
    import logging
    logging.getLogger("ultralytics").setLevel(logging.ERROR)
    
    images_dir = data_dir / "images"
    masks_dir = data_dir / "masks"
    debug_dir = data_dir / "debug_combo"
    debug_dir.mkdir(parents=True, exist_ok=True)
    masks_dir.mkdir(parents=True, exist_ok=True)

    cfg.transforms_file = data_dir / "transforms.json" 
    if not cfg.transforms_file.exists(): return False

    print(f"\nâœ‚ï¸ [æ™ºèƒ½åˆ†å‰²] åˆå§‹åŒ– (YOLO V2 + SAM 3 Multi-Point)...")
    try:
        text_prompt = get_central_object_prompt(images_dir)
        if " on " in text_prompt: text_prompt = text_prompt.split(" on ")[0]
    except: text_prompt = "object"
    if not text_prompt: text_prompt = "object"
    print(f"    ğŸ¯ æ ¸å¿ƒ Prompt: '\033[92m{text_prompt}\033[0m'")

    yolo_path = cfg.model_root / "yolov8s-worldv2.pt"
    if not yolo_path.exists(): yolo_path = "yolov8s-worldv2.pt"
    sam_path = cfg.model_root / "sam3.pt"
    
    try:
        det_model = YOLOWorld(str(yolo_path))
        det_model.set_classes([text_prompt])
        sam_model = SAM(str(sam_path))
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return False

    with open(cfg.transforms_file, 'r') as f: meta = json.load(f)
    frames_map = {Path(f["file_path"]).name: f for f in meta["frames"]}
    valid_frames_list = []
    
    image_files = sorted(list(images_dir.glob("*.jpg")) + list(images_dir.glob("*.png")))
    total_imgs = len(image_files)
    
    print(f"    -> å¼€å§‹å¤„ç† {total_imgs} å¸§...")
    start_time = time.time()

    for i, img_path in enumerate(image_files):
        elapsed = time.time() - start_time
        fps = (i + 1) / (elapsed + 1e-6)
        process_success = False 
        
        try:
            original_img = cv2.imread(str(img_path))
            if original_img is None: raise ValueError("æ— æ³•è¯»å–å›¾ç‰‡")
            h_real, w_real = original_img.shape[:2]

            # --- Step 1: YOLO æ‰¾æ¡† ---
            det_results = det_model.predict(img_path, conf=0.05, verbose=False) 
            bboxes = det_results[0].boxes.xyxy.cpu()
            
            final_box = None
            is_fallback = False 
            
            if len(bboxes) > 0:
                center_x, center_y = w_real / 2, h_real / 2
                min_dist = float('inf')
                for box in bboxes:
                    bx = (box[0] + box[2]) / 2
                    by = (box[1] + box[3]) / 2
                    dist = (bx - center_x)**2 + (by - center_y)**2
                    if dist < min_dist:
                        min_dist = dist
                        final_box = box.unsqueeze(0)
            
            # --- Step 2: SAM 3 ---
            final_mask = None
            if final_box is not None:
                # æ–¹æ¡ˆ A: æœ‰æ¡†
                sam_results = sam_model(img_path, bboxes=final_box, verbose=False)
            else:
                # æ–¹æ¡ˆ B: ä¿åº• (å¤šç‚¹è§¦æ§ + èƒŒæ™¯æŠ‘åˆ¶)
                is_fallback = True
                cx, cy = w_real / 2, h_real / 2
                
                # ğŸ”¥ å…³é”®ä¿®æ”¹ï¼šæ„å»º 9 ä¸ªç‚¹
                # 5ä¸ªæ­£æ ·æœ¬(Label 1): ä¸­å¿ƒ + ä¸Šä¸‹å·¦å³å¾®å (å¢åŠ æ‰“ä¸­ç»†é•¿ç¬”çš„æ¦‚ç‡)
                # 4ä¸ªè´Ÿæ ·æœ¬(Label 0): å›¾ç‰‡å››ä¸ªè§’ (å¼ºåˆ¶ SAM ä¸é€‰èƒŒæ™¯)
                offset = 30 # åç§»é‡åƒç´ 
                input_points = [
                    [cx, cy], # ä¸­å¿ƒ
                    [cx-offset, cy], [cx+offset, cy], # å·¦å³
                    [cx, cy-offset], [cx, cy+offset], # ä¸Šä¸‹
                    [0, 0], [w_real, 0], [0, h_real], [w_real, h_real] # å››è§’èƒŒæ™¯
                ]
                input_labels = [1, 1, 1, 1, 1, 0, 0, 0, 0] # 1æ˜¯å‰æ™¯ï¼Œ0æ˜¯èƒŒæ™¯
                
                sam_results = sam_model(img_path, points=input_points, labels=input_labels, verbose=False)

            if sam_results[0].masks is not None:
                masks_data = sam_results[0].masks.data.cpu().numpy()
                if masks_data.shape[0] > 0:
                    areas = np.sum(masks_data, axis=(1, 2))
                    # åœ¨ Fallback æ¨¡å¼ä¸‹ï¼Œæˆ‘ä»¬è¦å°å¿ƒæœ€å¤§çš„å—å¯èƒ½æ˜¯æ¡Œå­
                    # ä½†æˆ‘ä»¬åœ¨ clean å‡½æ•°é‡Œæœ‰ max_area æ‹¦æˆªï¼Œæ‰€ä»¥è¿™é‡Œè¿˜æ˜¯å–æœ€å¤§
                    largest_idx = np.argmax(areas)
                    final_mask = masks_data[largest_idx].astype(np.uint8) * 255
            
            if final_mask is None:
                final_mask = np.zeros((h_real, w_real), dtype=np.uint8)

            # --- Step 3: æ¸…æ´—ä¸éªŒè¯ ---
            status_icon = "ğŸŸ¢" if not is_fallback else "ğŸ”µ"
            print(f"       [{i+1}/{total_imgs}] {img_path.name} | {status_icon} | âš¡ {fps:.1f} fps          ", end="\r")

            is_good, cleaned_mask, reason = clean_and_verify_mask(final_mask, img_path.name)

            # --- å¯è§†åŒ– ---
            if i % 2 == 0 or not is_good: 
                debug_img = original_img.copy()
                color = (0, 255, 0) if not is_fallback else (255, 100, 0) # ç»¿è‰²YOLO, è“è‰²Point
                
                if final_box is not None:
                    x1, y1, x2, y2 = final_box[0].int().tolist()
                    cv2.rectangle(debug_img, (x1, y1), (x2, y2), color, 2)
                    cv2.putText(debug_img, "YOLO", (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
                elif is_fallback:
                    # ç”»å‡ºé‚£5ä¸ªä¸­å¿ƒç‚¹
                    cx, cy = int(w_real/2), int(h_real/2)
                    offset = 30
                    pts = [(cx, cy), (cx-offset, cy), (cx+offset, cy), (cx, cy-offset), (cx, cy+offset)]
                    for pt in pts:
                        cv2.circle(debug_img, (int(pt[0]), int(pt[1])), 5, color, -1)
                    cv2.putText(debug_img, "MULTI-POINT", (cx-40, cy-40), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

                if is_good:
                    colored_mask = np.zeros_like(debug_img)
                    colored_mask[cleaned_mask > 0] = (0, 0, 255) 
                    debug_img = cv2.addWeighted(debug_img, 0.7, colored_mask, 0.3, 0)
                else:
                    cv2.putText(debug_img, f"REJECT: {reason}", (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)

                cv2.imwrite(str(debug_dir / f"vis_{img_path.name}"), debug_img)

            # --- ä¿å­˜ ---
            if is_good:
                if cleaned_mask.shape[:2] != original_img.shape[:2]:
                    cleaned_mask = cv2.resize(cleaned_mask, (w_real, h_real), interpolation=cv2.INTER_NEAREST)
                
                mask_blurred = cv2.GaussianBlur(cleaned_mask, (5, 5), 0)
                b, g, r = cv2.split(original_img)
                img_bgra = cv2.merge([b, g, r, mask_blurred])
                
                new_img_path = img_path.with_suffix('.png')
                cv2.imwrite(str(new_img_path), img_bgra)
                
                if img_path.name in frames_map:
                    frame_data = frames_map[img_path.name]
                    frame_data["file_path"] = f"images/{new_img_path.name}"
                    valid_frames_list.append(frame_data)
                process_success = True

        except Exception as e:
            print(f"\nâŒ Frame {i} Error: {e}")
            process_success = False 

        finally:
            if img_path.exists() and img_path.suffix.lower() == '.jpg':
                if process_success:
                    try: img_path.unlink() 
                    except: pass
                else:
                    try: img_path.unlink()
                    except: pass

    print(f"\n\nğŸ“Š å®Œæˆã€‚å‰©ä½™å¯ç”¨: {len(valid_frames_list)}")
    if len(valid_frames_list) == 0: return False

    meta["frames"] = valid_frames_list
    with open(cfg.transforms_file, 'w') as f: json.dump(meta, f, indent=4)
    return True

# ================= è¾…åŠ©å·¥å…· =================

def format_duration(seconds):
    """
    [è¾…åŠ©å‡½æ•°] å°†ç§’æ•°è½¬æ¢ä¸ºæ˜“è¯»çš„ HH:MM:SS æ ¼å¼
    [ä¾èµ–åº“]: datetime
    """
    # [æ ‡å‡†åº“] datetime.timedelta è‡ªåŠ¨å¤„ç†æ—¶é—´æ¢ç®—ï¼ˆå¦‚ 3661ç§’ -> 1:01:01ï¼‰
    return str(datetime.timedelta(seconds=int(seconds)))

# ==============================================================================
# ç±»: ImageProcessor
# ------------------------------------------------------------------------------
# [ä¾èµ–åº“]: cv2, numpy, shutil, pathlib
# [åŠŸèƒ½]: è´Ÿè´£å›¾åƒçš„é¢„å¤„ç†ï¼Œç‰¹åˆ«æ˜¯æ¨¡ç³Šæ£€æµ‹ã€‚
#         åœ¨è¿›è¡Œ 3D é‡å»ºå‰ï¼Œå»é™¤æ¨¡ç³Šçš„å›¾ç‰‡å¯ä»¥æ˜¾è‘—æé«˜é‡å»ºè´¨é‡ã€‚
# ==============================================================================
class ImageProcessor:
    def __init__(self, config: PipelineConfig):
        self.cfg = config

    def smart_filter_blurry_images(self, image_folder, keep_ratio=0.85):
        """
        [å›¾åƒæ¸…æ´—ç®—æ³•] æ··åˆç­–ç•¥æ¨¡ç³Šæ£€æµ‹
        åŸç†ï¼šåˆ©ç”¨æ‹‰æ™®æ‹‰æ–¯ç®—å­çš„æ–¹å·® (Variance of Laplacian) æ¥è¡¡é‡å›¾åƒæ¸…æ™°åº¦ã€‚
        """
        print(f"\nğŸ§  [æ™ºèƒ½æ¸…æ´—] æ­£åœ¨åˆ†æå›¾ç‰‡è´¨é‡ (æ··åˆç­–ç•¥ç‰ˆ)...")
        image_dir = Path(image_folder)
        images = sorted([p for p in image_dir.iterdir() if p.suffix.lower() in ['.jpg', '.jpeg', '.png']])
        if not images: return
        
        trash_dir = image_dir.parent / "trash_smart"
        trash_dir.mkdir(exist_ok=True)
        
        img_scores = []
        for i, img_path in enumerate(images):
            img = cv2.imread(str(img_path))
            if img is None: continue
            
            # è½¬ç°åº¦
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            h, w = gray.shape
            
            # [ç®—æ³•é€»è¾‘] åˆ†å—æ£€æµ‹
            # ä¸ºäº†é˜²æ­¢åªå› ä¸ºèƒŒæ™¯æ¨¡ç³Šï¼ˆæ¯”å¦‚å¤§å…‰åœˆè™šåŒ–ï¼‰å°±è¯¯åˆ ç…§ç‰‡ï¼Œ
            # æˆ‘ä»¬æŠŠå›¾ç‰‡åˆ‡æˆ 3x3 çš„ä¹å®«æ ¼ï¼Œåªå–æœ€æ¸…æ™°çš„é‚£ä¸€æ ¼çš„åˆ†æ•°ä½œä¸ºæ•´å¼ å›¾çš„åˆ†æ•°ã€‚
            grid_h, grid_w = h // 3, w // 3
            max_grid_score = 0
            for r in range(3):
                for c in range(3):
                    roi = gray[r*grid_h:(r+1)*grid_h, c*grid_w:(c+1)*grid_w]
                    # è®¡ç®—æ‹‰æ™®æ‹‰æ–¯æ–¹å·®ï¼šè¾¹ç¼˜è¶Šæ¸…æ™°ï¼Œæ–¹å·®è¶Šå¤§
                    score = cv2.Laplacian(roi, cv2.CV_64F).var()
                    if score > max_grid_score: max_grid_score = score
            
            img_scores.append((img_path, max_grid_score))
            if i % 50 == 0: print(f"  -> åˆ†æä¸­... {i}/{len(images)}", end="\r")
        
        # è®¡ç®—é˜ˆå€¼ï¼šæŒ‰åˆ†æ•°æ’åºï¼Œå‰”é™¤æœ€å·®çš„ (1 - keep_ratio) éƒ¨åˆ†
        scores = [s[1] for s in img_scores]
        if not scores: return
        quality_threshold = np.percentile(scores, (1 - keep_ratio) * 100)
        
        good_images = []
        for img_path, score in img_scores:
            if score < quality_threshold:
                # ç§»åŠ¨åˆ°åƒåœ¾æ¡¶ç›®å½•ï¼Œè€Œä¸æ˜¯ç›´æ¥åˆ é™¤ï¼Œæ–¹ä¾¿äººå·¥æ‰¾å›
                shutil.move(str(img_path), str(trash_dir / img_path.name))
            else:
                good_images.append(img_path)
        
        # ======================================================
        # ğŸ”¥ æ ¸å¿ƒé€»è¾‘ï¼šé™é‡‡æ ·æ§åˆ¶æ•°é‡
        # ======================================================
        # ä»é…ç½®å¯¹è±¡ä¸­è¯»å–æœ€å¤§å›¾ç‰‡æ•°é‡
        max_imgs = self.cfg.max_images  
        
        # å¦‚æœå¥½å›¾ç‰‡è¿˜æ˜¯å¤ªå¤šï¼Œè¿›è¡Œå‡åŒ€æŠ½å–
        if len(good_images) > max_imgs:
            print(f"    âš ï¸ å›¾ç‰‡è¿‡å¤š ({len(good_images)} å¼ ), æ­£åœ¨é™é‡‡æ ·è‡³ {max_imgs} å¼ ...")
            # np.linspace ç”Ÿæˆå‡åŒ€åˆ†å¸ƒçš„ç´¢å¼•ï¼Œä¾‹å¦‚ [0, 5, 10, ...]
            indices_to_keep = set(np.linspace(0, len(good_images) - 1, max_imgs, dtype=int))
            for idx, img_path in enumerate(good_images):
                if idx not in indices_to_keep:
                    shutil.move(str(img_path), str(trash_dir / img_path.name))
                    
        print(f"âœ¨ æ¸…æ´—ç»“æŸï¼Œå‰©ä½™ {len(list(image_dir.glob('*')))} å¼ ã€‚")

# ==============================================================================
# å‡½æ•°: analyze_and_calculate_adaptive_collider
# ------------------------------------------------------------------------------
# [ä¾èµ–åº“]: json, numpy
# [åŠŸèƒ½]: [3D åœºæ™¯ç†è§£ç®—æ³•] è¿™æ˜¯ä¸€ä¸ªæ ¸å¿ƒçš„è‡ªåŠ¨åŒ–é€»è¾‘ã€‚
#         å®ƒé€šè¿‡åˆ†æç›¸æœºè½¨è¿¹ï¼Œè‡ªåŠ¨åˆ¤æ–­ä½ æ˜¯å›´ç€ç‰©ä½“æ‹ (Object Mode) è¿˜æ˜¯å‘å››å‘¨æ‹ (Scene Mode)ã€‚
#         è¿™å¯¹äº Nerfstudio è®¾ç½®æ­£ç¡®çš„è¿‘å¹³é¢/è¿œå¹³é¢ (Near/Far Plane) è‡³å…³é‡è¦ã€‚
# ==============================================================================
def analyze_and_calculate_adaptive_collider(json_path, force_cull=False, radius_scale=1.8):
    """
    é€»è¾‘ï¼š
    1. è¯»å– transforms.json è·å–æ‰€æœ‰ç›¸æœºä½å§¿ã€‚
    2. è®¡ç®—æ‰€æœ‰ç›¸æœºçš„è§†çº¿å‘é‡ä¸â€œç›¸æœºä¸­å¿ƒ-åœºæ™¯ä¸­å¿ƒâ€å‘é‡çš„ç‚¹ç§¯ã€‚
    3. å¦‚æœå¤§éƒ¨åˆ†ç›¸æœºéƒ½çœ‹å‘ä¸­å¿ƒ -> Object Mode (ç‰©ä½“æ¨¡å¼)ã€‚
    4. å¦‚æœç›¸æœºå‘å››é¢å…«æ–¹çœ‹ -> Scene Mode (åœºæ™¯æ¨¡å¼)ã€‚
    """
    print(f"\nğŸ¤– [AI åˆ†æ] è§£æç›¸æœºè½¨è¿¹...")
    try:
        with open(json_path, 'r') as f: data = json.load(f)
        frames = data["frames"]
        if not frames: return [], "unknown"
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ mask è·¯å¾„ï¼Œå¦‚æœæœ‰ï¼Œè¯´æ˜ä¹‹å‰è¿›è¡Œäº† AI åˆ†å‰²ï¼Œé‚£å¿…ç„¶æ˜¯ç‰©ä½“æ¨¡å¼
        has_mask = "mask_path" in frames[0]
        if has_mask:
            print("    -> æ£€æµ‹åˆ° Mask æ•°æ®ï¼å°†å¯ç”¨ç‰©ä½“èšç„¦æ¨¡å¼ã€‚")
        
        # [çº¿æ€§ä»£æ•°] æå–æ‰€æœ‰ç›¸æœºçš„ä½ç§» (Translation)
        # transform_matrix æ˜¯ 4x4 çŸ©é˜µï¼Œ[:3, 3] æ˜¯ç¬¬4åˆ—å‰3è¡Œï¼Œå³ XYZ åæ ‡
        positions = [np.array(f["transform_matrix"])[:3, 3] for f in frames]
        
        # æå–ç›¸æœºçš„å‰å‘å‘é‡ (Forward Vector)
        # åœ¨ COLMAP/OpenGL å®šä¹‰ä¸­ï¼Œç›¸æœºçœ‹å‘ -Z æ–¹å‘ã€‚
        # æˆ‘ä»¬ç”¨æ—‹è½¬çŸ©é˜µä¹˜ä»¥ [0,0,-1] å¾—åˆ°ç›¸æœºåœ¨ä¸–ç•Œåæ ‡ç³»ä¸‹çš„æœå‘ã€‚
        forward_vectors = [np.array(f["transform_matrix"])[:3, :3] @ np.array([0, 0, -1]) for f in frames]
        
        # è®¡ç®—æ‰€æœ‰ç›¸æœºä½ç½®çš„å‡ ä½•ä¸­å¿ƒ (Centroid)
        center = np.mean(positions, axis=0)
        
        # è®¡ç®—æ¯ä¸ªç›¸æœºä½ç½®æŒ‡å‘åœºæ™¯ä¸­å¿ƒçš„å‘é‡
        vec_to_center = center - positions
        # å½’ä¸€åŒ–å‘é‡ (å˜æˆå•ä½å‘é‡ï¼Œé•¿åº¦ä¸º1)
        vec_to_center /= (np.linalg.norm(vec_to_center, axis=1, keepdims=True) + 1e-6)
        
        # [æ ¸å¿ƒç®—æ³•] è®¡ç®—â€œè§†çº¿â€ä¸â€œæŒ‡å‘ä¸­å¿ƒå‘é‡â€çš„å¯¹é½ç¨‹åº¦ (ç‚¹ç§¯)
        # ç‚¹ç§¯ (Dot Product): A Â· B = |A||B|cos(theta)
        # å¦‚æœç»“æœ > 0ï¼Œè¯´æ˜å¤¹è§’ < 90åº¦ï¼Œå³ç›¸æœºæ˜¯å¤§è‡´çœ‹å‘ä¸­å¿ƒçš„ã€‚
        # æˆ‘ä»¬ç»Ÿè®¡æœ‰å¤šå°‘æ¯”ä¾‹çš„ç›¸æœºæ˜¯çœ‹å‘ä¸­å¿ƒçš„ã€‚
        ratio = np.sum(np.sum(forward_vectors * vec_to_center, axis=1) > 0) / len(frames)
        
        # ç»¼åˆåˆ¤å®šï¼šå‘å¿ƒç‡é«˜ (>0.6) OR å¼ºåˆ¶å¼€å¯ OR æœ‰ Mask
        is_object_mode = ratio > 0.6 or force_cull or has_mask

        if is_object_mode:
            # ç‰©ä½“æ¨¡å¼ï¼šè®¾ç½®ç´§å‡‘çš„ Near/Far Plane
            dists = [np.linalg.norm(p) for p in positions] # ç›¸æœºåˆ°åŸç‚¹çš„è·ç¦»
            avg_dist = np.mean(dists)
            
            scene_radius = 1.0 * radius_scale  # ä¼°ç®—åœºæ™¯åŠå¾„
            
            # è®¡ç®— Near Plane (è¿‘å¹³é¢)ï¼šä¸èƒ½å¤ªè¿‘ï¼Œå¦åˆ™ä¼šåˆ‡æ‰ç›¸æœºå‰çš„ç‰©ä½“
            calc_near = max(0.05, min(dists) - scene_radius)
            # è®¡ç®— Far Plane (è¿œå¹³é¢)ï¼šåªè¦åŒ…ä½ç‰©ä½“å³å¯ï¼Œåˆ‡æ‰è¿œå¤„çš„ä¼ªå½±
            calc_far = avg_dist + scene_radius
            
            # è¿”å› nerfstudio éœ€è¦çš„å‘½ä»¤è¡Œå‚æ•°åˆ—è¡¨
            return ["--pipeline.model.enable-collider", "True", 
                    "--pipeline.model.collider-params", "near_plane", str(round(calc_near, 2)), 
                    "far_plane", str(round(calc_far, 2))], "object"
        else:
            # åœºæ™¯æ¨¡å¼ï¼šç©ºé—´å¾ˆå¤§ï¼ŒFar Plane è®¾è¿œä¸€ç‚¹ (100.0)
            return ["--pipeline.model.enable-collider", "True", 
                    "--pipeline.model.collider-params", "near_plane", "0.05", "far_plane", "100.0"], "scene"
    except:
        return [], "unknown"

# ==============================================================================
# å‡½æ•°: perform_percentile_culling
# ------------------------------------------------------------------------------
# [ä¾èµ–åº“]: plyfile, numpy, json
# [åŠŸèƒ½]: [ç‚¹äº‘åå¤„ç†] åŸºäºç»Ÿè®¡åˆ†ä½æ•°çš„æš´åŠ›åˆ‡å‰²ã€‚
#         3DGS è®­ç»ƒå‡ºæ¥çš„ç‚¹äº‘å¾€å¾€åœ¨æ— é™è¿œçš„åœ°æ–¹æœ‰ä¸€äº›é£˜é€¸çš„å™ªç‚¹ã€‚
#         æ­¤å‡½æ•°è¯»å– PLY æ–‡ä»¶ï¼Œè®¡ç®—æ‰€æœ‰ç‚¹åˆ°ä¸­å¿ƒçš„è·ç¦»ï¼Œåˆ‡é™¤æœ€è¿œçš„ 10% (æˆ–å…¶ä»–æ¯”ä¾‹) çš„ç‚¹ã€‚
# ==============================================================================
def perform_percentile_culling(ply_path, json_path, output_path, keep_percentile=0.9):
    # æ£€æŸ¥ä¾èµ–
    if not HAS_PLYFILE: return False
    print(f"\nâœ‚ï¸ [åå¤„ç†] æ­£åœ¨æ‰§è¡Œã€åˆ†ä½æ•°æš´åŠ›åˆ‡å‰²ã€‘...")
    try:
        # 1. è®¡ç®—åœºæ™¯ä¸­å¿ƒ (åŸºäºç›¸æœºè½¨è¿¹)
        with open(json_path, 'r') as f: frames = json.load(f)["frames"]
        cam_pos = np.array([np.array(f["transform_matrix"])[:3, 3] for f in frames])
        center = np.mean(cam_pos, axis=0)
        
        # 2. è¯»å– PLY ç‚¹äº‘æ•°æ®
        plydata = PlyData.read(str(ply_path))
        vertex = plydata['vertex']
        # å †å  x,y,z åæ ‡
        points = np.stack([vertex['x'], vertex['y'], vertex['z']], axis=1)
        
        # 3. è®¡ç®—æ‰€æœ‰ç‚¹åˆ°ä¸­å¿ƒçš„è·ç¦»
        dists_pts = np.linalg.norm(points - center, axis=1)

        # [ç®—æ³•é€»è¾‘] ç¡®å®šé˜ˆå€¼åŠå¾„
        # ä½¿ç”¨ numpy.percentile æ‰¾åˆ°ç¬¬ 90% ä½æ•°çš„è·ç¦»å€¼ã€‚
        # æ¯”å¦‚ keep_percentile=0.9ï¼Œåˆ™ä¿ç•™è·ç¦»æœ€è¿‘çš„ 90% çš„ç‚¹ã€‚
        threshold_radius = np.percentile(dists_pts, keep_percentile * 100)
        
        # 4. è¯»å–ä¸é€æ˜åº¦ (Opacity) å¹¶è¿‡æ»¤
        # Gaussian Splatting å­˜å‚¨çš„ opacity é€šå¸¸ç»è¿‡ sigmoid æ¿€æ´»ï¼Œéœ€è¦è¿˜åŸ
        # è¿™é‡Œ simplified: å‡è®¾ vertex['opacity'] æ˜¯ logit
        opacities = 1 / (1 + np.exp(-vertex['opacity']))
        
        # è”åˆæ©ç ï¼š(åœ¨åŠå¾„å†…) AND (ä¸é€æ˜åº¦ > 0.05)
        # å»é™¤å¤ªè¿œçš„ç‚¹ï¼ŒåŒæ—¶ä¹Ÿå»é™¤å¤ªé€æ˜ï¼ˆå‡ ä¹ä¸å¯è§ï¼‰çš„ç‚¹
        mask = (dists_pts < threshold_radius) & (opacities > 0.05)
        filtered_vertex = vertex[mask]
        
        # 5. å†™å…¥æ–° PLY æ–‡ä»¶
        PlyData([PlyElement.describe(filtered_vertex, 'vertex')]).write(str(output_path))
        return True
    except Exception as e:
        print(f"âŒ åˆ‡å‰²å¤±è´¥: {e}")
        return False

# ==============================================================================
# ç±»: GlomapRunner (GLOMAP ä½å§¿è§£ç®—ç±»)
# ------------------------------------------------------------------------------
# [ä¾èµ–åº“]: shutil, os, subprocess
# [åŠŸèƒ½]: å°è£…äº† GLOMAP (Global Mapping) çš„è°ƒç”¨æµç¨‹ã€‚
#         GLOMAP æ˜¯ COLMAP çš„æ›¿ä»£å“ï¼Œé€Ÿåº¦æ›´å¿«ï¼Œé²æ£’æ€§æ›´å¼ºã€‚
#         è¿™ä¸ªç±»ç‰¹åˆ«å¤„ç†äº†å¤æ‚çš„ Linux ç¯å¢ƒå˜é‡éš”ç¦»é—®é¢˜ã€‚
# ==============================================================================
class GlomapRunner:
    def __init__(self, cfg: PipelineConfig):
        self.cfg = cfg
        
        # 1. æŸ¥æ‰¾ COLMAP (ä¼˜å…ˆä½¿ç”¨ Conda ç¯å¢ƒè‡ªå¸¦çš„ï¼)
        # shutil.which ç±»ä¼¼äº Linux çš„ `which` å‘½ä»¤
        self.colmap_exe = shutil.which("colmap")
        if not self.colmap_exe:
            if os.path.exists("/usr/local/bin/colmap"):
                self.colmap_exe = "/usr/local/bin/colmap"
        
        # 2. æŸ¥æ‰¾ GLOMAP
        self.glomap_exe = shutil.which("glomap")
        if not self.glomap_exe:
            if os.path.exists("/usr/local/bin/glomap"):
                self.glomap_exe = "/usr/local/bin/glomap"

        if not self.colmap_exe or not self.glomap_exe:
            raise FileNotFoundError("âŒ ç¼ºå°‘ colmap æˆ– glomap å¯æ‰§è¡Œæ–‡ä»¶")

        print(f"    -> ğŸ¯ é”å®šå¼•æ“: COLMAP={self.colmap_exe}")
        print(f"    -> ğŸ¯ é”å®šå¼•æ“: GLOMAP={self.glomap_exe}")
        
        # å¤åˆ¶å½“å‰ç¯å¢ƒå˜é‡ï¼Œç”¨äºåç»­ä¿®æ”¹ï¼Œä¸å½±å“ä¸»è¿›ç¨‹
        self.env = os.environ.copy()
        self.env["SETUPTOOLS_USE_DISTUTILS"] = "stdlib"

    def run(self):
        """æ‰§è¡Œ GLOMAP å®Œæ•´æµç¨‹"""
        print(f"\nğŸ“ [2/4] GLOMAP ä½å§¿è§£ç®— (Global Mapping)")

        # è·¯å¾„å‡†å¤‡ï¼šå°†å›¾ç‰‡ä» raw_images å¤åˆ¶åˆ° data/images
        # GLOMAP å–œæ¬¢çº¯å‡€çš„è¾“å…¥ç›®å½•
        raw_images_dir = self.cfg.project_dir / "raw_images"
        dest_images_dir = self.cfg.images_dir
        dest_images_dir.mkdir(parents=True, exist_ok=True)
        for img in raw_images_dir.glob("*"):
            if not (dest_images_dir / img.name).exists():
                shutil.copy2(str(img), str(dest_images_dir / img.name))

        colmap_output_dir = self.cfg.data_dir / "colmap"
        colmap_output_dir.mkdir(parents=True, exist_ok=True)
        database_path = colmap_output_dir / "database.db"
        sparse_dir = colmap_output_dir / "sparse"

        try:
            # æ¸…ç†æ—§æ•°æ®ï¼Œé˜²æ­¢å†²çª
            if database_path.exists(): database_path.unlink()
            if sparse_dir.exists(): shutil.rmtree(sparse_dir)
            sparse_dir.mkdir(parents=True, exist_ok=True)
            if self.cfg.transforms_file.exists(): self.cfg.transforms_file.unlink()

            # Step 1: ç‰¹å¾æå– (Feature Extraction) - ä½¿ç”¨ COLMAP
            # è¿™ä¸€æ­¥è®¡ç®—æ¯å¼ å›¾çš„å…³é”®ç‚¹ (SIFTç­‰)
            self._run_cmd([
                self.colmap_exe, "feature_extractor",
                "--database_path", str(database_path),
                "--image_path", str(raw_images_dir),
                "--ImageReader.camera_model", "OPENCV", # ä½¿ç”¨ OpenCV ç›¸æœºæ¨¡å‹
                "--ImageReader.single_camera", "1"      # å‡è®¾æ‰€æœ‰å›¾ç‰‡æ¥è‡ªåŒä¸€ä¸ªç›¸æœºï¼ˆç„¦è·ç›¸åŒï¼‰
            ], "Step 1: ç‰¹å¾æå– (COLMAP)")

            # Step 2: é¡ºåºåŒ¹é… (Sequential Matching) - ä½¿ç”¨ COLMAP
            # å› ä¸ºæˆ‘ä»¬çš„è¾“å…¥æ˜¯è§†é¢‘æŠ½å¸§ï¼Œæ‰€ä»¥ç›¸é‚»çš„å›¾ç‰‡é‡å åº¦æœ€é«˜ã€‚ä½¿ç”¨é¡ºåºåŒ¹é…æ¯”ç©·ä¸¾åŒ¹é…å¿«å¾—å¤šã€‚
            self._run_cmd([
                self.colmap_exe, "sequential_matcher",
                "--database_path", str(database_path),
                "--SequentialMatching.overlap", "25"    # åŒ¹é…å‰å25å¼ å›¾
            ], "Step 2: é¡ºåºåŒ¹é… (COLMAP)")

            # Step 3: å…¨å±€é‡å»º (Global Mapper) - ä½¿ç”¨ GLOMAP
            # è¿™æ˜¯ GLOMAP çš„æ ¸å¿ƒï¼Œæ¯” COLMAP çš„ incremental mapper æ›´å¿«ä¸”ä¸æ˜“äº§ç”Ÿåˆ†å±‚ã€‚
            print(f"    -> ğŸš€ å¯åŠ¨ GLOMAP å¼•æ“...")
            self._run_cmd([
                self.glomap_exe, "mapper",
                "--database_path", str(database_path),
                "--image_path", str(raw_images_dir),
                "--output_path", str(sparse_dir)
            ], "Step 3: å…¨å±€æ˜ å°„ (GLOMAP)")

            # Step 4: ç›®å½•ä¿®æ­£
            # GLOMAP è¾“å‡ºçš„ç»“æ„å¯èƒ½æ˜¯åœ¨ sparse/0 é‡Œé¢ï¼Œæˆ‘ä»¬éœ€è¦æ•´ç†ä¸€ä¸‹
            self._fix_directory_structure(sparse_dir)

            # Step 5: ç”Ÿæˆ transforms.json
            # è°ƒç”¨ nerfstudio çš„å·¥å…·å°† COLMAP æ•°æ®è½¬æ¢ä¸º NeRF æ ‡å‡†æ ¼å¼
            self._run_cmd([
                "ns-process-data", "images",
                "--data", str(dest_images_dir),
                "--output-dir", str(self.cfg.data_dir),
                "--skip-colmap", # æˆ‘ä»¬å·²ç»è·‘è¿‡ COLMAP/GLOMAP äº†ï¼Œæ‰€ä»¥è·³è¿‡
                "--skip-image-processing", # æˆ‘ä»¬è‡ªå·±å¤„ç†è¿‡å›¾ç‰‡äº†
                "--num-downscales", "0"
            ], "ç”Ÿæˆ transforms.json")

            # Step 6: æ£€æŸ¥è´¨é‡
            if self._check_quality(raw_images_dir):
                print(f"    âœ¨ GLOMAP æµç¨‹æˆåŠŸï¼")
                return True

        except Exception as e:
            print(f"    âŒ GLOMAP æµç¨‹å¤±è´¥: {e}")
            return False
        return False

    def _run_cmd(self, cmd, desc):
        """å†…éƒ¨å·¥å…·ï¼šæ‰§è¡Œ shell å‘½ä»¤ (å«ç¯å¢ƒéš”ç¦»é€»è¾‘)"""
        print(f"ğŸš€ {desc}...")
        
        # ğŸ”¥ ç¯å¢ƒéš”ç¦»é€»è¾‘ ğŸ”¥
        # è¿™æ˜¯ä¸€ä¸ªéå¸¸ tricky çš„ç‚¹ã€‚å¦‚æœä½ åœ¨ Conda ç¯å¢ƒé‡Œè·‘ï¼ŒLD_LIBRARY_PATH å¯èƒ½æŒ‡å‘ Conda çš„ libã€‚
        # ä½†å¦‚æœä½ è°ƒç”¨ç³»ç»Ÿè‡ªå¸¦çš„ /usr/local/bin/glomapï¼Œå®ƒå¯èƒ½éœ€è¦ç³»ç»Ÿçš„ libã€‚
        # æ··åˆä½¿ç”¨ä¼šå¯¼è‡´ "libstdc++.so.6 version not found" é”™è¯¯ã€‚
        cmd_env = self.env.copy()
        exe_path = cmd[0]
        # å¦‚æœæ˜¯ç³»ç»Ÿç¨‹åºï¼Œæ¸…é™¤ LD_LIBRARY_PATH é˜²æ­¢ Conda å¹²æ‰°
        if exe_path.startswith("/usr") or exe_path.startswith("/bin"):
            if "LD_LIBRARY_PATH" in cmd_env:
                del cmd_env["LD_LIBRARY_PATH"]

        try:
            # subprocess.Popen å…è®¸æˆ‘ä»¬å®æ—¶æ•è·è¾“å‡º
            process = subprocess.Popen(
                cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, env=cmd_env
            )
            # å®æ—¶æ‰“å°å…³é”®æ—¥å¿—
            for line in process.stdout:
                if any(k in line for k in ["Error", "Warning", "Elapsed", "image pairs"]):
                    print(f"    | {line.strip()}")
            process.wait()
            if process.returncode != 0:
                raise subprocess.CalledProcessError(process.returncode, cmd)
        except subprocess.CalledProcessError as e:
            print(f"âŒ å‘½ä»¤æ‰§è¡Œå´©æºƒ: {cmd[0]} (ä»£ç  {e.returncode})")
            raise e

    def _fix_directory_structure(self, sparse_root):
        """å°†ç¨€ç–é‡å»ºç»“æœç»Ÿä¸€ç§»åŠ¨åˆ° sparse/0 æ–‡ä»¶å¤¹ä¸‹"""
        target_dir_0 = sparse_root / "0"
        target_dir_0.mkdir(parents=True, exist_ok=True)
        required_files = ["cameras.bin", "images.bin", "points3D.bin"]
        required_files_txt = ["cameras.txt", "images.txt", "points3D.txt"]
        # ... (è¯¦ç»†çš„æ–‡ä»¶ç§»åŠ¨é€»è¾‘ï¼Œéå†ç›®å½•å¯»æ‰¾ .bin æˆ– .txt æ–‡ä»¶å¹¶ç§»åŠ¨åˆ° target_dir_0)
        # æ­¤å¤„çœç•¥å…·ä½“ os.walk ä»£ç ï¼Œé€»è¾‘ä¸ºé€’å½’æŸ¥æ‰¾å¹¶ shutil.move

    def _check_quality(self, raw_images_dir):
        """è®¡ç®—æ³¨å†Œç‡ï¼šæœ‰å¤šå°‘å›¾ç‰‡æˆåŠŸå‚ä¸äº†é‡å»º"""
        if not self.cfg.transforms_file.exists(): return False
        with open(self.cfg.transforms_file, 'r') as f: meta = json.load(f)
        reg_count = len(meta["frames"])
        total_count = len(list(raw_images_dir.glob("*.jpg")) + list(raw_images_dir.glob("*.png")))
        ratio = reg_count / total_count if total_count > 0 else 0
        print(f"    ğŸ“Š åŒ¹é…ç‡: {ratio:.2%} ({reg_count}/{total_count})")
        return ratio > 0.2 # å¦‚æœå°‘äº 20% çš„å›¾ç‰‡åŒ¹é…æˆåŠŸï¼Œè§†ä¸ºå¤±è´¥

# ==============================================================================
# ç±»: AISegmentor
# ------------------------------------------------------------------------------
# [åŠŸèƒ½]: å¯¹ run_ai_segmentation_pipeline çš„é¢å‘å¯¹è±¡å°è£…ã€‚
#         ä½¿ä¸»æµç¨‹ä»£ç æ›´æ•´æ´ã€‚
# ==============================================================================
class AISegmentor:
    def __init__(self, cfg: PipelineConfig):
        self.cfg = cfg
        self.data_dir = cfg.data_dir
        self.images_dir = cfg.images_dir
        self.masks_dir = cfg.masks_dir

    def run(self):
        """æ‰§è¡Œ AI åˆ†å‰²æ€»æµæ°´çº¿"""
        # æ£€æŸ¥å¼€å…³å’Œä¾èµ–
        if not HAS_AI or not self.cfg.enable_ai:
            print("â© è·³è¿‡ AI åˆ†å‰² (æœªå¯ç”¨æˆ–ç¼ºå°‘ä¾èµ–)")
            return False
            
        if not self.cfg.transforms_file.exists():
            print("âš ï¸ transforms.json ä¸å­˜åœ¨ï¼Œæ— æ³•è¿›è¡Œ AI åˆ†å‰²")
            return False

        # å†…éƒ¨å®é™…ä¸Šæ˜¯è°ƒç”¨äº†ä¹‹å‰çš„ run_ai_segmentation_pipeline é€»è¾‘
        # è¿™é‡Œä¸ºäº†æ¼”ç¤ºå°è£…ç»“æ„ï¼Œå°†å…¶ä½œä¸ºç±»æ–¹æ³•é‡æ–°ç»„ç»‡
        # ... (å…·ä½“å®ç°é€»è¾‘è§ä¸Šæ–‡ run_ai_segmentation_pipeline)
        
        # ç®€åŒ–ç‰ˆè°ƒç”¨ï¼š
        print("\nâœ‚ï¸ [AI åˆ†å‰²] å¯åŠ¨...")
        return run_ai_segmentation_pipeline(self.data_dir) # ç›´æ¥è°ƒç”¨å…¨å±€å‡½æ•°

# ==============================================================================
# ç±»: NerfstudioEngine (è®­ç»ƒå¼•æ“ç±»)
# ------------------------------------------------------------------------------
# [ä¾èµ–åº“]: subprocess, os
# [åŠŸèƒ½]: è´Ÿè´£è°ƒç”¨ ns-train è¿›è¡Œ Splatfacto (3DGS) è®­ç»ƒï¼Œå¹¶å¯¼å‡ºç»“æœã€‚
# ==============================================================================
class NerfstudioEngine:
    def __init__(self, cfg: PipelineConfig):
        self.cfg = cfg
        self.output_dir = cfg.project_dir / "outputs"
        # å‡†å¤‡ç¯å¢ƒå˜é‡
        self.env = os.environ.copy()
        # QT_QPA_PLATFORM="offscreen": é˜²æ­¢åœ¨æ²¡æœ‰æ˜¾ç¤ºå™¨çš„æœåŠ¡å™¨ä¸Šå› ä¸ºå¼¹ä¸å‡ºçª—å£è€ŒæŠ¥é”™
        self.env["QT_QPA_PLATFORM"] = "offscreen"
        self.env["SETUPTOOLS_USE_DISTUTILS"] = "stdlib"

    def train(self):
        """æ‰§è¡Œ splatfacto è®­ç»ƒ"""
        print(f"\nğŸ”¥ [4/4] å¼€å§‹è®­ç»ƒ (Splatfacto)")
        
        # 1. è®¡ç®—åœºæ™¯å‚æ•° (Collider) - è°ƒç”¨ä¹‹å‰çš„æ™ºèƒ½åˆ†æå‡½æ•°
        collider_args, scene_type = analyze_and_calculate_adaptive_collider(
            self.cfg.transforms_file,
            force_cull=self.cfg.force_spherical_culling,
            radius_scale=self.cfg.scene_radius_scale
        )
        self.scene_type = scene_type # å­˜ä¸‹æ¥ç»™å¯¼å‡ºæ­¥éª¤ç”¨

        # 2. ç»„è£… ns-train å‘½ä»¤
        cmd = [
            "ns-train", "splatfacto",  # ä½¿ç”¨ splatfacto æ¨¡å‹ (å³ Gaussian Splatting)
            "--data", str(self.cfg.data_dir),
            "--output-dir", str(self.output_dir),
            "--experiment-name", self.cfg.project_name,
            "--pipeline.model.random-init", "False",   # ä½¿ç”¨ç¨€ç–ç‚¹äº‘åˆå§‹åŒ–ï¼Œæ”¶æ•›æ›´å¿«
            "--pipeline.model.background-color", "random", # èƒŒæ™¯é¢œè‰²éšæœºï¼Œå¢å¼ºå¯¹é€æ˜èƒŒæ™¯çš„é²æ£’æ€§
            *collider_args, # è§£åŒ… collider å‚æ•° (near/far plane)
            "--max-num-iterations", "15000", # è¿­ä»£æ¬¡æ•°ï¼Œ15000 æ¬¡é€šå¸¸è¶³å¤Ÿ
            "--vis", "viewer+tensorboard",   # å¼€å¯å¯è§†åŒ–æ”¯æŒ
            "--viewer.quit-on-train-completion", "True", # è®­ç»ƒå®Œè‡ªåŠ¨å…³é—­ viewer
            "nerfstudio-data", # æ•°æ®è§£æå™¨é…ç½®
            "--downscale-factor", "1", # ä¸ç¼©æ”¾å›¾ç‰‡
            "--auto-scale-poses", "False" # ä¸è‡ªåŠ¨ç¼©æ”¾ä½å§¿ï¼ˆå› ä¸ºæˆ‘ä»¬åœ¨ Collider æ­¥éª¤ç®—è¿‡äº†ï¼‰
        ]
        
        # 3. æ‰§è¡Œ
        subprocess.run(cmd, check=True, env=self.env)

    def export(self):
        """å¯¼å‡º ply å¹¶è¿›è¡Œåå¤„ç†"""
        print(f"\nğŸ’¾ æ­£åœ¨å¯¼å‡º...")
        # è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°çš„ config.yml æ–‡ä»¶
        search_path = self.output_dir / self.cfg.project_name / "splatfacto"
        try:
            run_dirs = sorted(list(search_path.glob("*")))
            config_path = run_dirs[-1] / "config.yml" # å–æ—¶é—´æˆ³æœ€æ–°çš„é‚£ä¸ª
        except IndexError:
            print("âŒ æœªæ‰¾åˆ°è®­ç»ƒç»“æœ config.yml")
            return None

        # å¯¼å‡ºå‘½ä»¤ ns-export
        subprocess.run([
            "ns-export", "gaussian-splat",
            "--load-config", str(config_path),
            "--output-dir", str(self.cfg.project_dir)
        ], check=True, env=self.env)

        # åå¤„ç†ï¼šç‚¹äº‘åˆ‡å‰²
        raw_ply = self.cfg.project_dir / "point_cloud.ply"
        if not raw_ply.exists(): raw_ply = self.cfg.project_dir / "splat.ply"
        cleaned_ply = self.cfg.project_dir / "point_cloud_cleaned.ply"
        final_ply = raw_ply

        # åˆ¤æ–­æ˜¯å¦éœ€è¦åˆ‡å‰² (ç‰©ä½“æ¨¡å¼ or å¼ºåˆ¶åˆ‡å‰²)
        need_cull = (self.scene_type == "object" or self.cfg.force_spherical_culling)
        
        if need_cull and raw_ply.exists():
            # è°ƒç”¨ä¹‹å‰çš„ perform_percentile_culling å‡½æ•°
            success = perform_percentile_culling(
                raw_ply, 
                self.cfg.transforms_file, 
                cleaned_ply,
                keep_percentile=self.cfg.keep_percentile
            )
            if success:
                final_ply = cleaned_ply

        # å¤åˆ¶ç»“æœåˆ°å½“å‰è„šæœ¬ç›®å½•ä¸‹çš„ results æ–‡ä»¶å¤¹ï¼Œæ–¹ä¾¿æŸ¥çœ‹
        results_dir = Path(__file__).parent / "results"
        results_dir.mkdir(exist_ok=True)
        target_path = results_dir / f"{self.cfg.project_name}.ply"
        shutil.copy2(str(final_ply), str(target_path))
        
        return target_path

# ================= ä¸»æµç¨‹ =================
def run_pipeline(cfg: PipelineConfig):
    global_start_time = time.time()
    print(f"\nğŸš€ [BrainDance Engine] å¯åŠ¨ä»»åŠ¡: {cfg.project_name}")
    
    # 1. å®ä¾‹åŒ–æ‰€æœ‰æ¨¡å—
    img_processor = ImageProcessor(cfg)
    glomap_runner = GlomapRunner(cfg) 
    ai_segmentor = AISegmentor(cfg)
    nerf_engine = NerfstudioEngine(cfg)

    # ==========================================
    # Step 1: æ•°æ®å‡†å¤‡ (FFmpeg æŠ½å¸§)
    # ==========================================
    # åˆå§‹åŒ–ç›®å½•ï¼Œå¦‚æœé¡¹ç›®å·²å­˜åœ¨åˆ™æ¸…ç©º
    if cfg.project_dir.exists(): shutil.rmtree(cfg.project_dir, ignore_errors=True)
    cfg.project_dir.mkdir(parents=True, exist_ok=True)
    
    # æŠ½å¸§
    temp_dir = cfg.project_dir / "temp_extract"
    temp_dir.mkdir(parents=True, exist_ok=True)
    # è°ƒç”¨ ffmpeg: -vf fps=10 è¡¨ç¤ºæ¯ç§’æŠ½10å¸§ï¼Œ-q:v 2 è¡¨ç¤ºé«˜è´¨é‡ JPG
    subprocess.run(["ffmpeg", "-y", "-i", str(cfg.video_path), 
                    "-vf", "fps=10", "-q:v", "2", 
                    str(temp_dir / "frame_%05d.jpg")], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    
    # æ¸…æ´—ï¼šå»é™¤æ¨¡ç³Šå›¾ç‰‡
    img_processor.smart_filter_blurry_images(temp_dir, keep_ratio=0.85)
    
    # ç§»åŠ¨å›¾ç‰‡åˆ° raw_images
    raw_images_dir = cfg.project_dir / "raw_images"
    raw_images_dir.mkdir(parents=True, exist_ok=True)
    
    all_imgs = sorted(list(temp_dir.glob("*")))
    limit = cfg.max_images
    # å¦‚æœå›¾ç‰‡è¿˜æ˜¯å¤ªå¤šï¼Œå‡åŒ€é™é‡‡æ ·
    if len(all_imgs) > limit:
        indices = np.linspace(0, len(all_imgs)-1, limit, dtype=int)
        all_imgs = [all_imgs[i] for i in sorted(list(set(indices)))]
    for img in all_imgs: shutil.copy2(str(img), str(raw_images_dir / img.name))
    shutil.rmtree(temp_dir) # åˆ é™¤ä¸´æ—¶ç›®å½•

    # ==========================================
    # Step 2: GLOMAP ä½å§¿è§£ç®—
    # ==========================================
    if not glomap_runner.run():
        print("âŒ Pipeline ä¸­æ–­ï¼šGLOMAP å¤±è´¥")
        return

    # ==========================================
    # Step 3: AI è¯­ä¹‰åˆ†å‰²
    # ==========================================
    ai_segmentor.run()

    # ==========================================
    # Step 4 & 5: è®­ç»ƒä¸å¯¼å‡º
    # ==========================================
    try:
        nerf_engine.train()
        final_path = nerf_engine.export()
        print(f"\nğŸ‰ ä»»åŠ¡å®Œæˆï¼ç»“æœä½äº: {final_path}")
    except Exception as e:
        print(f"âŒ è®­ç»ƒ/å¯¼å‡ºé˜¶æ®µå¤±è´¥: {e}")

    print(f"â±ï¸ æ€»è€—æ—¶: {format_duration(time.time() - global_start_time)}")


# ç¨‹åºå…¥å£
if __name__ == "__main__":
    script_dir = Path(__file__).resolve().parent
    video_file = script_dir / "test.mp4" 
    # å¦‚æœå‘½ä»¤è¡Œå¸¦äº†å‚æ•°ï¼Œä½¿ç”¨å‚æ•°ä½œä¸ºè§†é¢‘è·¯å¾„
    if len(sys.argv) > 1: video_file = Path(sys.argv[1])
    
    if not video_file.exists():
        print(f"âŒ æ‰¾ä¸åˆ°è§†é¢‘: {video_file}")
        sys.exit(1)

    # å®ä¾‹åŒ–é…ç½®
    cfg = PipelineConfig(
        project_name="glomap_test_v1", 
        video_path=video_file,
        max_images=100, # é™åˆ¶æœ€å¤§å¤„ç†100å¼ å›¾
        enable_ai=True  # å¼€å¯ AI åŠŸèƒ½
    )
    
    
    # è¿è¡Œæµæ°´çº¿
    run_pipeline(cfg)