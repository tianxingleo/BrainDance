import subprocess
import sys
import shutil
import os
import time
import base64
import json
import logging
import numpy as np
import cv2
from pathlib import Path

# ğŸ”¥ã€ç»æ€ã€‘å¼ºåˆ¶å°†ç¼–è¯‘å¥½çš„ç³»ç»Ÿçº§ colmap è·¯å¾„æåˆ°æœ€å‰é¢
# è¿™æ ·ç³»ç»Ÿæ‰¾ colmap æ—¶ï¼Œç¬¬ä¸€ä¸ªçœ‹åˆ°çš„å°±æ˜¯ /usr/local/bin é‡Œçš„é‚£ä¸ªå¥½ç‰ˆæœ¬
sys_path = "/usr/local/bin"
current_path = os.environ.get("PATH", "")

if sys_path not in current_path.split(os.pathsep)[0]: # å¦‚æœä¸åœ¨ç¬¬ä¸€ä½
    print(f"âš¡ [ç¯å¢ƒä¿®æ­£] å¼ºåˆ¶è®¾ç½® PATH ä¼˜å…ˆçº§: {sys_path} -> Priority High")
    os.environ["PATH"] = f"{sys_path}{os.pathsep}{current_path}"

# éªŒè¯ä¸€ä¸‹
colmap_loc = shutil.which("colmap")
print(f"ğŸ§ [è‡ªæ£€] å½“å‰è„šæœ¬ä½¿ç”¨çš„ COLMAP è·¯å¾„: {colmap_loc}")

# ================= ğŸ”§ ç”¨æˆ·é…ç½®åŒºåŸŸ =================
# 1. OpenAI API Key (ç”¨äº GPT-4o è¯­ä¹‰åˆ†æ)
# å¦‚æœç•™ç©ºï¼Œå°†è‡ªåŠ¨é™çº§ä¸ºä½¿ç”¨â€œå‡ ä½•ç®—æ³•â€è¿›è¡Œåˆ†æï¼Œæ— éœ€è”ç½‘
OPENAI_API_KEY = "sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx" 

# 2. Linux å·¥ä½œåŒºè·¯å¾„
LINUX_WORK_ROOT = Path.home() / "braindance_workspace"

# 3. ç¡¬ä»¶é…ç½® (æ‚¨æœ‰ 5070ï¼Œç›´æ¥æ‹‰æ»¡)
# YOLO-World æ¨¡å‹: yolov8x-worldv2.pt (æœ€å¼ºç‰ˆ)
# SAM æ¨¡å‹: sam_l.pt (Largeç‰ˆï¼Œç²¾åº¦æœ€é«˜)
MODEL_YOLO = 'yolov8x-worldv2.pt'
MODEL_SAM = 'sam_l.pt'
MAX_IMAGES = 200 # ğŸ”¥ å…¨å±€æœ€å¤§å›¾ç‰‡æ•°é‡é™åˆ¶

# ================= ğŸ“¦ åº“å¯¼å…¥ä¸åˆå§‹åŒ– =================
logging.getLogger('nerfstudio').setLevel(logging.ERROR)
logging.getLogger('ultralytics').setLevel(logging.ERROR)

try:
    from openai import OpenAI
    has_openai = True if OPENAI_API_KEY and OPENAI_API_KEY.startswith("sk-") else False
except ImportError:
    has_openai = False
    print("âš ï¸ æœªå®‰è£… openai åº“ï¼Œå°†ä½¿ç”¨æœ¬åœ°å‡ ä½•åˆ†ææ¨¡å¼ã€‚")

try:
    from ultralytics import YOLOWorld, SAM
    has_ultralytics = True
except ImportError:
    has_ultralytics = False
    print("âš ï¸ æœªå®‰è£… ultralytics åº“ï¼Œå°†è·³è¿‡æœ¬åœ° AI æŠ å›¾ã€‚")

# ================= ğŸ§  æ ¸å¿ƒ AI æ¨¡å— =================

class SmartMasker:
    """
    æœ¬åœ° AI å¼•æ“ï¼šç»“åˆ YOLO-World (å¬è§‰/è§†è§‰) + SAM (è§¦è§‰/åˆ†å‰²)
    """
    def __init__(self):
        if not has_ultralytics: return
        print(f"\nğŸ¦¾ [æœ¬åœ° AI å¼•æ“] æ­£åœ¨åŠ è½½å¤§æ¨¡å‹ (RTX 5070 ç®—åŠ›å…¨å¼€)...")
        print(f"    -> åŠ è½½ {MODEL_YOLO} (è¯­ä¹‰è¯†åˆ«)...")
        self.detector = YOLOWorld(MODEL_YOLO)
        print(f"    -> åŠ è½½ {MODEL_SAM} (åƒç´ åˆ†å‰²)...")
        self.segmentor = SAM(MODEL_SAM)
        print("âœ… AI å¼•æ“å°±ç»ªã€‚")

    def generate_mask(self, image_path, prompt):
        """è¾“å…¥å›¾ç‰‡å’Œæç¤ºè¯ï¼Œè¿”å›è’™ç‰ˆ"""
        try:
            # 1. YOLO-World: å¯»æ‰¾ç‰©ä½“æ¡†
            self.detector.set_classes([prompt])
            det_results = self.detector.predict(image_path, conf=0.15, verbose=False)
            
            if len(det_results[0].boxes) == 0:
                return None # æ²¡æ‰¾åˆ°ç‰©ä½“
            
            # 2. SAM: æ ¹æ®æ¡†ç”Ÿæˆè’™ç‰ˆ
            bboxes = det_results[0].boxes.xyxy
            sam_results = self.segmentor(image_path, bboxes=bboxes, verbose=False)
            
            if len(sam_results[0].masks) == 0:
                return None

            # 3. åˆå¹¶è’™ç‰ˆ
            final_mask = np.zeros(sam_results[0].orig_shape[:2], dtype=np.uint8)
            masks = sam_results[0].masks.data.cpu().numpy()
            
            for mask in masks:
                mask_uint8 = (mask * 255).astype(np.uint8)
                if mask_uint8.shape != final_mask.shape:
                     mask_uint8 = cv2.resize(mask_uint8, (final_mask.shape[1], final_mask.shape[0]))
                final_mask = cv2.bitwise_or(final_mask, mask_uint8)
                
            return final_mask
        except Exception as e:
            print(f"âŒ åˆ†å‰²é”™è¯¯: {e}")
            return None

def analyze_with_gpt4o(images_dir):
    """
    ä½¿ç”¨ GPT-4o å¤šæ¨¡æ€å¤§æ¨¡å‹åˆ†æåœºæ™¯
    """
    if not has_openai: return None

    print(f"\nğŸ§  [GPT-4o] æ­£åœ¨ä¸Šä¼ å…³é”®å¸§è¿›è¡Œè¯­ä¹‰ç†è§£...")
    client = OpenAI(api_key=OPENAI_API_KEY)
    
    # å‡åŒ€æŠ½å– 6 å¼ å›¾
    all_imgs = sorted(list(images_dir.glob("*.jpg")))
    if not all_imgs: return None
    step = max(1, len(all_imgs) // 6)
    sampled_imgs = all_imgs[::step][:6]

    content = [{"type": "text", "text": "Analyzing frames for 3D Gaussian Splatting training."}]
    
    for img_path in sampled_imgs:
        with open(img_path, "rb") as f:
            b64 = base64.b64encode(f.read()).decode('utf-8')
        content.append({
            "type": "image_url",
            "image_url": {"url": f"data:image/jpeg;base64,{b64}", "detail": "low"}
        })

    prompt = """
    Analyze these images and output a JSON with:
    1. "type": "object" (if focusing on specific items like a toy, shoe, person) or "scene" (room, street, large area).
    2. "subject": If "object", give a short English specific description for detection (e.g. "red nike shoes", "anime figure"). If "scene", use "none".
    3. "masking_needed": Boolean. True if it's an object with messy background. False if it's a scene OR object with white/clean background.
    """
    content.append({"type": "text", "text": prompt})

    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": content}],
            response_format={"type": "json_object"}
        )
        result = json.loads(response.choices[0].message.content)
        print(f"ğŸ¤– GPT æ´å¯Ÿ: ç±»å‹=[{result['type']}] | ä¸»ä½“=[{result['subject']}] | éœ€æŠ å›¾=[{result['masking_needed']}]")
        return result
    except Exception as e:
        print(f"âš ï¸ GPT åˆ†æå¤±è´¥: {e}")
        return None

def analyze_scene_geometry(json_path):
    """
    (å¤‡ç”¨æ–¹æ¡ˆ) å‡ ä½•åˆ†æç®—æ³•ï¼Œå½“ GPT ä¸å¯ç”¨æ—¶ä½¿ç”¨
    """
    print(f"ğŸ“ [å‡ ä½•åˆ†æ] GPT æœªå¯ç”¨ï¼Œæ­£åœ¨è®¡ç®—ç›¸æœºè½¨è¿¹èšåˆåº¦...")
    try:
        with open(json_path, 'r') as f: frames = json.load(f)["frames"]
        if not frames: return "scene", False, "none"

        positions, forwards = [], []
        for f in frames:
            c2w = np.array(f["transform_matrix"])
            positions.append(c2w[:3, 3])
            forwards.append(c2w[:3, :3] @ np.array([0, 0, -1]))
            
        center = np.mean(positions, axis=0)
        vecs = center - np.array(positions)
        vecs /= (np.linalg.norm(vecs, axis=1, keepdims=True) + 1e-6)
        
        # è®¡ç®—è§†çº¿èšåˆåº¦
        ratio = np.sum(np.sum(np.array(forwards) * vecs, axis=1) > 0) / len(frames)
        
        if ratio > 0.6:
            print(f"    -> èšåˆåº¦ {ratio:.2f} (>0.6)ï¼Œåˆ¤å®šä¸ºç‰©ä½“æ¨¡å¼ã€‚")
            return "object", True, "object" # å‡ ä½•æ¨¡å¼ä¸‹é»˜è®¤å¼€å¯æŠ å›¾ï¼Œé€šç”¨æç¤ºè¯
        else:
            print(f"    -> èšåˆåº¦ {ratio:.2f} (<0.6)ï¼Œåˆ¤å®šä¸ºåœºæ™¯æ¨¡å¼ã€‚")
            return "scene", False, "none"
    except:
        return "scene", False, "none"


# ================= ğŸš€ ä¸»æµç¨‹ =================

def run_pipeline(video_path, project_name):
    print(f"\nğŸš€ [BrainDance Engine 5.0] å¯åŠ¨ä»»åŠ¡: {project_name}")
    print(f"âš¡ ç¡¬ä»¶åŠ é€Ÿ: Intel 14600KF + RTX 5070 | AI æ ¸å¿ƒ: GPT-4o + YOLO-World + SAM")
    
    # 1. è·¯å¾„è§£æ
    video_src = Path(video_path).resolve()
    work_dir = LINUX_WORK_ROOT / project_name
    data_dir = work_dir / "data"
    output_dir = work_dir / "outputs"
    transforms_file = data_dir / "transforms.json"
    env = os.environ.copy()
    env["QT_QPA_PLATFORM"] = "offscreen" 

    # ================= [Step 1] æ•°æ®å¤„ç† (æ ‡å‡†æµç¨‹) =================
    if transforms_file.exists():
        print(f"\nâ© [æ–­ç‚¹ç»­ä¼ ] æ£€æµ‹åˆ° COLMAP æ•°æ®")
    else:
        print(f"ğŸ†• [æ–°ä»»åŠ¡] åˆå§‹åŒ–å·¥ä½œåŒº...")
        if work_dir.exists(): shutil.rmtree(work_dir)
        work_dir.mkdir(parents=True)
        data_dir.mkdir(parents=True)
        shutil.copy(str(video_src), str(work_dir / video_src.name))

        print(f"\nğŸ¥ [1/3] è§†é¢‘å¤„ç† (COLMAP)")
        
        # 1. å®šä¹‰ä¸¤ä¸ªéš”ç¦»åŒºåŸŸ
        # temp_dir: å­˜æ”¾ ffmpeg åŸå§‹äº§ç‰©
        temp_dir = work_dir / "temp_extract"
        temp_dir.mkdir(parents=True, exist_ok=True)
        
        img_dir = data_dir / "images"
        img_dir.mkdir(parents=True, exist_ok=True)
        
        # 2. FFmpeg æŠ½å¸§ (è¾“å‡ºåˆ°ä¸´æ—¶ç›®å½•)
        print(f"    -> æ­£åœ¨æŠ½å¸§åˆ°ä¸´æ—¶ç›®å½•...")
        subprocess.run([
            "ffmpeg", "-y", "-i", str(work_dir / video_src.name), 
            "-vf", "scale=1920:-1,fps=4", "-q:v", "2", 
            str(temp_dir / "frame_%05d.jpg")
        ], check=True) 
        
        # 3. ã€å…³é”®æ­¥éª¤ã€‘æ•°é‡é™åˆ¶ä¸è¿ç§» (ä» temp -> images)
        print("    -> æ­£åœ¨æ‰§è¡Œã€æ•°é‡é™åˆ¶ä¸è¿ç§»ã€‘...")
        
        # è¯»å–æ‰€æœ‰å›¾ç‰‡
        all_candidates = sorted(list(temp_dir.glob("*.jpg")) + list(temp_dir.glob("*.png")))
        total_candidates = len(all_candidates)
        
        final_images_list = []
        
        if total_candidates > MAX_IMAGES:
            print(f"    âš ï¸ å›¾ç‰‡è¿‡å¤š ({total_candidates}), æ­£åœ¨å‡åŒ€é€‰å– {MAX_IMAGES} å¼ ...")
            # å‡åŒ€é‡‡æ ·ç´¢å¼•
            indices = np.linspace(0, total_candidates - 1, MAX_IMAGES, dtype=int)
            # ä½¿ç”¨é›†åˆå»é‡
            indices = sorted(list(set(indices)))
            
            for idx in indices:
                final_images_list.append(all_candidates[idx])
        else:
            print(f"    âœ… å›¾ç‰‡æ•°é‡ ({total_candidates}) æœªè¶…æ ‡ï¼Œå…¨éƒ¨ä¿ç•™ã€‚")
            final_images_list = all_candidates

        # æ‰§è¡Œå¤åˆ¶
        for img_path in final_images_list:
            shutil.copy2(str(img_path), str(img_dir / img_path.name))
            
        print(f"    âœ… å·²å°† {len(final_images_list)} å¼ å¹²å‡€å›¾ç‰‡ç§»å…¥ COLMAP ä¸“ç”¨ç›®å½•ã€‚")
        print(f"    ğŸ§¹ æ­£åœ¨æ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
        shutil.rmtree(temp_dir) 
        
        # =========================================================
        # ğŸš€ COLMAP å¯åŠ¨ (æ‰‹åŠ¨æŒ¡ + å¼ºåˆ¶ä¿®æ­£)
        # =========================================================
        print(f"    âœ… å‡†å¤‡å¯åŠ¨ COLMAP (Linux GPU æ¨¡å¼)...")
        
        colmap_output_dir = data_dir / "colmap"
        colmap_output_dir.mkdir(parents=True, exist_ok=True)
        database_path = colmap_output_dir / "database.db"
        
        # ç»å¯¹è·¯å¾„è°ƒç”¨
        system_colmap_exe = "/usr/local/bin/colmap" 
        if not os.path.exists(system_colmap_exe):
            found_path = shutil.which("colmap")
            if found_path and "conda" not in found_path:
                system_colmap_exe = found_path
                print(f"    âš ï¸ è­¦å‘Š: /usr/local/bin/colmap ä¸å­˜åœ¨ï¼Œå°è¯•ä½¿ç”¨: {system_colmap_exe}")

        def run_colmap_step(cmd, step_desc):
            print(f"\nâš¡ {step_desc}...")
            try:
                with subprocess.Popen(
                    cmd, 
                    stdout=subprocess.PIPE, 
                    stderr=subprocess.STDOUT,
                    text=True, 
                    env=env,
                    bufsize=1 
                ) as process:
                    for line in process.stdout:
                        print(line, end='') 
                    process.wait()
                    if process.returncode != 0:
                        raise subprocess.CalledProcessError(process.returncode, cmd)
            except Exception as e:
                print(f"\nâŒ {step_desc} æ‰§è¡Œå¼‚å¸¸: {e}")
                raise e

        # 1. Feature Extractor
        run_colmap_step([
            system_colmap_exe, "feature_extractor",
            "--database_path", str(database_path),
            "--image_path", str(img_dir),
            "--ImageReader.camera_model", "OPENCV",
            "--ImageReader.single_camera", "1"
        ], "[1/4] GPU ç‰¹å¾æå–")

        # 2. Sequential Matcher
        run_colmap_step([
            system_colmap_exe, "sequential_matcher",
            "--database_path", str(database_path),
            "--SequentialMatching.overlap", "25" 
        ], "[2/4] GPU é¡ºåºåŒ¹é…")

        # 3. Mapper
        sparse_output_dir = colmap_output_dir / "sparse" / "0"
        sparse_output_dir.mkdir(parents=True, exist_ok=True)
        
        run_colmap_step([
            system_colmap_exe, "mapper",
            "--database_path", str(database_path),
            "--image_path", str(img_dir),
            "--output_path", str(sparse_output_dir)
        ], "[3/4] ç¨€ç–é‡å»º (Mapper)")

        print(f"âœ… COLMAP è®¡ç®—å®Œæˆï¼æ­£åœ¨æ£€æŸ¥å¹¶ä¿®æ­£ç›®å½•ç»“æ„...")

        # =========================================================
        # ğŸ”§ [3.5] ç›®å½•ç»“æ„å¼ºåŠ›ä¿®æ­£ (Auto-Fixer)
        # =========================================================
        colmap_root = colmap_output_dir
        sparse_root = colmap_root / "sparse"
        target_dir_0 = sparse_root / "0"
        target_dir_0.mkdir(parents=True, exist_ok=True)

        required_files_bin = ["cameras.bin", "images.bin", "points3D.bin"]
        required_files_txt = ["cameras.txt", "images.txt", "points3D.txt"]
        
        model_found = False

        # 1. æ£€æŸ¥æ˜¯ä¸æ˜¯å·²ç»åœ¨ sparse/0
        if all((target_dir_0 / f).exists() for f in required_files_bin):
            model_found = True
        elif all((target_dir_0 / f).exists() for f in required_files_txt):
            model_found = True
            
        # 2. æ£€æŸ¥æ˜¯ä¸æ˜¯åœ¨ sparse æ ¹ç›®å½• -> æ¬è¿
        if not model_found:
            if all((sparse_root / f).exists() for f in required_files_bin):
                print("    ğŸ”§ æ£€æµ‹åˆ° BIN æ¨¡å‹åœ¨ sparse æ ¹ç›®å½•ï¼Œæ­£åœ¨å½’ä½...")
                for f in required_files_bin:
                    shutil.move(str(sparse_root / f), str(target_dir_0 / f))
                model_found = True
            elif all((sparse_root / f).exists() for f in required_files_txt):
                print("    ğŸ”§ æ£€æµ‹åˆ° TXT æ¨¡å‹åœ¨ sparse æ ¹ç›®å½•ï¼Œæ­£åœ¨å½’ä½...")
                for f in required_files_txt:
                    shutil.move(str(sparse_root / f), str(target_dir_0 / f))
                model_found = True

        # 3. æ£€æŸ¥æ˜¯ä¸æ˜¯åœ¨å­ç›®å½• -> æ¬è¿
        if not model_found:
            for root, dirs, files in os.walk(sparse_root):
                if all(f in files for f in required_files_bin):
                    src_path = Path(root)
                    if src_path == target_dir_0: continue
                    print(f"    ğŸ”§ åœ¨å­ç›®å½• {src_path} æ‰¾åˆ° BIN æ¨¡å‹ï¼Œæ­£åœ¨å½’ä½...")
                    for f in required_files_bin:
                        shutil.move(str(src_path / f), str(target_dir_0 / f))
                    model_found = True
                    break
                if all(f in files for f in required_files_txt):
                    src_path = Path(root)
                    if src_path == target_dir_0: continue
                    print(f"    ğŸ”§ åœ¨å­ç›®å½• {src_path} æ‰¾åˆ° TXT æ¨¡å‹ï¼Œæ­£åœ¨å½’ä½...")
                    for f in required_files_txt:
                        shutil.move(str(src_path / f), str(target_dir_0 / f))
                    model_found = True
                    break

        if not model_found:
            raise FileNotFoundError("COLMAP Mapper failed to generate valid model files.")

        # 4. ç”Ÿæˆ transforms.json (è·³è¿‡ COLMAP)
        print(f"\nğŸ”„ [4/4] ç”Ÿæˆ transforms.json (Nerfstudio)...")
        res = subprocess.run([
            "ns-process-data", "images",
            "--data", str(img_dir),
            "--output-dir", str(data_dir),
            "--skip-colmap", 
            "--verbose"
        ], check=True, env=env, capture_output=True, text=True)
        print(res.stdout)

    # ================= [Step 2] æ™ºèƒ½åˆ†æä¸è®­ç»ƒ =================
    search_path = output_dir / project_name / "splatfacto"
    run_dirs = sorted(list(search_path.glob("*"))) if search_path.exists() else []

    if run_dirs:
        print(f"\nâ© [è®­ç»ƒè·³è¿‡] å·²å®Œæˆ")
    else:
        img_dir = data_dir / "images"
        
        # --- A. æ™ºèƒ½å†³ç­–é˜¶æ®µ ---
        gpt_result = analyze_with_gpt4o(img_dir)
        
        if gpt_result:
            # ä½¿ç”¨ GPT çš„ç»“æœ
            scene_type = gpt_result["type"]
            need_mask = gpt_result["masking_needed"]
            subject_prompt = gpt_result["subject"]
        else:
            # å›é€€åˆ°å‡ ä½•åˆ†æ
            scene_type, need_mask, subject_prompt = analyze_scene_geometry(transforms_file)

        # --- B. å†³ç­–æ‰§è¡Œé˜¶æ®µ ---
        collider_params = []
        
        if scene_type == "scene":
            print("ğŸ’¡ ç­–ç•¥ï¼šã€åœºæ™¯æ¨¡å¼ã€‘ -> ç¦ç”¨æŠ å›¾ï¼Œå®½æ¾è£å‰ªã€‚")
            collider_params = ["near_plane", "0.05", "far_plane", "100.0"]
            
        else: # object
            print(f"ğŸ’¡ ç­–ç•¥ï¼šã€ç‰©ä½“æ¨¡å¼ã€‘ -> ä¸»ä½“: '{subject_prompt}'")
            collider_params = ["near_plane", "2.0", "far_plane", "6.0"]
            
            # --- C. æ‰§è¡Œ YOLO+SAM æŠ å›¾ ---
            if need_mask and has_ultralytics:
                print(f"âœ‚ï¸ [AI æ‰§è¡Œ] å¯åŠ¨ YOLO-World + SAM è¯­ä¹‰æŠ å›¾...")
                masks_dir = data_dir / "masks"
                masks_dir.mkdir(exist_ok=True)
                
                # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†è¿‡
                if not any(masks_dir.iterdir()):
                    masker = SmartMasker()
                    all_imgs = sorted(list(img_dir.glob("*.jpg")))
                    processed_count = 0
                    
                    for img_p in all_imgs:
                        mask = masker.generate_mask(str(img_p), subject_prompt)
                        if mask is not None:
                            # å­˜ä¸º png
                            cv2.imwrite(str(masks_dir / (img_p.stem + ".png")), mask)
                            processed_count += 1
                        print(f"    å¤„ç†: {img_p.name} ... {'âœ…' if mask is not None else 'âš ï¸'}", end="\r")
                    
                    print(f"\nâœ… è’™ç‰ˆç”Ÿæˆå®Œæˆï¼š{processed_count}/{len(all_imgs)}")
                    
                    # æ›´æ–° transforms.json
                    with open(transforms_file, 'r') as f: meta = json.load(f)
                    for frame in meta["frames"]:
                        msk_p = masks_dir / (Path(frame["file_path"]).stem + ".png")
                        if msk_p.exists():
                            frame["mask_path"] = f"masks/{msk_p.name}"
                    with open(transforms_file, 'w') as f: json.dump(meta, f, indent=4)
                else:
                    print("â© æ£€æµ‹åˆ°ç°æœ‰è’™ç‰ˆï¼Œè·³è¿‡ç”Ÿæˆã€‚")

        # --- D. å¼€å§‹è®­ç»ƒ ---
        print(f"\nğŸ§  [2/3] å¼€å§‹è®­ç»ƒ...")
        cmd_train = [
            "ns-train", "splatfacto",
            "--data", str(data_dir),
            "--output-dir", str(output_dir),
            "--experiment-name", project_name,
            "--pipeline.model.random-init", "False", 
            "--pipeline.model.cull-alpha-thresh", "0.005",
            # æ’å…¥æ™ºèƒ½å‚æ•°
            "--pipeline.model.enable-collider", "True",
            "--pipeline.model.collider-params", *collider_params,
            
            "--max-num-iterations", "15000",
            "--vis", "viewer+tensorboard", 
            "--viewer.quit-on-train-completion", "True",
            "colmap",
        ]
        subprocess.run(cmd_train, check=True, env=env)

    # ================= [Step 3] å¯¼å‡ºç»“æœ (ä¿æŒåŸåŠŸèƒ½) =================
    print(f"\nğŸ’¾ [3/3] å¯¼å‡ºç»“æœ")
    if not run_dirs: run_dirs = sorted(list(search_path.glob("*")))
    if not run_dirs: return None
        
    latest_run = run_dirs[-1]
    cmd_export = [
        "ns-export", "gaussian-splat",
        "--load-config", str(latest_run / "config.yml"),
        "--output-dir", str(work_dir)
    ]
    subprocess.run(cmd_export, check=True, env=env)
    time.sleep(5)

    # ================= [Step 4] å›ä¼ ä¸å§¿æ€å¤„ç† (ä¿æŒåŸåŠŸèƒ½) =================
    print(f"\nğŸ“¦ [IO åŒæ­¥] å›ä¼ è‡³ Windows...")
    target_dir = Path(__file__).parent / "results"
    target_dir.mkdir(exist_ok=True)

    temp_ply = work_dir / "point_cloud.ply"
    if not temp_ply.exists(): temp_ply = work_dir / "splat.ply"
    
    # WebGL å§¿æ€ç”Ÿæˆ (ä¿ç•™)
    final_webgl_poses = target_dir / "webgl_poses.json"
    if (data_dir / "transforms.json").exists():
        try:
            with open(data_dir / "transforms.json", 'r') as f: data = json.load(f)
            webgl_frames = []
            for frame in data["frames"]:
                c2w = np.array(frame["transform_matrix"], dtype=np.float32)
                # è®¡ç®— W2C
                w2c = np.linalg.inv(c2w) 
                webgl_frames.append({
                    "file_path": frame["file_path"],
                    "pose_matrix_c2w": c2w.tolist() # ä¿ç•™ C2W
                })
            with open(final_webgl_poses, 'w') as f:
                json.dump({"camera_model": data.get("camera_model","OPENCV"), 
                           "frames": webgl_frames}, f, indent=4)
            print(f"âœ… WebGL å§¿æ€å·²ä¿å­˜: {final_webgl_poses}")
        except Exception as e: print(f"âŒ å§¿æ€å¤„ç†å¤±è´¥: {e}")

    final_ply = target_dir / f"{project_name}.ply"
    if temp_ply and temp_ply.exists():
        shutil.copy(str(temp_ply), str(final_ply))
        if (data_dir / "transforms.json").exists():
            shutil.copy(str(data_dir / "transforms.json"), str(target_dir / "transforms.json"))
        shutil.rmtree(work_dir)
        print(f"âœ… æˆåŠŸï¼æ¨¡å‹å·²ä¿å­˜è‡³: {final_ply}")
        return str(final_ply)
    
    return None

if __name__ == "__main__":
    script_dir = Path(__file__).resolve().parent
    video_file = script_dir / "test.mp4" 
    if len(sys.argv) > 1: video_file = Path(sys.argv[1])

    if video_file.exists():
        # å¦‚æœä½ æƒ³é‡æ–°è§¦å‘ GPT åˆ†æï¼Œè¯·æ‰‹åŠ¨åˆ é™¤ transforms.json æˆ– masks ç›®å½•
        run_pipeline(video_file, "scene_auto_sync")
    else:
        print(f"âŒ æ‰¾ä¸åˆ°è§†é¢‘: {video_file}")