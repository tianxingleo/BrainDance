import subprocess
import sys
import shutil
import os
import time
import base64
import json
import logging
import numpy as np
import cv2
from pathlib import Path

# ================= ğŸ”§ ç”¨æˆ·é…ç½®åŒºåŸŸ =================
# 1. OpenAI API Key (ç”¨äº GPT-4o è¯­ä¹‰åˆ†æ)
# å¦‚æœç•™ç©ºï¼Œå°†è‡ªåŠ¨é™çº§ä¸ºä½¿ç”¨â€œå‡ ä½•ç®—æ³•â€è¿›è¡Œåˆ†æï¼Œæ— éœ€è”ç½‘
OPENAI_API_KEY = "sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx" 

# 2. Linux å·¥ä½œåŒºè·¯å¾„
LINUX_WORK_ROOT = Path.home() / "braindance_workspace"

# 3. ç¡¬ä»¶é…ç½® (æ‚¨æœ‰ 5070ï¼Œç›´æ¥æ‹‰æ»¡)
# YOLO-World æ¨¡å‹: yolov8x-worldv2.pt (æœ€å¼ºç‰ˆ)
# SAM æ¨¡å‹: sam_l.pt (Largeç‰ˆï¼Œç²¾åº¦æœ€é«˜)
MODEL_YOLO = 'yolov8x-worldv2.pt'
MODEL_SAM = 'sam_l.pt'

# ================= ğŸ“¦ åº“å¯¼å…¥ä¸åˆå§‹åŒ– =================
logging.getLogger('nerfstudio').setLevel(logging.ERROR)
logging.getLogger('ultralytics').setLevel(logging.ERROR)

try:
    from openai import OpenAI
    has_openai = True if OPENAI_API_KEY and OPENAI_API_KEY.startswith("sk-") else False
except ImportError:
    has_openai = False
    print("âš ï¸ æœªå®‰è£… openai åº“ï¼Œå°†ä½¿ç”¨æœ¬åœ°å‡ ä½•åˆ†ææ¨¡å¼ã€‚")

try:
    from ultralytics import YOLOWorld, SAM
    has_ultralytics = True
except ImportError:
    has_ultralytics = False
    print("âš ï¸ æœªå®‰è£… ultralytics åº“ï¼Œå°†è·³è¿‡æœ¬åœ° AI æŠ å›¾ã€‚")

# ================= ğŸ§  æ ¸å¿ƒ AI æ¨¡å— =================

class SmartMasker:
    """
    æœ¬åœ° AI å¼•æ“ï¼šç»“åˆ YOLO-World (å¬è§‰/è§†è§‰) + SAM (è§¦è§‰/åˆ†å‰²)
    """
    def __init__(self):
        if not has_ultralytics: return
        print(f"\nğŸ¦¾ [æœ¬åœ° AI å¼•æ“] æ­£åœ¨åŠ è½½å¤§æ¨¡å‹ (RTX 5070 ç®—åŠ›å…¨å¼€)...")
        print(f"    -> åŠ è½½ {MODEL_YOLO} (è¯­ä¹‰è¯†åˆ«)...")
        self.detector = YOLOWorld(MODEL_YOLO)
        print(f"    -> åŠ è½½ {MODEL_SAM} (åƒç´ åˆ†å‰²)...")
        self.segmentor = SAM(MODEL_SAM)
        print("âœ… AI å¼•æ“å°±ç»ªã€‚")

    def generate_mask(self, image_path, prompt):
        """è¾“å…¥å›¾ç‰‡å’Œæç¤ºè¯ï¼Œè¿”å›è’™ç‰ˆ"""
        try:
            # 1. YOLO-World: å¯»æ‰¾ç‰©ä½“æ¡†
            self.detector.set_classes([prompt])
            det_results = self.detector.predict(image_path, conf=0.15, verbose=False)
            
            if len(det_results[0].boxes) == 0:
                return None # æ²¡æ‰¾åˆ°ç‰©ä½“
            
            # 2. SAM: æ ¹æ®æ¡†ç”Ÿæˆè’™ç‰ˆ
            bboxes = det_results[0].boxes.xyxy
            sam_results = self.segmentor(image_path, bboxes=bboxes, verbose=False)
            
            if len(sam_results[0].masks) == 0:
                return None

            # 3. åˆå¹¶è’™ç‰ˆ
            final_mask = np.zeros(sam_results[0].orig_shape[:2], dtype=np.uint8)
            masks = sam_results[0].masks.data.cpu().numpy()
            
            for mask in masks:
                mask_uint8 = (mask * 255).astype(np.uint8)
                if mask_uint8.shape != final_mask.shape:
                     mask_uint8 = cv2.resize(mask_uint8, (final_mask.shape[1], final_mask.shape[0]))
                final_mask = cv2.bitwise_or(final_mask, mask_uint8)
                
            return final_mask
        except Exception as e:
            print(f"âŒ åˆ†å‰²é”™è¯¯: {e}")
            return None

def analyze_with_gpt4o(images_dir):
    """
    ä½¿ç”¨ GPT-4o å¤šæ¨¡æ€å¤§æ¨¡å‹åˆ†æåœºæ™¯
    """
    if not has_openai: return None

    print(f"\nğŸ§  [GPT-4o] æ­£åœ¨ä¸Šä¼ å…³é”®å¸§è¿›è¡Œè¯­ä¹‰ç†è§£...")
    client = OpenAI(api_key=OPENAI_API_KEY)
    
    # å‡åŒ€æŠ½å– 6 å¼ å›¾
    all_imgs = sorted(list(images_dir.glob("*.jpg")))
    if not all_imgs: return None
    step = max(1, len(all_imgs) // 6)
    sampled_imgs = all_imgs[::step][:6]

    content = [{"type": "text", "text": "Analyzing frames for 3D Gaussian Splatting training."}]
    
    for img_path in sampled_imgs:
        with open(img_path, "rb") as f:
            b64 = base64.b64encode(f.read()).decode('utf-8')
        content.append({
            "type": "image_url",
            "image_url": {"url": f"data:image/jpeg;base64,{b64}", "detail": "low"}
        })

    prompt = """
    Analyze these images and output a JSON with:
    1. "type": "object" (if focusing on specific items like a toy, shoe, person) or "scene" (room, street, large area).
    2. "subject": If "object", give a short English specific description for detection (e.g. "red nike shoes", "anime figure"). If "scene", use "none".
    3. "masking_needed": Boolean. True if it's an object with messy background. False if it's a scene OR object with white/clean background.
    """
    content.append({"type": "text", "text": prompt})

    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": content}],
            response_format={"type": "json_object"}
        )
        result = json.loads(response.choices[0].message.content)
        print(f"ğŸ¤– GPT æ´å¯Ÿ: ç±»å‹=[{result['type']}] | ä¸»ä½“=[{result['subject']}] | éœ€æŠ å›¾=[{result['masking_needed']}]")
        return result
    except Exception as e:
        print(f"âš ï¸ GPT åˆ†æå¤±è´¥: {e}")
        return None

def analyze_scene_geometry(json_path):
    """
    (å¤‡ç”¨æ–¹æ¡ˆ) å‡ ä½•åˆ†æç®—æ³•ï¼Œå½“ GPT ä¸å¯ç”¨æ—¶ä½¿ç”¨
    """
    print(f"ğŸ“ [å‡ ä½•åˆ†æ] GPT æœªå¯ç”¨ï¼Œæ­£åœ¨è®¡ç®—ç›¸æœºè½¨è¿¹èšåˆåº¦...")
    try:
        with open(json_path, 'r') as f: frames = json.load(f)["frames"]
        if not frames: return "scene", False, "none"

        positions, forwards = [], []
        for f in frames:
            c2w = np.array(f["transform_matrix"])
            positions.append(c2w[:3, 3])
            forwards.append(c2w[:3, :3] @ np.array([0, 0, -1]))
            
        center = np.mean(positions, axis=0)
        vecs = center - np.array(positions)
        vecs /= (np.linalg.norm(vecs, axis=1, keepdims=True) + 1e-6)
        
        # è®¡ç®—è§†çº¿èšåˆåº¦
        ratio = np.sum(np.sum(np.array(forwards) * vecs, axis=1) > 0) / len(frames)
        
        if ratio > 0.6:
            print(f"    -> èšåˆåº¦ {ratio:.2f} (>0.6)ï¼Œåˆ¤å®šä¸ºç‰©ä½“æ¨¡å¼ã€‚")
            return "object", True, "object" # å‡ ä½•æ¨¡å¼ä¸‹é»˜è®¤å¼€å¯æŠ å›¾ï¼Œé€šç”¨æç¤ºè¯
        else:
            print(f"    -> èšåˆåº¦ {ratio:.2f} (<0.6)ï¼Œåˆ¤å®šä¸ºåœºæ™¯æ¨¡å¼ã€‚")
            return "scene", False, "none"
    except:
        return "scene", False, "none"


# ================= ğŸš€ ä¸»æµç¨‹ =================

def run_pipeline(video_path, project_name):
    print(f"\nğŸš€ [BrainDance Engine 5.0] å¯åŠ¨ä»»åŠ¡: {project_name}")
    print(f"âš¡ ç¡¬ä»¶åŠ é€Ÿ: Intel 14600KF + RTX 5070 | AI æ ¸å¿ƒ: GPT-4o + YOLO-World + SAM")
    
    # 1. è·¯å¾„è§£æ
    video_src = Path(video_path).resolve()
    work_dir = LINUX_WORK_ROOT / project_name
    data_dir = work_dir / "data"
    output_dir = work_dir / "outputs"
    transforms_file = data_dir / "transforms.json"
    env = os.environ.copy()
    env["QT_QPA_PLATFORM"] = "offscreen" 

    # ================= [Step 1] æ•°æ®å¤„ç† (æ ‡å‡†æµç¨‹) =================
    if transforms_file.exists():
        print(f"\nâ© [æ–­ç‚¹ç»­ä¼ ] æ£€æµ‹åˆ° COLMAP æ•°æ®")
    else:
        print(f"ğŸ†• [æ–°ä»»åŠ¡] åˆå§‹åŒ–å·¥ä½œåŒº...")
        if work_dir.exists(): shutil.rmtree(work_dir)
        work_dir.mkdir(parents=True)
        data_dir.mkdir(parents=True)
        shutil.copy(str(video_src), str(work_dir / video_src.name))

        print(f"\nğŸ¥ [1/3] è§†é¢‘å¤„ç† (COLMAP)")
        img_dir = data_dir / "images"
        img_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿æŒåŸæœ‰ 1920 / 4fps è®¾ç½®
        subprocess.run([
            "ffmpeg", "-y", "-i", str(work_dir / video_src.name), 
            "-vf", "scale=1920:-1,fps=4", "-q:v", "2", 
            str(img_dir / "frame_%05d.jpg")
        ], check=True) 
        
        # COLMAP
        res = subprocess.run([
            "ns-process-data", "images",
            "--data", str(img_dir),
            "--output-dir", str(data_dir),
            "--verbose"
        ], check=True, env=env, capture_output=True, text=True)
        print(res.stdout)
        if "COLMAP only found poses" in res.stdout: raise RuntimeError("COLMAP å¤±è´¥")

    # ================= [Step 2] æ™ºèƒ½åˆ†æä¸è®­ç»ƒ =================
    search_path = output_dir / project_name / "splatfacto"
    run_dirs = sorted(list(search_path.glob("*"))) if search_path.exists() else []

    if run_dirs:
        print(f"\nâ© [è®­ç»ƒè·³è¿‡] å·²å®Œæˆ")
    else:
        img_dir = data_dir / "images"
        
        # --- A. æ™ºèƒ½å†³ç­–é˜¶æ®µ ---
        gpt_result = analyze_with_gpt4o(img_dir)
        
        if gpt_result:
            # ä½¿ç”¨ GPT çš„ç»“æœ
            scene_type = gpt_result["type"]
            need_mask = gpt_result["masking_needed"]
            subject_prompt = gpt_result["subject"]
        else:
            # å›é€€åˆ°å‡ ä½•åˆ†æ
            scene_type, need_mask, subject_prompt = analyze_scene_geometry(transforms_file)

        # --- B. å†³ç­–æ‰§è¡Œé˜¶æ®µ ---
        collider_params = []
        
        if scene_type == "scene":
            print("ğŸ’¡ ç­–ç•¥ï¼šã€åœºæ™¯æ¨¡å¼ã€‘ -> ç¦ç”¨æŠ å›¾ï¼Œå®½æ¾è£å‰ªã€‚")
            collider_params = ["near_plane", "0.05", "far_plane", "100.0"]
            
        else: # object
            print(f"ğŸ’¡ ç­–ç•¥ï¼šã€ç‰©ä½“æ¨¡å¼ã€‘ -> ä¸»ä½“: '{subject_prompt}'")
            collider_params = ["near_plane", "2.0", "far_plane", "6.0"]
            
            # --- C. æ‰§è¡Œ YOLO+SAM æŠ å›¾ ---
            if need_mask and has_ultralytics:
                print(f"âœ‚ï¸ [AI æ‰§è¡Œ] å¯åŠ¨ YOLO-World + SAM è¯­ä¹‰æŠ å›¾...")
                masks_dir = data_dir / "masks"
                masks_dir.mkdir(exist_ok=True)
                
                # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†è¿‡
                if not any(masks_dir.iterdir()):
                    masker = SmartMasker()
                    all_imgs = sorted(list(img_dir.glob("*.jpg")))
                    processed_count = 0
                    
                    for img_p in all_imgs:
                        mask = masker.generate_mask(str(img_p), subject_prompt)
                        if mask is not None:
                            # å­˜ä¸º png
                            cv2.imwrite(str(masks_dir / (img_p.stem + ".png")), mask)
                            processed_count += 1
                        print(f"    å¤„ç†: {img_p.name} ... {'âœ…' if mask is not None else 'âš ï¸'}", end="\r")
                    
                    print(f"\nâœ… è’™ç‰ˆç”Ÿæˆå®Œæˆï¼š{processed_count}/{len(all_imgs)}")
                    
                    # æ›´æ–° transforms.json
                    with open(transforms_file, 'r') as f: meta = json.load(f)
                    for frame in meta["frames"]:
                        msk_p = masks_dir / (Path(frame["file_path"]).stem + ".png")
                        if msk_p.exists():
                            frame["mask_path"] = f"masks/{msk_p.name}"
                    with open(transforms_file, 'w') as f: json.dump(meta, f, indent=4)
                else:
                    print("â© æ£€æµ‹åˆ°ç°æœ‰è’™ç‰ˆï¼Œè·³è¿‡ç”Ÿæˆã€‚")

        # --- D. å¼€å§‹è®­ç»ƒ ---
        print(f"\nğŸ§  [2/3] å¼€å§‹è®­ç»ƒ...")
        cmd_train = [
            "ns-train", "splatfacto",
            "--data", str(data_dir),
            "--output-dir", str(output_dir),
            "--experiment-name", project_name,
            "--pipeline.model.random-init", "False", 
            "--pipeline.model.cull-alpha-thresh", "0.005",
            # æ’å…¥æ™ºèƒ½å‚æ•°
            "--pipeline.model.enable-collider", "True",
            "--pipeline.model.collider-params", *collider_params,
            
            "--max-num-iterations", "15000",
            "--vis", "viewer+tensorboard", 
            "--viewer.quit-on-train-completion", "True",
            "colmap",
        ]
        subprocess.run(cmd_train, check=True, env=env)

    # ================= [Step 3] å¯¼å‡ºç»“æœ (ä¿æŒåŸåŠŸèƒ½) =================
    print(f"\nğŸ’¾ [3/3] å¯¼å‡ºç»“æœ")
    if not run_dirs: run_dirs = sorted(list(search_path.glob("*")))
    if not run_dirs: return None
        
    latest_run = run_dirs[-1]
    cmd_export = [
        "ns-export", "gaussian-splat",
        "--load-config", str(latest_run / "config.yml"),
        "--output-dir", str(work_dir)
    ]
    subprocess.run(cmd_export, check=True, env=env)
    time.sleep(5)

    # ================= [Step 4] å›ä¼ ä¸å§¿æ€å¤„ç† (ä¿æŒåŸåŠŸèƒ½) =================
    print(f"\nğŸ“¦ [IO åŒæ­¥] å›ä¼ è‡³ Windows...")
    target_dir = Path(__file__).parent / "results"
    target_dir.mkdir(exist_ok=True)

    temp_ply = work_dir / "point_cloud.ply"
    if not temp_ply.exists(): temp_ply = work_dir / "splat.ply"
    
    # WebGL å§¿æ€ç”Ÿæˆ (ä¿ç•™)
    final_webgl_poses = target_dir / "webgl_poses.json"
    if (data_dir / "transforms.json").exists():
        try:
            with open(data_dir / "transforms.json", 'r') as f: data = json.load(f)
            webgl_frames = []
            for frame in data["frames"]:
                c2w = np.array(frame["transform_matrix"], dtype=np.float32)
                # è®¡ç®— W2C
                w2c = np.linalg.inv(c2w) 
                webgl_frames.append({
                    "file_path": frame["file_path"],
                    "pose_matrix_c2w": c2w.tolist() # ä¿ç•™ C2W
                })
            with open(final_webgl_poses, 'w') as f:
                json.dump({"camera_model": data.get("camera_model","OPENCV"), 
                           "frames": webgl_frames}, f, indent=4)
            print(f"âœ… WebGL å§¿æ€å·²ä¿å­˜: {final_webgl_poses}")
        except Exception as e: print(f"âŒ å§¿æ€å¤„ç†å¤±è´¥: {e}")

    final_ply = target_dir / f"{project_name}.ply"
    if temp_ply and temp_ply.exists():
        shutil.copy(str(temp_ply), str(final_ply))
        if (data_dir / "transforms.json").exists():
            shutil.copy(str(data_dir / "transforms.json"), str(target_dir / "transforms.json"))
        shutil.rmtree(work_dir)
        print(f"âœ… æˆåŠŸï¼æ¨¡å‹å·²ä¿å­˜è‡³: {final_ply}")
        return str(final_ply)
    
    return None

if __name__ == "__main__":
    script_dir = Path(__file__).resolve().parent
    video_file = script_dir / "test.mp4" 
    if len(sys.argv) > 1: video_file = Path(sys.argv[1])

    if video_file.exists():
        # å¦‚æœä½ æƒ³é‡æ–°è§¦å‘ GPT åˆ†æï¼Œè¯·æ‰‹åŠ¨åˆ é™¤ transforms.json æˆ– masks ç›®å½•
        run_pipeline(video_file, "scene_auto_sync")
    else:
        print(f"âŒ æ‰¾ä¸åˆ°è§†é¢‘: {video_file}")