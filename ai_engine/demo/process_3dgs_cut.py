import subprocess
import sys
import shutil
import os
import time
import datetime
from pathlib import Path
import json
import numpy as np
import logging
import cv2
import re

# ================= ğŸ§  AI ä¾èµ–å¼•å…¥ =================
try:
    import dashscope
    from dashscope import MultiModalConversation
    from ultralytics import SAM, YOLOWorld
    HAS_AI = True
except ImportError:
    HAS_AI = False
    print("âš ï¸ [ç¯å¢ƒè­¦å‘Š] æœªæ£€æµ‹åˆ° dashscope æˆ– ultralytics åº“ã€‚")
    print("    -> æ™ºèƒ½åˆ†å‰²åŠŸèƒ½å°†è¢«ç¦ç”¨ã€‚è¯·è¿è¡Œ: pip install dashscope ultralytics")

# ğŸ”¥ è¯·åœ¨æ­¤å¤„å¡«å…¥ä½ çš„ API KEY (æˆ–è€…ç¡®ä¿ç¯å¢ƒå˜é‡ DASHSCOPE_API_KEY å·²å­˜åœ¨)
# os.environ["DASHSCOPE_API_KEY"] = "sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"

# ================= ğŸ”§ åŸºç¡€é…ç½® =================
# ğŸ”¥ã€ç»æ€ã€‘å¼ºåˆ¶å°†ç¼–è¯‘å¥½çš„ç³»ç»Ÿçº§ colmap è·¯å¾„æåˆ°æœ€å‰é¢
sys_path = "/usr/local/bin"
current_path = os.environ.get("PATH", "")
if sys_path not in current_path.split(os.pathsep)[0]:
    print(f"âš¡ [ç¯å¢ƒä¿®æ­£] å¼ºåˆ¶è®¾ç½® PATH ä¼˜å…ˆçº§: {sys_path} -> Priority High")
    os.environ["PATH"] = f"{sys_path}{os.pathsep}{current_path}"

# è®¾ç½®æ—¥å¿—çº§åˆ«
logging.getLogger('nerfstudio').setLevel(logging.ERROR) 

# å·¥ä½œåŒºé…ç½®
LINUX_WORK_ROOT = Path.home() / "braindance_workspace"
# ğŸ”¥ æ–°å¢ï¼šè¯æ±‡æ ‘æ–‡ä»¶è·¯å¾„ (è¯·ç¡®ä¿ä½ ä¸‹è½½äº†å®ƒï¼)
VOCAB_TREE_PATH = LINUX_WORK_ROOT / "vocab_tree_flickr100k_words.bin" 
SCENE_RADIUS_SCALE = 1.8 
MAX_IMAGES = 180 

# åˆ‡å‰²é…ç½®
FORCE_SPHERICAL_CULLING = True
KEEP_PERCENTILE = 0.9

# æ£€æŸ¥ plyfile
try:
    from plyfile import PlyData, PlyElement
    HAS_PLYFILE = True
except ImportError:
    HAS_PLYFILE = False

# ================= ğŸ§  AI æ ¸å¿ƒé€»è¾‘å‡½æ•° =================

def get_central_object_prompt(images_dir: Path, sample_count=3):
    """
    [Step 1.1] ä½¿ç”¨ Qwen-VL-Plus å¤šå›¾åˆ†æï¼Œæå–ä¸­å¿ƒç‰©ä½“çš„æ–‡æœ¬æè¿°
    """
    api_key = os.environ.get("DASHSCOPE_API_KEY")
    if not api_key:
        print("âŒ æœªè®¾ç½® DASHSCOPE_API_KEYï¼Œæ— æ³•è°ƒç”¨å¤§æ¨¡å‹ã€‚")
        return None

    print(f"\nğŸ§  [AI åˆ†æ] æ­£åœ¨è°ƒç”¨ Qwen-VL-Plus åˆ†æåœºæ™¯...")
    
    # éšæœºé‡‡æ · 3 å¼ å›¾ç‰‡
    image_files = sorted(list(images_dir.glob("*.jpg")) + list(images_dir.glob("*.png")))
    if not image_files: return None
    
    indices = np.linspace(0, len(image_files) - 1, sample_count, dtype=int)
    sampled_imgs = [image_files[i] for i in indices]
    
    # æ„å»ºå¤šæ¨¡æ€æ¶ˆæ¯
    content = [{"image": str(img_path)} for img_path in sampled_imgs]
    content.append({
        "text": (
            "è¿™äº›æ˜¯ä¸€ä¸ªè§†é¢‘çš„æŠ½å¸§å›¾ç‰‡ã€‚è¯·åˆ†æç”»é¢ä¸­å¿ƒå§‹ç»ˆå­˜åœ¨çš„ã€æœ€ä¸»è¦çš„ä¸€ä¸ªç‰©ä½“æ˜¯ä»€ä¹ˆã€‚"
            "è¯·è¾“å‡ºä¸€ä¸ªé€‚åˆç”¨äºç‰©ä½“æ£€æµ‹æ¨¡å‹çš„è‹±æ–‡åè¯çŸ­è¯­ï¼ˆPromptï¼‰ã€‚"
            "âš ï¸ å…³é”®ç­–ç•¥ï¼šè¯·ä¼˜å…ˆæè¿°ã€è§†è§‰ç‰¹å¾ã€‘ï¼ˆé¢œè‰²ã€æè´¨ã€å½¢çŠ¶ï¼‰ï¼Œè€Œä¸æ˜¯ã€åŠŸèƒ½åç§°ã€‘ã€‚"
            "è¶Šç®€å•ã€è¶Š'åœŸ'çš„è¯ï¼Œæ£€æµ‹æ¨¡å‹è¶Šå®¹æ˜“è¯†åˆ«ã€‚"
            "ä¾‹å¦‚ï¼š"
            " - ä¸è¦è¯´ 'electric shaver' (ç”µåŠ¨å‰ƒé¡»åˆ€)ï¼Œè¯·è¯´ 'gray metal object' æˆ– 'device'ã€‚"
            " - ä¸è¦è¯´ 'portable charger' (å……ç”µå®)ï¼Œè¯·è¯´ 'white rectangular box'ã€‚"
            "è¦æ±‚ï¼šä¸¥æ ¼åªè¾“å‡ºè¿™ä¸ªè‹±æ–‡çŸ­è¯­ï¼Œä¸è¦åŒ…å«ä»»ä½•æ ‡ç‚¹ç¬¦å·ã€è§£é‡Šã€‚"
        )
    })

    messages = [{"role": "user", "content": content}]

    try:
        response = dashscope.MultiModalConversation.call(
            model='qwen-vl-plus', 
            messages=messages
        )
        
        if response.status_code == 200:
            prompt_text = response.output.choices[0].message.content[0]["text"].strip()
            # ç®€å•çš„æ¸…æ´—ï¼Œå»æ‰å¯èƒ½çš„æ ‡ç‚¹
            prompt_text = prompt_text.replace(".", "").replace('"', "").replace("'", "")
            print(f"    ğŸ¤– Qwen è®¤ä¸ºä¸­å¿ƒç‰©ä½“æ˜¯: [ \033[92m{prompt_text}\033[0m ]")
            return prompt_text
        else:
            print(f"âŒ Qwen è°ƒç”¨å¤±è´¥: {response.code} - {response.message}")
            return None
    except Exception as e:
        print(f"âŒ API è¿æ¥å¼‚å¸¸: {e}")
        return None

def clean_and_verify_mask(mask, img_name=""):
    """
    [å‡€åŒ–ç‰ˆ] 
    1. å¼ºåˆ¶æ¸…æ´—ï¼šåªä¿ç•™ç”»é¢ä¸­æœ€å¤§çš„è¿é€šå— (å»é™¤å­¤ç«‹å™ªç‚¹)ã€‚
    2. ä¸¥æ ¼è´¨æ£€ï¼šæ¸…æ´—åå¦‚æœå½¢çŠ¶ä¾ç„¶æ¯›ç³™(ç²˜è¿é˜´å½±)ï¼Œåˆ™å‰”é™¤ã€‚
    3. è¿”å›ï¼š(æ˜¯å¦åˆæ ¼, æ¸…æ´—åçš„å¹²å‡€Mask, åŸå› )
    """
    h, w = mask.shape
    
    # --- 1. è¿é€šåŸŸåˆ†æ & å¼ºåˆ¶æ¸…æ´— (Cleaning) ---
    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(mask, connectivity=8)
    
    # å¦‚æœå…¨é»‘ï¼Œç›´æ¥æ‰”
    if num_labels < 2: 
        return False, None, "ç©ºè’™ç‰ˆ"

    # æ‰¾å‡ºæœ€å¤§çš„å‰æ™¯å— (å¿½ç•¥ index 0 çš„èƒŒæ™¯)
    max_area = 0
    max_label = -1
    for i in range(1, num_labels):
        if stats[i, cv2.CC_STAT_AREA] > max_area:
            max_area = stats[i, cv2.CC_STAT_AREA]
            max_label = i
            
    # å¦‚æœæœ€å¤§çš„å—ä¹Ÿå¤ªå° (æ¯”å¦‚åªå å±å¹• 0.5%)ï¼Œé‚£æ˜¯åƒåœ¾
    if max_area < (h * w * 0.005):
        return False, None, "ä¸»ä½“è¿‡å°ï¼Œç–‘ä¼¼å™ªç‚¹"

    # ğŸ”¥ æ ¸å¿ƒæ“ä½œï¼šé‡æ„ Maskï¼Œåªä¿ç•™æœ€å¤§çš„é‚£ä¸€å—
    # Frame 103 çš„é¡¶éƒ¨å™ªç‚¹å’Œ Frame 13 çš„å·¦ä¸‹è§’ç¢ç‚¹åœ¨è¿™é‡Œä¼šè¢«ç›´æ¥æŠ¹é™¤
    cleaned_mask = (labels == max_label).astype(np.uint8) * 255

    # --- 2. å¯¹æ¸…æ´—åçš„ Mask è¿›è¡Œâ€œä½“æ£€â€ (Verification) ---
    
    # æå–è½®å»“
    contours, _ = cv2.findContours(cleaned_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if not contours: return False, None, "æ¸…æ´—åæ— è½®å»“"
    
    main_cnt = max(contours, key=cv2.contourArea)
    
    # [æ£€æŸ¥ A] å®å¿ƒåº¦ (Solidity)
    # é’ˆå¯¹ Frame 21 åº•éƒ¨é‚£ç§ç²˜è¿çš„é”¯é½¿çŠ¶é˜´å½±ã€‚
    # æ­£å¸¸çš„å‰ƒé¡»åˆ€æ˜¯åœ†æ¶¦çš„ï¼ŒSolidity åº”è¯¥æ¥è¿‘ 0.95 ä»¥ä¸Šã€‚
    # å¦‚æœåº•éƒ¨ç²˜äº†ä¸€æ»©çƒ‚æ³¥ä¸€æ ·çš„é˜´å½±ï¼ŒSolidity ä¼šæ‰åˆ° 0.85 ä»¥ä¸‹ã€‚
    hull = cv2.convexHull(main_cnt)
    hull_area = cv2.contourArea(hull)
    if hull_area == 0: return False, None, "å‡¸åŒ…é¢ç§¯ä¸º0"
    
    solidity = max_area / hull_area
    
    # é˜ˆå€¼è®¾å®šï¼š0.88 (éå¸¸ä¸¥æ ¼ï¼Œåªå…è®¸æå…¶è½»å¾®çš„è¾¹ç¼˜ä¸å¹³æ•´)
    if solidity < 0.88:
        return False, None, f"è¾¹ç¼˜ä¸¥é‡æ¯›ç³™/ç²˜è¿é˜´å½± (å®å¿ƒåº¦ {solidity:.2f})"

    # [æ£€æŸ¥ B] æå…¶å¤¸å¼ çš„é•¿å®½æ¯” (é˜²æ­¢æŠŠæ¡Œå­ç¼éš™å½“æˆç‰©ä½“)
    x, y, w_rect, h_rect = cv2.boundingRect(main_cnt)
    aspect_ratio = w_rect / h_rect
    if aspect_ratio > 4.5: # æ”¾å®½äº†ä¹‹å‰çš„æ ‡å‡†ï¼Œä½†å¤ªç¦»è°±çš„é•¿æ¡è¿˜æ˜¯è¦æ€
        return False, None, f"å½¢çŠ¶å¼‚å¸¸ (é•¿å®½æ¯” {aspect_ratio:.1f})"

    # æ³¨æ„ï¼šè¿™é‡Œå®Œå…¨ç§»é™¤äº†â€œè¾¹ç•Œæº¢å‡ºâ€æ£€æŸ¥ï¼Œç¢°åˆ°è¾¹ç•Œä¹Ÿèƒ½è¿‡ã€‚

    # ğŸ”¥ æ–°å¢ï¼šè¾¹ç¼˜è…èš€ (Erosion)
    # è¿™ä¸€æ­¥æ˜¯ä¸ºäº†åˆ‡æ‰ç‰©ä½“è¾¹ç¼˜æ²¾æŸ“çš„æ¡Œé¢åå…‰å’Œé‚£ä¸€åœˆæ·¡æ·¡çš„é˜´å½±
    kernel_size = 3  # è…èš€åŠ›åº¦ï¼Œ3x3 çº¦ç­‰äºç¼©å‡ 1-2 ä¸ªåƒç´ 
    kernel = np.ones((kernel_size, kernel_size), np.uint8)
    cleaned_mask = cv2.erode(cleaned_mask, kernel, iterations=1)
    
    return True, cleaned_mask, "åˆæ ¼"

def get_salient_box(img_path, margin_ratio=0.1):
    """
    [çº¯æœ¬åœ° CV ç®—æ³•] è®¡ç®—ç”»é¢çš„'è§†è§‰æ˜¾è‘—åŒºåŸŸ'ï¼Œä»¥æ­¤ä½œä¸º SAM çš„æç¤ºæ¡†ã€‚
    åŸç†ï¼šåˆ©ç”¨æ‹‰æ™®æ‹‰æ–¯ç®—å­æ‰¾è¾¹ç¼˜ -> è†¨èƒ€è¿æ¥ -> æ‰¾æœ€å¤§å¤–æ¥çŸ©å½¢
    """
    try:
        img = cv2.imread(str(img_path))
        if img is None: return None
        
        # 1. è½¬ç°åº¦å¹¶è®¡ç®—è¾¹ç¼˜ (Laplacian)
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        # è®¡ç®—æ¢¯åº¦/è¾¹ç¼˜ï¼Œè¶Šæ˜¯ç‰©ä½“è¾¹ç¼˜è¶Šäº®
        laplacian = cv2.Laplacian(gray, cv2.CV_64F)
        laplacian = np.uint8(np.absolute(laplacian))
        
        # 2. æ¨¡ç³Šä¸äºŒå€¼åŒ– (æŠŠé›¶æ•£çš„è¾¹ç¼˜è¿æˆå—)
        # é«˜æ–¯æ¨¡ç³Šè®©çº¹ç†èšé›†
        blurred = cv2.GaussianBlur(laplacian, (25, 25), 0)
        # é˜ˆå€¼å¤„ç†ï¼šåªä¿ç•™æœ€'å¼ºçƒˆ'çš„çº¹ç†åŒºåŸŸ (å–å‰20%äº®çš„åŒºåŸŸ)
        threshold_val = np.percentile(blurred, 80) 
        _, binary = cv2.threshold(blurred, threshold_val, 255, cv2.THRESH_BINARY)
        
        # 3. æ‰¾æœ€å¤§è½®å»“
        contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        if not contours: return None
        
        # æ‰¾åˆ°é¢ç§¯æœ€å¤§çš„è½®å»“ï¼ˆé€šå¸¸å°±æ˜¯ä¸»ä½“ï¼‰
        max_cnt = max(contours, key=cv2.contourArea)
        x, y, w, h = cv2.boundingRect(max_cnt)
        
        # 4. åŠ ä¸Šä¸€ç‚¹å®‰å…¨è¾¹è· (Padding)ï¼Œé˜²æ­¢æ¡†å¤ªç´§
        H, W = img.shape[:2]
        pad_x = int(w * margin_ratio)
        pad_y = int(h * margin_ratio)
        
        x1 = max(0, x - pad_x)
        y1 = max(0, y - pad_y)
        x2 = min(W, x + w + pad_x)
        y2 = min(H, y + h + pad_y)
        
        # è¿”å›ç¬¦åˆ YOLO/SAM æ ¼å¼çš„ tensor
        import torch
        return torch.tensor([[x1, y1, x2, y2]], dtype=torch.float32)
        
    except Exception as e:
        print(f"       âš ï¸ è§†è§‰é‡å¿ƒè®¡ç®—å¤±è´¥: {e}")
        return None

def run_ai_segmentation_pipeline(data_dir: Path):
    """
    [Step 1.2] æ‰§è¡Œ AI åˆ†å‰²
    é€»è¾‘ï¼šQwenåˆ†æ -> å¤±è´¥åˆ™ç”¨é€šç”¨è¯ -> YOLOè¯†åˆ« -> å¤±è´¥åˆ™å¼ºåˆ¶ä¸­å¿ƒæ¡† -> SAMåˆ†å‰²
    """
    if not HAS_AI: return False
    
    images_dir = data_dir / "images"
    masks_dir = data_dir / "masks"
    transforms_file = data_dir / "transforms.json"

    if not transforms_file.exists():
        print("âš ï¸ æœªæ‰¾åˆ° transforms.jsonï¼Œæ— æ³•è¿›è¡Œ Mask å¤„ç†ã€‚")
        return False

    # ================= æ ¸å¿ƒä¿®æ”¹é€»è¾‘å¼€å§‹ =================
    print(f"\nâœ‚ï¸ [AI åˆ†å‰²] æ­£åœ¨åˆå§‹åŒ–...")

    # --- ç¬¬ä¸€å±‚ï¼šå°è¯•è°ƒç”¨å¤§æ¨¡å‹è·å–ç²¾å‡† Prompt ---
    text_prompt = None
    try:
        # å°è¯•è°ƒç”¨ä½ å†™çš„é‚£ä¸ªå‡½æ•°
        text_prompt = get_central_object_prompt(images_dir)
    except Exception as e:
        print(f"    âš ï¸ å¤§æ¨¡å‹è°ƒç”¨å‡ºé”™: {e}")

    # --- ç¬¬äºŒå±‚ï¼šå¦‚æœå¤§æ¨¡å‹å¤±è´¥ï¼Œä½¿ç”¨é€šç”¨ Prompt ---
    if not text_prompt:
        # ä½¿ç”¨ä¸€ä¸ªéå¸¸é€šç”¨çš„è¯ï¼Œè®© YOLO-World å»æ‰¾ç”»é¢é‡Œæœ€æ˜¾è‘—çš„ä¸œè¥¿
        # "salient object" (æ˜¾è‘—ç‰©ä½“) æˆ– "central object" (ä¸­å¿ƒç‰©ä½“) æ•ˆæœé€šå¸¸ä¸é”™
        text_prompt = "central object; single object"
        print(f"    âš ï¸ æœªèƒ½è·å–ç²¾å‡†æè¿°ï¼Œé™çº§ä½¿ç”¨é€šç”¨ Prompt: '{text_prompt}'")
    else:
        print(f"    ğŸ¯ è·å–åˆ°ç²¾å‡† Prompt: '\033[92m{text_prompt}\033[0m'")

    masks_dir.mkdir(parents=True, exist_ok=True)
    # ================= æ ¸å¿ƒä¿®æ”¹é€»è¾‘ç»“æŸ =================

    # 2. åŠ è½½æ¨¡å‹ (æ¨èç”¨ Large)
    print("    -> æ­£åœ¨åŠ è½½ SAM 2 Large æ¨¡å‹...")
    
    # ğŸ”¥ è‡ªåŠ¨è¿ç§» AI æ¨¡å‹æ–‡ä»¶
    model_files = ["yolov8s-worldv2.pt", "sam2.1_l.pt"]
    for model_name in model_files:
        target_model_path = LINUX_WORK_ROOT / model_name
        local_model_path = Path(__file__).parent / model_name
        
        if not target_model_path.exists():
            if local_model_path.exists():
                print(f"    ğŸ“¦ æ£€æµ‹åˆ°æœ¬åœ°æ¨¡å‹ {model_name}ï¼Œæ­£åœ¨è¿ç§»è‡³å·¥ä½œåŒº...")
                shutil.copy2(str(local_model_path), str(target_model_path))
            else:
                print(f"    âš ï¸ æœªåœ¨è„šæœ¬ç›®å½•æ‰¾åˆ° {model_name}ï¼Œå°†å°è¯•è‡ªåŠ¨ä¸‹è½½...")

    try:
        # ä½¿ç”¨ç»å¯¹è·¯å¾„åŠ è½½ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤åç§°ï¼ˆè§¦å‘ä¸‹è½½ï¼‰
        yolo_path = LINUX_WORK_ROOT / "yolov8s-worldv2.pt"
        sam_path = LINUX_WORK_ROOT / "sam2.1_l.pt"
        
        # YOLO-World: å¬æ‡‚æ–‡å­—ï¼Œæ‰¾æ¡†
        det_model = YOLOWorld(str(yolo_path) if yolo_path.exists() else "yolov8s-worldv2.pt") 
        det_model.set_classes([text_prompt])
        
        # SAM 2: æ ¹æ®æ¡†ï¼ŒæŠ å›¾
        # æ³¨æ„ï¼šä½¿ç”¨ sam2.1_l.pt (Largeç‰ˆæœ¬) ç²¾åº¦æ›´é«˜ï¼Œé€Ÿåº¦è¾ƒæ…¢
        sam_model = SAM(str(sam_path) if sam_path.exists() else "sam2.1_l.pt") 
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return False

    # 3. è¯»å– transforms.json (ç”¨äºæœ€åè¿‡æ»¤)
    with open(transforms_file, 'r') as f:
        meta = json.load(f)
    
    # å»ºç«‹æ–‡ä»¶ååˆ°å¸§æ•°æ®çš„æ˜ å°„ï¼Œæ–¹ä¾¿åç»­åˆ é™¤
    # æ³¨æ„ï¼šfile_path å¯èƒ½æ˜¯ "images/frame_001.jpg"ï¼Œæˆ‘ä»¬åªå–æ–‡ä»¶ååŒ¹é…
    frames_map = {Path(f["file_path"]).name: f for f in meta["frames"]}
    
    image_files = sorted(list(images_dir.glob("*.jpg")) + list(images_dir.glob("*.png")))
    total_imgs = len(image_files)
    
    valid_frames_list = [] # å­˜æ”¾åˆæ ¼çš„å¸§æ•°æ®
    deleted_count = 0
    
    print(f"    -> å¼€å§‹å¤„ç† {total_imgs} å¼ å›¾ç‰‡...")

    for i, img_path in enumerate(image_files):
        # --- A. æ£€æµ‹ä¸åˆ†å‰² (åŒå‰) ---
        try:
            # 1. YOLO æ£€æµ‹
            det_results = det_model.predict(img_path, conf=0.05, verbose=False)
            
            # ============================================================
            # ğŸ•µï¸â€â™‚ï¸ [DEBUG æ¨¡å¼] çœ‹çœ‹ YOLO åˆ°åº•çœ‹åˆ°äº†ä»€ä¹ˆï¼Ÿ
            # ============================================================
            
            # 1. å‡†å¤‡è°ƒè¯•ç›®å½• (åªä¼šåˆ›å»ºä¸€æ¬¡)
            debug_dir = data_dir / "debug_yolo_visuals"
            debug_dir.mkdir(parents=True, exist_ok=True)
            
            # 2. æ£€æŸ¥æ£€æµ‹ç»“æœ
            num_boxes = len(det_results[0].boxes)
            
            if num_boxes > 0:
                # è·å–ç”»äº†æ¡†çš„å›¾ç‰‡ (numpy æ•°ç»„)
                plotted_img = det_results[0].plot()
                
                # ä¿å­˜åˆ° debug ç›®å½•ï¼Œæ–‡ä»¶ååŠ ä¸ªå‰ç¼€æ–¹ä¾¿æ‰¾
                debug_path = debug_dir / f"debug_{img_path.name}"
                cv2.imwrite(str(debug_path), plotted_img)
                
                # åœ¨æ§åˆ¶å°æ‰“å°åæ ‡ä¿¡æ¯ (åªæ‰“å°å‰ 3 å¼ å›¾ï¼Œé¿å…åˆ·å±)
                if i < 3: 
                    print(f"\n    ğŸ‘€ [DEBUG] {img_path.name}: æ‰¾åˆ°äº† {num_boxes} ä¸ªç›®æ ‡")
                    box_coords = det_results[0].boxes.xyxy.cpu().numpy()[0] # å–ç¬¬ä¸€ä¸ªæ¡†
                    conf_score = det_results[0].boxes.conf.cpu().numpy()[0]
                    print(f"       -> ä½ç½®: {box_coords} (ç½®ä¿¡åº¦: {conf_score:.2f})")
                    print(f"       -> è°ƒè¯•å›¾å·²ä¿å­˜: {debug_path}")
            else:
                if i < 3:
                    print(f"\n    ğŸ™ˆ [DEBUG] {img_path.name}: YOLO æ²¡æ‰¾åˆ°ä»»ä½•ä¸œè¥¿ (0 boxes)")
            
            # ============================================================

            bboxes = det_results[0].boxes.xyxy.cpu() 

            # ============================================================
            # ğŸ”¥ æ ¸å¿ƒä¿®æ”¹ï¼šä»â€œæ­»æ¡†â€æ”¹ä¸ºâ€œæ™ºèƒ½ä¸­å¿ƒç‚¹æ‰©æ•£â€
            # ============================================================
            
            # æ ‡è®°æ˜¯å¦ä½¿ç”¨ç‚¹æç¤º
            use_point_prompt = False
            
            # å¦‚æœ YOLO æ²¡æ‰¾åˆ°æ¡†ï¼Œæˆ–è€…æ¡†å¤ªç¦»è°±
            if len(bboxes) == 0:
                print(f"       âš ï¸ YOLO æœªè¯†åˆ«åˆ°ç‰©ä½“ï¼Œåˆ‡æ¢ä¸º [SAM ä¸­å¿ƒç‚¹æ¨¡å¼]")
                h, w = det_results[0].orig_shape[:2]
                import torch
                
                # ç­–ç•¥ï¼šç»™ SAM ä¸€ä¸ªä¸­å¿ƒç‚¹ (x, y)ï¼Œè®©å®ƒè‡ªå·±å»â€œæ³›æ´ªå¡«å……â€
                # points æ ¼å¼: [[x, y]]
                input_points = [[w / 2, h / 2]]
                # labels æ ¼å¼: [1] (1è¡¨ç¤ºå‰æ™¯ç‚¹ï¼Œ0è¡¨ç¤ºèƒŒæ™¯ç‚¹)
                input_labels = [1]
                
                use_point_prompt = True
            
            # 3. æ‰§è¡Œ SAM åˆ†å‰²
            if use_point_prompt:
                # æ–¹å¼ A: ä½¿ç”¨ç‚¹æç¤º (Point Prompt)
                # æ³¨æ„ï¼šUltralytics çš„ SAM æ¥å£è°ƒç”¨æ–¹å¼å¯èƒ½ç•¥æœ‰ä¸åŒï¼Œ
                # å¦‚æœæ˜¯å®˜æ–¹ SAMï¼Œé€šå¸¸æ˜¯ predict(points=..., labels=...)
                # åœ¨ Ultralytics å°è£…ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸æŠŠç‚¹è½¬æˆå¾®å°çš„æ¡†ï¼Œæˆ–è€…ç›´æ¥ä¼ å‚
                
                # ä¸ºäº†å…¼å®¹æ€§æœ€å¼ºï¼Œæˆ‘ä»¬è¿™é‡Œç”¨ä¸€ä¸ªâ€œæå°æ¡†â€æ¨¡æ‹Ÿâ€œç‚¹â€
                # è¿™æ · SAM ä¼šè®¤ä¸ºè¿™æ˜¯ä¸€ä¸ªéå¸¸ç¡®å®šçš„ä¸­å¿ƒåŒºåŸŸ
                cx, cy = w / 2, h / 2
                margin = 5 # 5åƒç´ çš„ä¸­å¿ƒåŒºåŸŸ
                bboxes = torch.tensor([[cx-margin, cy-margin, cx+margin, cy+margin]], device=det_model.device)
                
                # è°ƒç”¨ SAM (Ultralytics ä¼šæŠŠè¿™ä¸ªå°æ¡†å½“åšæç¤º)
                sam_results = sam_model(img_path, bboxes=bboxes, verbose=False)
            else:
                # æ–¹å¼ B: ä½¿ç”¨ YOLO çš„æ¡† (Box Prompt)
                sam_results = sam_model(img_path, bboxes=bboxes, verbose=False)
            
            if sam_results[0].masks is not None:
                all_masks = sam_results[0].masks.data.cpu().numpy()
                final_mask = np.any(all_masks, axis=0).astype(np.uint8) * 255
            else:
                final_mask = np.zeros(det_results[0].orig_shape[:2], dtype=np.uint8)

            # -------------------------------------------------
            # ğŸ”¥ æ ¸å¿ƒä¿®æ”¹ï¼šä½¿ç”¨å‡€åŒ–å‡½æ•° ğŸ”¥
            # -------------------------------------------------
            # æ³¨æ„ï¼šè¿™é‡Œæ¥æ”¶ 3 ä¸ªè¿”å›å€¼ (æ˜¯å¦åˆæ ¼, æ–°Mask, åŸå› )
            is_good, cleaned_mask, reason = clean_and_verify_mask(final_mask, img_path.name)

            if is_good:
                # âœ… åˆæ ¼ï¼šä½¿ç”¨æ¸…æ´—åçš„ Mask (cleaned_mask) è¿›è¡Œå¤„ç†
                
                # 1. æ¶‚é»‘æ“ä½œ -> æ”¹ä¸ºç”Ÿæˆ RGBA (PNG) å›¾ç‰‡
                original_img = cv2.imread(str(img_path))
                if original_img is not None:
                    # ç¾½åŒ–è¾¹ç¼˜ (å‡å°‘ç¡¬åˆ‡ä¼ªå½±)
                    mask_blurred = cv2.GaussianBlur(cleaned_mask, (5, 5), 0)
                    
                    # ç¡®ä¿ alpha_channel æ˜¯ float32
                    alpha_channel = mask_blurred.astype(np.float32) / 255.0
                    
                    # è½¬æ¢åŸå›¾ä¸º float32 ä»¥ä¾¿è®¡ç®—
                    img_float = original_img.astype(np.float32)
                    
                    # é¢„ä¹˜ Alpha (Premultiplied Alpha)
                    b, g, r = cv2.split(img_float)
                    b = b * alpha_channel
                    g = g * alpha_channel
                    r = r * alpha_channel
                    
                    # ğŸ”¥ ä¿®å¤ç‚¹ï¼šåœ¨ merge ä¹‹å‰ï¼Œå¼ºåˆ¶æ‰€æœ‰é€šé“è½¬å› uint8
                    # è¿™æ · b, g, r, a å…¨éƒ¨éƒ½æ˜¯ uint8 ç±»å‹ï¼ŒOpenCV å°±ä¸ä¼šæŠ¥é”™äº†
                    img_bgra = cv2.merge([
                        b.astype(np.uint8), 
                        g.astype(np.uint8), 
                        r.astype(np.uint8), 
                        mask_blurred # å·²ç»æ˜¯ uint8ï¼Œç›´æ¥ç”¨
                    ])
                    
                    # ä¿å­˜ä¸º PNG (å¿…é¡»ç”¨ PNG å­˜é€æ˜é€šé“)
                    new_img_path = img_path.with_suffix('.png')
                    cv2.imwrite(str(new_img_path), img_bgra)
                    
                    # å¦‚æœåŸå›¾æ˜¯ jpgï¼Œåˆ æ‰å®ƒï¼Œé¿å…é‡å¤
                    if img_path.suffix.lower() == '.jpg':
                        try: img_path.unlink()
                        except: pass
                        
                    final_img_path_name = new_img_path.name
                else:
                    final_img_path_name = img_path.name

                # 2. ä¿å­˜ Mask (ä¸€å®šè¦ä¿å­˜æ¸…æ´—åçš„ï¼)
                cv2.imwrite(str(masks_dir / f"{img_path.stem}.png"), cleaned_mask)

                # 3. åŠ å…¥åˆæ ¼åˆ—è¡¨
                # è®°å¾—åœ¨è¿™é‡Œæ›´æ–° json é‡Œçš„æ–‡ä»¶å (åç¼€å˜æˆäº† .png)
                if img_path.name in frames_map:
                    frame_data = frames_map[img_path.name]
                    frame_data["file_path"] = f"images/{final_img_path_name}" 
                    frame_data["mask_path"] = f"masks/{img_path.stem}.png"
                    valid_frames_list.append(frame_data)

            else:
                # âŒ ä¸åˆæ ¼ï¼šç‰©ç†åˆ é™¤
                print(f"       ğŸ—‘ï¸ [å‰”é™¤] {img_path.name}: {reason}")
                img_path.unlink() # åˆ é™¤å›¾ç‰‡æ–‡ä»¶
                deleted_count += 1
                # æ³¨æ„ï¼šè¿™é‡Œä¸æŠŠå®ƒåŠ å…¥ valid_frames_listï¼Œå®ƒè‡ªç„¶å°±ä» transforms.json é‡Œæ¶ˆå¤±äº†

        except Exception as e:
            print(f"       âŒ å¤„ç†å‡ºé”™ {img_path.name}: {e}")
            # å‡ºé”™ä¹Ÿè§†ä¸ºä¸åˆæ ¼ï¼Œä¸åŠ å…¥åˆ—è¡¨
            continue

        if i % 10 == 0:
            print(f"       è¿›åº¦: {i}/{total_imgs} (å·²å‰”é™¤ {deleted_count} å¼ )...", end="\r")

    # 4. ç»“ç®—ä¸æ›´æ–°
    print(f"\n\nğŸ“Š ç­›é€‰æŠ¥å‘Š:")
    print(f"   - åŸå§‹æ€»æ•°: {total_imgs}")
    print(f"   - å‰”é™¤æ•°é‡: {deleted_count} ({deleted_count/total_imgs:.1%})")
    print(f"   - å‰©ä½™å¯ç”¨: {len(valid_frames_list)}")

    if len(valid_frames_list) == 0:
        print("âŒ é”™è¯¯ï¼šæ‰€æœ‰å›¾ç‰‡éƒ½è¢«å‰”é™¤äº†ï¼è¯·æ£€æŸ¥æç¤ºè¯æˆ–æ‹æ‘„è´¨é‡ã€‚")
        return False

    # 5. é‡å†™ transforms.json
    # åªä¿ç•™åˆæ ¼çš„å¸§ï¼Œè¿™æ · Nerfstudio å°±åªä¼šè®­ç»ƒè¿™äº›â€œçº¯å‡€â€çš„é»‘èƒŒæ™¯å›¾
    meta["frames"] = valid_frames_list
    with open(transforms_file, 'w') as f:
        json.dump(meta, f, indent=4)
        
    print(f"    âœ… transforms.json å·²æ›´æ–°ï¼Œæ•°æ®é›†å·²æ¸…æ´—å®Œæ¯•ã€‚")
    return True

# ================= è¾…åŠ©å·¥å…· =================
def format_duration(seconds):
    return str(datetime.timedelta(seconds=int(seconds)))

def smart_filter_blurry_images(image_folder, keep_ratio=0.85, max_images=MAX_IMAGES):
    # (ä¿æŒåŸæœ‰çš„æ¸…æ´—é€»è¾‘ä¸å˜)
    print(f"\nğŸ§  [æ™ºèƒ½æ¸…æ´—] æ­£åœ¨åˆ†æå›¾ç‰‡è´¨é‡ (æ··åˆç­–ç•¥ç‰ˆ)...")
    image_dir = Path(image_folder)
    images = sorted([p for p in image_dir.iterdir() if p.suffix.lower() in ['.jpg', '.jpeg', '.png']])
    if not images: return
    trash_dir = image_dir.parent / "trash_smart"
    trash_dir.mkdir(exist_ok=True)
    img_scores = []
    for i, img_path in enumerate(images):
        img = cv2.imread(str(img_path))
        if img is None: continue
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        h, w = gray.shape
        grid_h, grid_w = h // 3, w // 3
        max_grid_score = 0
        for r in range(3):
            for c in range(3):
                roi = gray[r*grid_h:(r+1)*grid_h, c*grid_w:(c+1)*grid_w]
                score = cv2.Laplacian(roi, cv2.CV_64F).var()
                if score > max_grid_score: max_grid_score = score
        img_scores.append((img_path, max_grid_score))
        if i % 50 == 0: print(f"  -> åˆ†æä¸­... {i}/{len(images)}", end="\r")
    
    scores = [s[1] for s in img_scores]
    if not scores: return
    quality_threshold = np.percentile(scores, (1 - keep_ratio) * 100)
    good_images = []
    for img_path, score in img_scores:
        if score < quality_threshold:
            shutil.move(str(img_path), str(trash_dir / img_path.name))
        else:
            good_images.append(img_path)
    
    if len(good_images) > max_images:
        indices_to_keep = set(np.linspace(0, len(good_images) - 1, max_images, dtype=int))
        for idx, img_path in enumerate(good_images):
            if idx not in indices_to_keep:
                shutil.move(str(img_path), str(trash_dir / img_path.name))
    print(f"âœ¨ æ¸…æ´—ç»“æŸï¼Œå‰©ä½™ {len(list(image_dir.glob('*')))} å¼ ã€‚")

def analyze_and_calculate_adaptive_collider(json_path):
    # (ä¿æŒåŸæœ‰é€»è¾‘ï¼Œä½†å¦‚æœæ£€æµ‹åˆ° Maskï¼Œå¯ä»¥æ›´åŠ æ¿€è¿›)
    print(f"\nğŸ¤– [AI åˆ†æ] è§£æç›¸æœºè½¨è¿¹...")
    try:
        with open(json_path, 'r') as f: data = json.load(f)
        frames = data["frames"]
        if not frames: return [], "unknown"
        
        # ç®€å•åˆ¤å®šï¼šæ˜¯å¦æœ‰ mask_path
        has_mask = "mask_path" in frames[0]
        if has_mask:
            print("    -> æ£€æµ‹åˆ° Mask æ•°æ®ï¼å°†å¯ç”¨ç‰©ä½“èšç„¦æ¨¡å¼ã€‚")
        
        # (åŸæœ‰çš„è½¨è¿¹åˆ†æé€»è¾‘...)
        positions = [np.array(f["transform_matrix"])[:3, 3] for f in frames]
        forward_vectors = [np.array(f["transform_matrix"])[:3, :3] @ np.array([0, 0, -1]) for f in frames]
        center = np.mean(positions, axis=0)
        vec_to_center = center - positions
        vec_to_center /= (np.linalg.norm(vec_to_center, axis=1, keepdims=True) + 1e-6)
        ratio = np.sum(np.sum(forward_vectors * vec_to_center, axis=1) > 0) / len(frames)
        
        # å¦‚æœæœ‰ Maskï¼Œæˆ–è€…ç›¸æœºå‘å†…çœ‹ï¼Œéƒ½è®¤ä¸ºæ˜¯ç‰©ä½“æ¨¡å¼
        is_object_mode = ratio > 0.6 or FORCE_SPHERICAL_CULLING or has_mask

        if is_object_mode:
            dists = [np.linalg.norm(p) for p in positions]
            avg_dist = np.mean(dists)
            scene_radius = 1.0 * SCENE_RADIUS_SCALE
            calc_near = max(0.05, min(dists) - scene_radius)
            calc_far = avg_dist + scene_radius
            return ["--pipeline.model.enable-collider", "True", 
                    "--pipeline.model.collider-params", "near_plane", str(round(calc_near, 2)), 
                    "far_plane", str(round(calc_far, 2))], "object"
        else:
            return ["--pipeline.model.enable-collider", "True", 
                    "--pipeline.model.collider-params", "near_plane", "0.05", "far_plane", "100.0"], "scene"
    except:
        return [], "unknown"

def perform_percentile_culling(ply_path, json_path, output_path):
    # (ä¿æŒåŸæœ‰é€»è¾‘ä¸å˜)
    if not HAS_PLYFILE: return False
    print(f"\nâœ‚ï¸ [åå¤„ç†] æ­£åœ¨æ‰§è¡Œã€åˆ†ä½æ•°æš´åŠ›åˆ‡å‰²ã€‘...")
    try:
        with open(json_path, 'r') as f: frames = json.load(f)["frames"]
        cam_pos = np.array([np.array(f["transform_matrix"])[:3, 3] for f in frames])
        center = np.mean(cam_pos, axis=0)
        
        plydata = PlyData.read(str(ply_path))
        vertex = plydata['vertex']
        points = np.stack([vertex['x'], vertex['y'], vertex['z']], axis=1)
        
        dists_pts = np.linalg.norm(points - center, axis=1)
        threshold_radius = np.percentile(dists_pts, KEEP_PERCENTILE * 100)
        
        opacities = 1 / (1 + np.exp(-vertex['opacity']))
        mask = (dists_pts < threshold_radius) & (opacities > 0.05)
        filtered_vertex = vertex[mask]
        
        PlyData([PlyElement.describe(filtered_vertex, 'vertex')]).write(str(output_path))
        return True
    except Exception as e:
        print(f"âŒ åˆ‡å‰²å¤±è´¥: {e}")
        return False

# ================= ä¸»æµç¨‹ =================

def run_pipeline(video_path, project_name):
    global_start_time = time.time()
    print(f"\nğŸš€ [BrainDance Engine AI-Enhanced] å¯åŠ¨ä»»åŠ¡: {project_name}")
    print(f"ğŸ•’ {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    video_src = Path(video_path).resolve()
    work_dir = LINUX_WORK_ROOT / project_name
    data_dir = work_dir / "data"
    output_dir = work_dir / "outputs"
    transforms_file = data_dir / "transforms.json"
    env = os.environ.copy()
    env["QT_QPA_PLATFORM"] = "offscreen" 
    # ã€æ–°å¢ã€‘ä¿®å¤ distutils æŠ¥é”™çš„å…³é”®ç¯å¢ƒå˜é‡
    env["SETUPTOOLS_USE_DISTUTILS"] = "stdlib" 

    # [Step 1] æ•°æ®å¤„ç†
    step1_start = time.time()
    
    # ... (ç›®å½•åˆå§‹åŒ–é€»è¾‘ä¿æŒä¸å˜)
    if work_dir.exists(): shutil.rmtree(work_dir, ignore_errors=True)
    work_dir.mkdir(parents=True, exist_ok=True)
    data_dir.mkdir(parents=True, exist_ok=True)
    shutil.copy(str(video_src), str(work_dir / video_src.name))

    print(f"\nğŸ¥ [1/4] æ•°æ®å‡†å¤‡ä¸æ¸…æ´—")
    temp_dir = work_dir / "temp_extract"
    temp_dir.mkdir(parents=True, exist_ok=True)
    extracted_images_dir = work_dir / "raw_images"
    extracted_images_dir.mkdir(parents=True, exist_ok=True)
    
    # FFmpeg æŠ½å¸§
    try:
        subprocess.run(["ffmpeg", "-y", "-i", str(work_dir / video_src.name), 
                        "-vf", "fps=10", "-q:v", "2", 
                        str(temp_dir / "frame_%05d.jpg")], check=False, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL) 
    except: pass
    
    # æ¸…æ´—
    smart_filter_blurry_images(temp_dir, keep_ratio=0.85)
    
    # è¿ç§»
    all_candidates = sorted(list(temp_dir.glob("*.jpg")) + list(temp_dir.glob("*.png")))
    final_images_list = []
    if len(all_candidates) > MAX_IMAGES:
        indices = np.linspace(0, len(all_candidates) - 1, MAX_IMAGES, dtype=int)
        indices = sorted(list(set(indices)))
        for idx in indices: final_images_list.append(all_candidates[idx])
    else:
        final_images_list = all_candidates

    for img_path in final_images_list:
        shutil.copy2(str(img_path), str(extracted_images_dir / img_path.name))
    shutil.rmtree(temp_dir)

    # COLMAP æµç¨‹ (å¢å¼ºç‰ˆ - åŒ…å«è‡ªåŠ¨ä¿®æ­£)
    print(f"\nğŸ“ [2/4] COLMAP ä½å§¿è§£ç®— (å¢å¼ºç‰ˆ)")
    colmap_output_dir = data_dir / "colmap"
    colmap_output_dir.mkdir(parents=True, exist_ok=True)
    database_path = colmap_output_dir / "database.db"
    
    # æŸ¥æ‰¾ colmap
    system_colmap_exe = shutil.which("colmap") or "/usr/local/bin/colmap"

    full_log_content = []

    def run_colmap_step(cmd, description):
        print(f"\nğŸš€ {description}...")
        try:
            # ä½¿ç”¨ Popen å®æ—¶æ‰“å°è¾“å‡º
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                universal_newlines=True,
                env=env
            )
            
            # å®æ—¶è¯»å–è¾“å‡º
            for line in process.stdout:
                full_log_content.append(line)
                # è¿‡æ»¤æ‰è¿‡äºé¢‘ç¹çš„è¿›åº¦è¾“å‡ºï¼Œä¿ç•™å…³é”®ä¿¡æ¯
                # æ‰©å……å…³é”®è¯ï¼Œç¡®ä¿ Mapper é˜¶æ®µèƒ½çœ‹åˆ° Registering å’Œ Bundle adjustment ç­‰ä¿¡æ¯
                if any(k in line for k in ["Iteration", "Error", "Loading", "Elapsed", "Registering", "Image #", "Bundle adjustment", "Retriangulation", "Filtering"]):
                    print(f"    [COLMAP] {line.strip()}")
            
            process.wait()
            
            if process.returncode != 0:
                raise subprocess.CalledProcessError(process.returncode, cmd)
                
        except Exception as e:
            print(f"âŒ {description} å¤±è´¥: {e}")
            raise e

    # ==========================================
    # ğŸ”¥ æ ¸å¿ƒä¿®æ”¹ï¼šé—­ç¯é‡è¯•æœºåˆ¶ (åŒ…å«è´¨é‡æ£€æµ‹) ğŸ”¥
    # ==========================================
    MAX_RETRIES = 3
    colmap_success = False

    # æå‰åŒæ­¥å›¾ç‰‡ (åªéœ€è¦åšä¸€æ¬¡)
    dest_images_dir = data_dir / "images"
    dest_images_dir.mkdir(parents=True, exist_ok=True)
    for img in extracted_images_dir.glob("*"): 
        shutil.copy2(str(img), str(dest_images_dir / img.name))

    for attempt in range(1, MAX_RETRIES + 1):
        print(f"\nğŸ”„ [COLMAP] æ­£åœ¨æ‰§è¡Œç¬¬ {attempt} / {MAX_RETRIES} æ¬¡å°è¯•...")
        
        # --- 1. æ¯æ¬¡é‡è¯•å‰ï¼Œå¼ºåˆ¶æ¸…ç†ç¯å¢ƒ ---
        if attempt > 1:
            print("    ğŸ§¹ [é‡è¯•å‡†å¤‡] æ­£åœ¨æ¸…ç†æ—§æ•°æ®...")
            if database_path.exists(): 
                try: database_path.unlink()
                except: pass
            
            sparse_dir = colmap_output_dir / "sparse"
            if sparse_dir.exists(): 
                try: shutil.rmtree(sparse_dir)
                except: pass
            sparse_dir.mkdir(parents=True, exist_ok=True)
            
            # åˆ é™¤å¯èƒ½å­˜åœ¨çš„æ—§ transforms.jsonï¼Œé˜²æ­¢è¯¯è¯»
            if transforms_file.exists():
                transforms_file.unlink()

        try:
            # --- 2. è¿è¡Œ COLMAP æ ¸å¿ƒæµç¨‹ ---
            
            # Step 1: ç‰¹å¾æå–
            run_colmap_step([
                system_colmap_exe, "feature_extractor", 
                "--database_path", str(database_path), 
                "--image_path", str(extracted_images_dir), 
                "--ImageReader.camera_model", "OPENCV", 
                "--ImageReader.single_camera", "1"
            ], "Step 1: ç‰¹å¾æå–")

            # Step 2: è¯æ±‡æ ‘åŒ¹é… (Vocab Tree)
            print("    -> ğŸŒ³ è¯æ±‡æ ‘åŒ¹é… (Vocab Tree Matcher)...")
            local_vocab_path = Path(__file__).parent / "vocab_tree_flickr100k_words.bin"
            if not VOCAB_TREE_PATH.exists():
                if local_vocab_path.exists():
                    shutil.copy2(str(local_vocab_path), str(VOCAB_TREE_PATH))
                else:
                    raise FileNotFoundError(f"Missing vocab tree: {VOCAB_TREE_PATH}")

            run_colmap_step([
                system_colmap_exe, "vocab_tree_matcher", 
                "--database_path", str(database_path),
                "--VocabTreeMatching.vocab_tree_path", str(VOCAB_TREE_PATH),
                "--VocabTreeMatching.match_list_path", "" 
            ], "Step 2: è¯æ±‡æ ‘åŒ¹é…")

            # Step 3: ç¨€ç–é‡å»º
            sparse_dir = colmap_output_dir / "sparse"
            sparse_dir.mkdir(parents=True, exist_ok=True)
            run_colmap_step([
                system_colmap_exe, "mapper", 
                "--database_path", str(database_path), 
                "--image_path", str(extracted_images_dir), 
                "--output_path", str(sparse_dir)
            ], "Step 3: ç¨€ç–é‡å»º")

            # --- 3. ç«‹å³æ‰§è¡Œ Auto-Fix (ç›®å½•ä¿®æ­£) ---
            # (å¿…é¡»åœ¨å¾ªç¯å†…åšï¼Œå› ä¸ºæ¯æ¬¡ mapper å¯èƒ½ä¼šä¹±ç”Ÿæˆç›®å½•)
            print("    ğŸ”§ æ­£åœ¨æ£€æŸ¥æ¨¡å‹ç»“æ„...")
            sparse_root = colmap_output_dir / "sparse"
            target_dir_0 = sparse_root / "0"
            target_dir_0.mkdir(parents=True, exist_ok=True)
            
            required_files = ["cameras.bin", "images.bin", "points3D.bin"]
            model_found = False
            
            # æ‰«æå¹¶å½’ä½
            if all((target_dir_0 / f).exists() for f in required_files):
                model_found = True
            else:
                for root, dirs, files in os.walk(sparse_root):
                    if all(f in files for f in required_files):
                        source_dir = Path(root)
                        if source_dir != target_dir_0:
                            for f in required_files:
                                if (target_dir_0/f).exists(): (target_dir_0/f).unlink()
                                shutil.move(str(source_dir/f), str(target_dir_0/f))
                        model_found = True
                        break
            
            if not model_found:
                raise RuntimeError("COLMAP æœªç”Ÿæˆæœ‰æ•ˆçš„ç¨€ç–æ¨¡å‹æ–‡ä»¶ï¼")

            # --- 4. ç«‹å³ç”Ÿæˆ transforms.json ä»¥æ£€æµ‹è´¨é‡ ---
            print("    -> æ­£åœ¨ç”Ÿæˆæ•°æ®ä»¥è¿›è¡Œè´¨é‡æ£€æµ‹...")
            run_colmap_step([
                "ns-process-data", "images", 
                "--data", str(dest_images_dir), 
                "--output-dir", str(data_dir), 
                "--skip-colmap", 
                "--skip-image-processing", 
                "--num-downscales", "0"
            ], "ç”Ÿæˆ transforms.json")

            # --- 5. ğŸ”¥ å…³é”®ï¼šè´¨é‡åˆ¤å†³ (Quality Gate) ğŸ”¥ ---
            if not transforms_file.exists():
                raise RuntimeError("transforms.json ç”Ÿæˆå¤±è´¥")

            with open(transforms_file, 'r') as f:
                meta = json.load(f)
            
            registered_count = len(meta["frames"])
            total_count = len(list(extracted_images_dir.glob("*.jpg")) + list(extracted_images_dir.glob("*.png")))
            
            match_ratio = registered_count / total_count if total_count > 0 else 0
            print(f"    ğŸ“Š æœ¬æ¬¡åŒ¹é…ç‡: {match_ratio:.2%} ({registered_count}/{total_count})")

            if match_ratio < 0.35: # é˜ˆå€¼ 35%
                print(f"    âš ï¸ åŒ¹é…ç‡è¿‡ä½ï¼Œåˆ¤å®šä¸ºå¤±è´¥ï¼å‡†å¤‡é‡è¯•...")
                # ä¸»åŠ¨æŠ›å‡ºå¼‚å¸¸ï¼Œè§¦å‘ except å—ï¼Œè¿›å…¥ä¸‹ä¸€æ¬¡å¾ªç¯
                raise RuntimeError(f"Low match ratio: {match_ratio:.2%}")
            
            # å¦‚æœèµ°åˆ°è¿™é‡Œï¼Œè¯´æ˜ä¸€åˆ‡æ­£å¸¸
            print(f"    âœ¨ è´¨é‡è¾¾æ ‡ï¼COLMAP åœ¨ç¬¬ {attempt} æ¬¡å°è¯•ä¸­æˆåŠŸï¼")
            colmap_success = True
            break # è·³å‡ºé‡è¯•å¾ªç¯

        except Exception as e:
            print(f"    âŒ ç¬¬ {attempt} æ¬¡å°è¯•å¤±è´¥: {e}")
            if attempt < MAX_RETRIES:
                print("    â³ 3ç§’åè¿›è¡Œä¸‹ä¸€æ¬¡é‡è¯•...")
                time.sleep(3)
            else:
                print("    ğŸ›‘ å·²è€—å°½æ‰€æœ‰é‡è¯•æœºä¼šã€‚")

    if not colmap_success:
        print("âŒ COLMAP æœ€ç»ˆå¤±è´¥ (è´¨é‡ä¸è¾¾æ ‡)ï¼Œä»»åŠ¡ç»ˆæ­¢ã€‚")
        return None

    # ================= ğŸ”¥ AI ä»‹å…¥ç‚¹ (æ–°å¢) =================
    if HAS_AI:
        print(f"\nğŸ§  [3/4] AI æ™ºèƒ½åˆ†å‰²ä»‹å…¥ (Qwen + YOLO + SAM)")
        ai_success = run_ai_segmentation_pipeline(data_dir)
        if ai_success:
            print("âœ¨ AI åˆ†å‰²æµç¨‹å®Œæˆï¼ŒMask å·²æ³¨å…¥ï¼")
        else:
            print("âš ï¸ AI åˆ†å‰²æµç¨‹é‡åˆ°é—®é¢˜ï¼Œå°†ä½¿ç”¨åŸå§‹å›¾åƒè®­ç»ƒã€‚")
    else:
        print("\nâ© è·³è¿‡ AI åˆ†å‰² (æœªæ»¡è¶³ä¾èµ–)")
    # ======================================================

    step1_duration = time.time() - step1_start
    print(f"â±ï¸ [é¢„å¤„ç†å®Œæˆ] è€—æ—¶: {format_duration(step1_duration)}")

    # [Step 2] è®­ç»ƒ
    step2_start = time.time()
    print(f"\nğŸ”¥ [4/4] å¼€å§‹è®­ç»ƒ (Splatfacto)")
    
    collider_args, scene_type = analyze_and_calculate_adaptive_collider(transforms_file)
    
    # æ„å»ºè®­ç»ƒå‘½ä»¤
    train_cmd = [
        "ns-train", "splatfacto", 
        "--data", str(data_dir), 
        "--output-dir", str(output_dir), 
        "--experiment-name", project_name, 
        "--pipeline.model.random-init", "False", 
        
        # ğŸ”¥ æ–°å¢å‚æ•° 1: å‘Šè¯‰ Nerfstudio èƒŒæ™¯æ˜¯é€æ˜çš„ï¼Œä¸è¦æŠŠé»‘è‰²æ¸²æŸ“å‡ºæ¥
        "--pipeline.model.background-color", "random", 
        
        # ğŸ”¥ æ–°å¢å‚æ•° 2: æé«˜ä¸é€æ˜åº¦é˜ˆå€¼ï¼Œè®©é‚£å±‚è–„è–„çš„é»‘è‰²çƒŸé›¾ç›´æ¥æ¶ˆå¤±
        "--pipeline.model.cull-alpha-thresh", "0.05", # é»˜è®¤æ˜¯ 0.005ï¼Œæ”¹å¤§åˆ° 0.05

        # ğŸ”¥ æ–°å¢ï¼šæé«˜åˆ†è£‚é—¨æ§› (é»˜è®¤ 0.0002 -> 0.0008)
        "--pipeline.model.densify-grad-thresh", "0.0008",
        # ğŸ”¥ æ–°å¢ï¼šæå‰åœæ­¢åˆ†è£‚ (é»˜è®¤ 15000 -> 10000)
        "--pipeline.model.stop-split-at", "10000",
        # ğŸ”¥ æ–°å¢ï¼šç¼©çŸ­çƒ­èº«æœŸ (é»˜è®¤ 500 -> 500)
        "--pipeline.model.warmup-length", "500",
        *collider_args,
        "--max-num-iterations", "15000", 
        "--vis", "viewer+tensorboard", 
        "--viewer.quit-on-train-completion", "True", 
        "nerfstudio-data", 
        "--downscale-factor", "1",
        "--orientation-method", "none", 
        "--center-method", "none",
        "--auto-scale-poses", "False"
    ]
    
    subprocess.run(train_cmd, check=True, env=env)
    step2_duration = time.time() - step2_start

    # [Step 3] å¯¼å‡º
    step3_start = time.time()
    print(f"\nğŸ’¾ æ­£åœ¨å¯¼å‡º...")
    search_path = output_dir / project_name / "splatfacto"
    run_dirs = sorted(list(search_path.glob("*")))
    latest_run = run_dirs[-1]
    
    subprocess.run([
        "ns-export", "gaussian-splat", 
        "--load-config", str(latest_run/"config.yml"), 
        "--output-dir", str(work_dir)
    ], check=True, env=env)
    
    # æš´åŠ›åˆ‡å‰²
    raw_ply = work_dir / "point_cloud.ply"
    if not raw_ply.exists(): raw_ply = work_dir / "splat.ply"
    cleaned_ply = work_dir / "point_cloud_cleaned.ply"
    final_ply = raw_ply
    
    if (scene_type == "object" or FORCE_SPHERICAL_CULLING) and raw_ply.exists():
        if perform_percentile_culling(raw_ply, transforms_file, cleaned_ply):
            final_ply = cleaned_ply
    
    step3_duration = time.time() - step3_start

    # [Step 4] å›ä¼ 
    target_dir = Path(__file__).parent / "results"
    target_dir.mkdir(exist_ok=True)
    shutil.copy2(str(final_ply), str(target_dir / f"{project_name}.ply"))
    
    total_duration = time.time() - global_start_time
    print(f"\nğŸ‰ å…¨éƒ¨å®Œæˆï¼æ¨¡å‹å·²ä¿å­˜è‡³: {target_dir / f'{project_name}.ply'}")
    print(f"ğŸ“Š è€—æ—¶ç»Ÿè®¡:")
    print(f"   - é¢„å¤„ç† (COLMAP + AI): {format_duration(step1_duration)}")
    print(f"   - è®­ç»ƒ (Splatfacto):    {format_duration(step2_duration)}")
    print(f"   - å¯¼å‡ºä¸åå¤„ç†:         {format_duration(step3_duration)}")
    print(f"   - æ€»è€—æ—¶:               {format_duration(total_duration)}")
    
    return str(target_dir / f"{project_name}.ply")

if __name__ == "__main__":
    script_dir = Path(__file__).resolve().parent
    video_file = script_dir / "test.mp4" 
    if len(sys.argv) > 1: video_file = Path(sys.argv[1])

    if video_file.exists():
        run_pipeline(video_file, "scene_ai_test")
    else:
        print(f"âŒ æ‰¾ä¸åˆ°è§†é¢‘: {video_file}")